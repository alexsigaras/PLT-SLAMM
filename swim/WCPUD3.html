 <html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
 <title>
  Lecture 1: January 23, 2013
 </title>
</head>
<body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 1: January 23, 2013<br/>
  Introduction to PLT
 </h1>

 <h2>1. Teaching Staff</h2>
 <ul>
 <h3>Instructor</h3>
 <dl>
  <dt>Professor Alfred V. Aho</dt>
  <dt><a href="http://www.cs.columbia.edu/%7Eaho"> http://www.cs.columbia.edu/~aho  </a></dt>
  <dt>aho@cs.columbia.edu</dt>
  <dt>513 Computer Science Building</dt>
  <dt>Office hours: Mondays and Wednesdays 1:00-2:00pm</dt>
  <dt>Course webpage: <a href="http://www.cs.columbia.edu/%7Eaho/cs4115">
      http://www.cs.columbia.edu/~aho/cs4115 </a></dt>
  <dt>Courseworks website: <a href="https://courseworks.columbia.edu/">
      https://courseworks.columbia.edu </a></dt>
  <dt>Piazza bulletin board: <a href="https://piazza.com/class#spring2013/comsw4115">
      https://piazza.com/class#spring2013/comsw4115 </a></dt>

  <p>
  </p><dt>Lectures on Mondays and Wednesdays, 2:40-3:55pm, 535 Mudd</dt>
 </dl>


 <h3>TAs</h3>

 <dl>
  <dt>Karan Bathla</dt>
  <dt>kb2658@columbia.edu</dt>
  <dt>Office hours: Mondays &amp; Tuesdays 4:00-5:00</dt>
  <dt>TA Room: 122 Mudd</dt>
 </dl>

 <br/>
 <dl>
  <dt>Melanie Kambadur</dt>
  <dt>melanie@cs.columbia.edu</dt>
  <dt>Office hours: Thursdays 10:00-12:00</dt>
  <dt>TA Room: 122 Mudd</dt>
 </dl>

 <br/>
 <dl>
  <dt>Jared Pochtar</dt>
  <dt>jrp2181@columbia.edu</dt>
  <dt>Office hours: Fridays 3:00-5:00</dt>
  <dt>TA Room: 122 Mudd</dt>

 </dl>

 <br/>
 <dl>
  <dt>Maria Taku</dt>
  <dt>mat2185@columbia.edu</dt>
  <dt>Office hours: Tuesdays and Thursdays 12:30-2:30</dt>
  <dt>TA Room: 122 Mudd</dt>
 </dl>

 </ul>

 <h2>2. Course Objectives</h2>
 <ul>
  <li>You will learn about the syntactic and semantic elements of modern programming languages.</li>
  <li>You will learn the important algorithms used by compilers to
       translate high-level source languages into machine and other
       target languages.</li>
  <li>You will learn about imperative, object-oriented, functional, logic, scripting languages, and parallel languages.</li>
  <li>A highlight of this course is a semester-long programming
      project in which you will work in a small team to create
      and implement an innovative little language of your own design.</li>
  <li>You will learn computational thinking and good software engineering practices.</li>
  <li>The concepts, techniques, and tools that you will learn in this
      course have broad application to many areas of computer science
      and software development outside of programming
      languages and compilers.</li>
 </ul>

 <h2>3. Course Syllabus</h2>
  <ul>
   <li>Computational thinking</li>
   <li>Kinds of programming languages</li>
   <li>Principles of compilers</li>
   <li>Lexical analysis</li>
   <li>Syntax analysis</li>
   <li>Tools for constructing compilers</li>
   <li>Syntax-directed translation</li>
   <li>Semantic analysis</li>
   <li>Run-time organization</li>
   <li>Intermediate code generation</li>
   <li>Code generation</li>
   <li>Code optimization</li>
   <li>Parallel and concurrent programming languages</li>
  </ul>

 
 
 <h2>4. Textbooks and References</h2>
 <ul>
  <li>The course text is</li>
   <ul>
    <li>Alfred V. Aho, Monica Lam, Ravi Sethi, and Jeffrey D. Ullman</li>
     <dt><i>Compilers: Principles, Techniques, and Tools</i>, Second Edition</dt>
     <dt>Pearson Addison-Wesley, 2007</dt>
   </ul><br/>
  <li>Other good references are</li>
   <ul>

     <li>Andrew W. Appel</li>
      <dt><i>Modern Compiler Implementation in Java</i>, second edition</dt>
      <dt>Cambridge University Press, 2002</dt>
     <br/>

     <li>Keith D. Cooper and Linda Torczon</li>
      <dt><i>Engineering a Compiler</i>, Second Edition</dt>
      <dt>Morgan Kaufmann, 2012</dt>
     <br/>

     <li>Steven S. Muchnick</li>
      <dt><i>Advanced Compiler Design and Implementation</i></dt>
      <dt>Morgan Kaufmann, 1997</dt>
     <br/>

     <li>Michael L. Scott</li>
      <dt><i>Programming Language Pragmatics</i>, Third Edition</dt>
      <dt>Morgan Kaufman, 2009</dt>
     <br/>

     <li>Robert W. Sebesta</li>
      <dt><i>Concepts of Programming Languages</i>, Tenth Edition</dt>
      <dt>Pearson/Addison-Wesley, 2012</dt>
     <br/>

    </ul><br/>
   <li><a href="http://www1.cs.columbia.edu/%7Esedwards/classes/2012/w4115-fall/index.html">
        Also see Professor Stephen Edwards' PLT website. Well worth a look!</a></li>

 </ul>

 <h2>5. Course Requirements, Grading, and Late Policy</h2>
 <ul>
  <li>Homework (10% of final grade)</li>
  <li>Midterm (20% of final grade): Wednesday, March 13, 2013</li>
  <li>Final (30% of final grade): Monday, May 6, 2013</li>
  <li>Course project (40% of final grade): project has team and individual components</li>
  <li>All assignments can be handed in one week after they are due for 50% credit.</li>
 </ul>

 <h2>6. Project Requirements</h2>
 <ul>
  <li>Form a team of five. Teams should be formed by Monday, February 4, 2013.</li>
  <li>Design a new innovative little language</li>
  <li>Build a compiler or interpreter for it.</li>
  <li>Project deliverables and due dates:</li>
  </ul><ol>
    <li>Feb 27: Language white paper.
        See Section 1.2 of
        <a href="http://www.oracle.com/technetwork/java/index-136113.html">
         http://www.oracle.com/technetwork/java/index-136113.html</a>
        for a sample white paper on Java.</li>
    <li>Mar 27: Language tutorial and reference manual.</li>
    </ol><ul>
     <li>See Chapter 1 of K&amp;R for a sample language tutorial.</li>
     <li>See Appendix A of K&amp;R for a sample language reference manual.</li>
    </ul>
    <li>May 14-16: Final project report and demo to teaching staff.</li>
   
  <li>Start to form project teams of five right away.  Elect a</li>
  <ul>
   <li>Project manager</li>
   <li>Language guru</li>
   <li>System architect</li>
   <li>System integrator</li>
   <li>Verification and validation person</li>
  </ul>
 


 <h2>7. Programming Languages</h2>
 <ul>
  <li>A programming language is a notation for specifying computational tasks that a person can understand
      and a computer can execute.</li>
  <li>Every programming language has a syntax and a semantics.</li>
  <ul>
   <li>The syntax specifies how a concept is expressed.</li>
   <li>The syntax is often defined using a (context-free) grammar.</li>
   <dl>
    <dd><i>statement</i> -&gt; <code>while (</code> <i>expression</i> <code>)</code> <i>statement</i></dd>
   </dl>
   <li>The semantics specifies what the concept means or does.</li>
   <li>Semantics can be specified operationally, axiomatically or denotationally.</li>
  </ul>
  <li>Ambiguity</li>
  <ul> 
   <li>"Time flies like an arrow."</li>
  </ul>
  <li>Domains of application</li>
  <ul>
   <li>Scientific</li>
   <ul>
    <li>Fortran</li>
   </ul>
   <li>Business</li>
   <ul>
    <li>COBOL</li>
   </ul>
   <li>Artificial intelligence</li>
   <ul>
    <li>LISP</li>
   </ul>
   <li>Systems</li>
   <ul>
    <li>C</li>
   </ul>
   <li>Web</li>
   <ul>
    <li>Java</li>
   </ul>
   <li>General purpose</li>
   <ul>
    <li>C++</li>
   </ul>
  </ul>
 </ul>


 <h2>8. Kinds of Languages</h2>
 <ul>
  <li>Imperative</li>
  <ul>
   <li>Specifies how a computation is to be done.</li>
   <li>Examples: C, C++, C#, Fortran, Java</li>
  </ul>
  <li>Declarative</li>
  <ul>
   <li>Specifies what computation is to be done.</li>
   <li>Examples: Haskell, ML, Prolog</li>
  </ul>
  <li>von Neumann</li>
  <ul>
   <li>One whose computational model is based on the von Neumann architecture.</li>
   <li>Basic means of computation is through the modification of variables (computing
       via side effects).</li>
   <li>Statements influence subsequent computations by changing the value of memory.</li>
   <li>Examples: C, C++, C#, Fortran, Java</li>
  </ul>
  <li>Object-oriented</li>
  <ul>
   <li>Program consists of interacting objects.</li>
   <li>Each object has its own internal state and executable functions (methods)
       to manage that state.</li>
   <li>Object-oriented programming is based on encapsulation, modularity,
       polymorphism, and inheritance.</li>
   <li>Examples: C++, C#, Java, OCaml, Simula 67, Smalltalk</li>
  </ul>
  <li>Scripting</li>
  <ul>
   <li>An interpreted language with high-level operators for
       "gluing together" computations.</li>
   <li>Examples: AWK, Perl, PHP, Python, Ruby</li>
  </ul>
  <li>Functional</li>
  <ul>
   <li>One whose computational model is based on the recursive definition of functions
       (lambda calculus).</li>
   <li>Examples: Haskell, Lisp, ML.</li>
  </ul>
  <li>Parallel</li>
  <ul>
   <li>One that allows a computation to run concurrently on multiple processors.</li>
   <li>Examples</li>
   <ul>
    <li>Libraries: POSIX threads, MPI</li>
    <li>Languages: Ada, Cilk, OpenCL, Chapel, X10</li>
    <li>Architecture: CUDA (parallel programming architecture for GPUs)</li>
   </ul>
  </ul>
  <li>Domain specific</li>
  <ul>
   <li>Many areas have special-purpose languages to facilitate the creation of applications.</li>
   <li>Examples</li>
   <ul>
    <li>YACC for creating parsers</li>
    <li>LEX for creating lexical analyzers</li>
    <li>MATLAB for numerical computations</li>
    <li>SQL for database applications</li>
   </ul>
  </ul>
  <li>Markup</li>
  <ul>
   <li>Not programming languages in the sense of being Turing complete, but widely used
       for document preparation.</li>
   <li>Examples: HTML, XHTML, XML</li>
  </ul>
 </ul>

 <h2>9. Influential Languages</h2>
 <ul>
  <li>1950s: assembler, Cobol, Fortran, Lisp</li>
  <li>1960s: Algol60, Basic, Simula67</li>
  <li>1970s: C, ML, scripting languages, application-specific languages</li>
  <li>See
      <a href="http://www.tiobe.com/">TIOBE Index</a>
      for their list of this month's 100 most popular programming languages.</li>
  <ul>
   <li>TIOBE top ten for January 2013: C, Java, Objective-C, C++, C#, PHP,
       Visual Basic, Python, Perl, Ruby</li>
  </ul>
 </ul>

 <h2>10. Language Design Issues</h2>
 <ul>
  <li>Application domain</li>
  <ul>
   <li>Exploit domain restrictions for expressiveness and performance.</li>
  </ul>
  <li>Computational model</li>
  <ul>
   <li>Choose a model that encourages simplicity and ease of expression.</li>
   <li>Incorporate a few primitives that can be elegantly combined to
       solve large classes of problems.</li>
  </ul>
  <li>Abstraction mechanisms</li>
  <ul>
   <li>Abstractions should foster reuse and be suggestive of solutions.</li>
  </ul>
  <li>Type system</li>
  <ul>
   <li>Type systems can help reliability and security of programs.</li>
  </ul>
  <li>Usability</li>
  <ul>
   <li>Language design should promote readability, writability, and efficiency.</li>
  </ul>
 </ul>

 <h2>11. To Do</h2>
 <ol>
  <li>Start forming your project team immediately. Teams should be in place by Feb 4, 2013.</li>
  <li>Use Courseworks discussion board (https://courseworks.columbia.edu) to publicize your interests.</li>
  <li>Contact Maria Taku (mat2185@columbia.edu) for help forming
      or finding a team.</li>
  <li>Give your language a name.</li>
 </ol>

 <h2>12. Reading Assignment</h2>
 <ul>
  <li>ALSU: Chapters 1 and 2</li>
 </ul>
<br/>



<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>



</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 2: January 28, 2013</title>
</head>
<body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 2: January 28, 2013<br/>
  Basic Elements of Languages and Compilers
 </h1>

 <h2>Overview</h2>
 <ol>
  <li>Course project</li>
  <li>What's in a language specification?</li>
  <li>The C programming language</li>
  <li>Fundamental elements of programming languages</li>
  <li>Language processing tools</li>
 </ol>

 <h2>1. Course Project</h2>
 <ul>
  <li>Project description</li>
  <ul>
   <li>Form a team of five. Contact Maria Taku (mat2185@columbia.edu) for help
       finding or forming a team.
       Once your team is complete, let Maria know who is on your team.</li>
   <li>Create an innovative little language of your own design.</li>
   <li>Write a compiler or interpreter for your language.</li>
   <li>Each team member must write at least 500 lines of
       code of the compiler or interpreter.</li>
   <li>Project constitutes 40% of final grade.</li>
  </ul>

  <li>Project team: elect one person to serve each of the following functions.</li>
  <ul>
   <li>Project manager</li>
   <ul>
    <li>This person sets the project schedule, holds weekly meetings
        with the entire team, maintains the project log, and makes
        sure the project deliverables get done on time.</li>
   </ul>
   <li>Language and tools guru</li>
   <ul>
    <li>This person defines the baseline process to track language changes
        and maintain the intellectual integrity of the language.</li>
    <li>This person teaches the team how to use various tools
        used to build the compiler.</li>
   </ul>
   <li>System architect</li>
   <ul>
    <li>This person defines the compiler architecture, modules, and
        interfaces.</li>
   </ul>
   <li>System integrator</li>
   <ul>
    <li>This person defines the system integration environment
       and makes sure the compiler components work together.</li>
   </ul>
   <li>Tester and validator</li>
   <ul>
    <li>This person defines the test suites and executes them
        to make sure the compiler meets the language specification.</li>
   </ul>
  </ul>
 
  <li>Project due dates and deliverables:</li>
  <ul>
    <li>Feb. 27: Language white paper (written by entire team, 3-4 pages).</li>
    <ul>
      <li>See
        <a href="http://www.oracle.com/technetwork/java/index-136113.html">
         http://www.oracle.com/technetwork/java/index-136113.html</a>
        for a sample white paper on Java.</li>
    </ul>
    <li>Mar. 27: Language tutorial (written by entire team, 15-20 pages).</li>
    <ul>
     <li>Chapter 1 of Kernighan and Ritchie is a good model of a
         language tutorial.</li>
     <li>Describe a few representative programs that illustrate the
         nature and scope of your language.</li>
     <li>A "hello, world" program is de rigueur.</li>
    </ul>
    <li>Mar. 27: Language reference manual (written by entire team, 20-25 pages).</li>
    <ul>
     <li>Appendix A of Kernighan and Ritchie is a good model.</li>
     <li>Give a complete description of the lexical and syntactic
         structure of your language.</li>
     <li>Include a full grammar for your language.</li>
    </ul>
    <li>May 14-16: Working compiler and demo.</li>
    <li>May 14-16: Final project report due at project demo.</li>
   </ul>
 </ul>
 
 <h2>2. A Language Specification Defines</h2>
 <ul>
  <li>the representation of programs</li>
  <li>the syntax and constraints of the language</li>
  <li>the semantic rules for interpreting programs</li>
  <li>the representation of input data to be processed by programs</li>
  <li>the representation of output data produced by programs</li>
  <li>other restrictions on programs (such as what makes a program portable
      across different implementations and platforms)</li>
 </ul>

 <h2>3. The C Programming Language</h2>
 <ul>
  <li>C is a general-purpose procedural programming language that was designed in 1969-1972
      at Bell Labs by Dennis Ritchie who was working on developing the Unix
      operating system with Ken Thompson. It is still one of the most widely
      used programming languages in the world.</li> 
  <li>C was originally designed for and implemented on the UNIX operating system
      on the DEC PDP-11.</li>
  <li>In 1978 Brian Kernighan and Dennis Ritchie published the book
      "The C Programming Language" which for many years
      served as the informal definition of C ("K&amp;R C").</li>
  <li>Kernighan and Ritchie described C as "a general-purpose programming language
      which features economy of expression, modern control flow and data structures,
      and a rich set of operators."</li>
  <li>C has gone through a number of versions since K&amp;R C:</li>
   <ul>
    <li>ANSI C (1989) and ISO C (1990): these versions are identical and are
        commonly referred to as C89</li>
    <li>C99: ISO/IEC9899:1999</li>
    <li>C11, the current standard, adopted in 12/8/2011</li>
    <li>Embedded C: in 2008 the C Standards Committee extended C to create
        programs to meet the stringent requirements of microcontroller systems</li>
   </ul>
   <br/>

  <li>A sample C program
 <ul>
 </ul><pre><code>
 (1)  /* this program computes the greatest common divisor
 (2)     of two integers entered on the command line */

 (3)  #include &lt;stdio.h&gt;
 (4)  #include &lt;stdlib.h&gt;

 (5)  int gcd(int m, int n)
 (6)  {
 (7)    int r;
 (8)    while ((r = m % n) != 0) {
 (9)      m = n;
(10)      n = r;
(11)    }
(12)    return n;
(13)  }

(14)  int main(int argc, char *argv[])
(15)  {
(16)    int m, n;
(17)    m = atoi(argv[1]);
(18)    n = atoi(argv[2]);
(19)    printf("gcd of %d and %d is %d\n", m, n, gcd(m, n)); 
(20)    return 0;
(21)  }
 </code></pre>
 </li></ul>
 

 <h2>4. Fundamental Elements of Programming Languages</h2>
 <ul>
  <li>Programming model</li>
  <ul>
   <li>The programming model is the model of computation encapsulated
       into the programming language.</li>
   <ul><font color="#0000ff">
    <li>For example, C is an imperative language, designed around the
      von Neumann model of computation.</li>
   </font></ul>
  </ul>

  <li>Program structure</li>
  <ul>
   <li>A program typically consists of one or more translation units stored in files.</li>
   <ul><font color="#0000ff">
    <li>In C, a translation unit is a sequence of function definitions and declarations.</li>
   </font></ul>
  </ul>
  <li>Character set and lexical conventions</li>
  <ul>
   <li>Source and target character sets may be different.</li>
   <ul><font color="#0000ff">
    <li>The character set of C source programs in contained within seven-bit ASCII.</li>
   </font></ul>
   <li>A token is a meaningful sequence of characters in a source program.</li>
   <ul><font color="#0000ff">
    <li>C has six classes of tokens: identifiers, keywords, constants, string literals,
        operators, and separators.</li>
   </font></ul>
  </ul>
  <li>Names, scopes, bindings, and lifetimes</li>
  <ul>
   <li>Names (often called identifiers) have a specified lexical structure.</li>
   <ul><font color="#0000ff">
    <li>In C identifiers are sequences of letters (here, underscore is considered a letter) and digits.
        The first character of an identifier must be a letter. At least the first 31 characters in an
        identifier are significant.</li>
   </font></ul>
   <li>The scope of a name is the region of the program in
       which it is known (visible).</li>
   <li>A binding is an association between two things such as between a
       variable and its type or between a symbol and the operation it represents.
       The time at which this association is determined is called
       the binding time. Bindings can take place at various times ranging
       from language design time to run time.</li>
   <li>The lifetime of a variable is the time during which the variable
       is bound to a specific memory location.</li>
  </ul>

  <li>Memory management</li>
  <ul>
   <li>One important function of a programming language is to provide facilities
       for managing memory and the objects stored in memory.</li>
   <ul><font color="#0000ff">
    <li>C provides three ways to allocate memory for objects:</li>
    <ul>
     <li>Static allocation where space for an object is provided by the compiler at run time.</li>
     <li>Automatic allocation where temporary objects are stored by the compiler on the runtime stack.
         This space is automatically freed by the compiler after the block in which the object is
         declared is exited.</li>
     <li>Dynamic allocation where blocks of memory of arbitrary size can be requested
         by a programmer using runtime library functions such as malloc from a region of
         memory called the heap.  These blocks
         persist until freed by a call to a runtime library function such as free.</li>
    </ul>
   </font></ul>
  </ul>


  <li>Data types and operators</li>
  <ul>
   <li>A data type defines a set of data values and the operations allowed
       on those values.</li>
   <ul><font color="#0000ff">
    <li>C has a small number of basic types, including <code>char, int, double, float,
          enum, void</code>.</li>
    <li>C has a potentially infinite number of recursively
          defined derived types
          such as arrays of objects of some type, functions returning objects of
          some type, pointers to objects of some type, structures containing a sequence
          of objects of various types, and unions containing any one of several
          objects of various types.</li>
    <li>C has a rich set of arithmetic, relational, logical, and assignment operators.</li>
   </font></ul>
  </ul>
  
  <li>Expressions and assignment statements</li>
  <ul>
   <li>Expressions are the primary means for specifying computations
       in a programming language.</li>
   <li>Assignment statements are basic constructs in imperative programming languages.
       Assignment statements allow the programmer to dynamically change
       the bindings of values to variables.</li>
  </ul>

  <li>Control flow</li>
  <ul>
   <li>Flow of control refers to the sequence in which the operations specified in
       a program are executed at run time. There are flow-of-control issues at
       many levels such as flow of control within
       expressions, among statements, and among program units.
       Most programming languages have control statements and other control
       structures for controlling the flow of control within a program.</li>
   <ul><font color="#0000ff">
    <li>C has a variety of flow-of-control constructs such as blocks and
        control statements such as <code>if-else, switch, while, for,
        do-while, break, continue</code> and <code>goto</code>.</li>
   </font></ul>
  </ul>

  <li>Functions and process abstraction</li>
  <ul>
   <li>Functions are perhaps the most important building blocks of programs.
       Functions are often called procedures, subroutines, or subprograms.
       Functions break large computing tasks into smaller ones and facilitate
       code reuse.  Functions are such an important topic in programming
       languages that we will talk about them in much more detail later in
       this course.</li>
  </ul>

  <li>Data abstraction and object orientation</li>
  <ul>
   <li>Data abstraction in the form of abstract data types was introduced
       into programming languages after process abstraction. The programming
       language Simula67 was instrumental
       in motivating constructs for supporting object-oriented programming
       in modern programming languages such as C++, C#, and Java.
       Object orientation is characterized by encapsulation, polymorphism,
       inheritance, and dynamic binding.</li>
  </ul>

  <li>Concurrency</li>
  <ul>
   <li>Concurrent execution of programs has assumed much more importance with
       the widespread use of multi-core and many-core processors.</li>
   <li>Concurrency
       in software execution can occur many levels of granularity: instruction,
       statement, subprogram, and program.</li>
   <li>Concurrency can be achieved with libraries (like MPI for Fortran, pthreads
       for C) or with direct language support (as in Cilk, X10).</li>
   <li>However, effective exploitation of concurrency is still an open
       research area in software.</li>
  </ul>
   
  <li>Exception and event handling</li>
  <ul>
   <li>Many languages have facilities for reacting to run-time error conditions.
       C++ has the <code>try-catch</code> construct to catch exceptions raised by the
       <code>throw</code> statement.</li>
   <li>Event handling is like exception handling in that an event handler is called
       by the occurence of an event. Implementing reactions to user interactions
       with GUI components is a common application of event handling.</li>
  </ul>
 </ul>



 <h2>5. Language Processing Tools</h2>
 <ul>
  <li>Basic compiler</li>
  <li>Interpreter</li>
  <li>Bytecode interpreter</li>
  <li>Just-in-time compiler</li>
  <li>Linker and loader</li>
  <li>Preprocessor</li>
 </ul>

 <h2>6. Practice Problems</h2>
 <ol>
  <li>Describe the von Neumann model of computation (computer architecture).</li>
  <li>Compare C and Java with regard to their (a) programming model and
      (b) primitive data types.</li>
 </ol>

 <h2>7. Reading Assignment</h2>
 <ul>
  <li>ALSU: Chapters 1 and 2</li>
 </ul>

 <h2>8. Reference</h2>
 <ul>
  <li>Brian Kernighan and Dennis Ritchie, <i>The C Programming Language</i>,
      2nd edition, Prentice Hall, 1988.
      This is the classic reference on ANSI C (C89).</li>
 </ul>


<br/>

<hr/>

<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>


</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
 <title>
  Lecture 3: January 30, 2013
 </title>
</head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 3: January 30, 2013<br/>
  Structure of a Compiler
 </h1>

 <h2>Overview</h2>
 <ol>
  <li>Language processing tools</li>
  <li>Structure of a compiler</li>
  <li>The lexical analyzer</li>
  <li>Language theory background</li>
  <li>Regular expressions</li>
  <li>Tokens/patterns/lexemes/attributes</li>
 </ol>


 <h2>1. Language Processing Tools</h2>
 <ul>
  <li>Compiler</li>
  <li>Interpreter</li>
  <li>C compiler</li>
  <li>Java compiler</li>
  <li>Just-in-time compiler</li>
 </ul>

 <h2>2. Structure of a Compiler</h2>
 <ul>
  <li>Front end: analysis</li>
  <li>Back end: synthesis</li>
  <li>IR: Intermediate representation(s)</li>
  <li>Phases</li>
  <ul>
    <li>lexical analyzer (scanner)</li>
    <li>syntax analyzer (parser)</li>
    <li>semantic analyzer</li>
    <li>intermediate code generator</li>
    <li>code optimizer</li>
    <li>code generator</li>
    <li>machine-specific code optimizer</li>
  </ul>
  <li>Symbol table</li>
  <li>Error handler</li>
  <li>Compiler component generators</li>
   <ul>
    <li><a href="http://dinosaur.compilertools.net/">
        lexical analyzer generators: lex, flex</a></li>
    <li><a href="http://dinosaur.compilertools.net/">
        syntax analyzer generator: yacc, bison</a></li>
    <li><a href="http://www.antlr.org/wiki/display/ANTLR3/Five+minute+introduction+to+ANTLR+3/">
        front-end generator: antlr</a></li>
   </ul>
 </ul>

 <h2>3. The Lexical Analyzer</h2>
 <ul>
  <li>The first phase of the compiler is the lexical analyzer,
      often called a lexer or scanner.</li>
  <li>The lexer reads the stream of characters making up the source
      program and groups the characters into logically meaningful sequences
      called lexemes.</li>
  <li>Many lexers use a leftmost-longest rule. For example,
      <code>a+++++b</code> would be partitioned into the lexemes
      <code>a ++ ++ + b</code>, not <code>a ++ + ++ b.</code></li>
  <li>For each lexeme the lexer sends to the parser a token of the
      form &lt;token-name, attribute-value&gt;.</li>
  <li>For a token such as an identifier, the lexer will make an entry into
      the symbol table in which it stores attributes such as
      the lexeme and type associated with the token.</li>
  <li>The lexer will also strip out whitespace
      (blanks, horizontal and vertical tabs, newlines, formfeeds, comments).</li>
  <li>Tokens in C</li>
  <ul>
   <li>identifiers</li>
   <li>keywords</li>
   <li>constants</li>
   <li>string literals</li>
   <li>operators</li>
   <li>separators</li>
  </ul>

  <li>Issues in the design of a lexical analyzer</li>
  <ul>
   <li>efficiency: buffered reads</li>
   <li>portability and character sets</li>
   <li>need for lookahead</li>
  </ul>

  <li>Coping with lexical errors</li>
  <ul>
   <li>types of lexical errors</li>
   <ul>
    <li>insertion/deletion/replacement/transposition errors</li>
   </ul>
   <li>edit distance</li>
   <li>panic mode of error recovery</li>
  </ul>

 </ul>


 <h2>4. Language Theory Background</h2>
 <ul>
  <li>Symbol (character, letter)</li>
  <li>Alphabet: a finite nonempty set of characters</li>
  <ul>
   <li>Examples: {0, 1}, ASCII, Unicode</li>
  </ul>
  <li>String (sentence, word): a finite sequence of characters, possibly empty.</li>
  <li>Language: a (countable) set of strings, possibly empty.</li>
  <li>Operations on strings</li>
  <ul>
   <li>concatenation</li>
   <li>exponentiation</li>
   <ul>
    <li><i>x</i><sup>0</sup> is the empty string &#949;.</li>
    <li><i>x<sup>i</sup></i> = <i>x<sup>i</sup></i><sup>-1</sup><i>x</i>, for <i>i</i> &gt; 0</li>
   </ul>
   <li>prefix, suffix, substring, subsequence</li>
  </ul>
  <li>Operations on languages</li>
  <ul>
   <li>union</li>
   <li>concatenation</li>
   <li>exponentiation</li>
   <ul>
    <li><i>L</i><sup>0</sup> is { &#949; }, even when <i>L</i>
        is the empty set.</li>
    <li><i>L<sup>i</sup></i> = <i>L<sup>i</sup></i><sup>-1</sup><i>L</i>, for <i>i</i> &gt; 0</li>
   </ul>
   <li>Kleene closure</li>
   <ul>
    <li><i>L*</i> = <i>L</i><sup>0</sup> &#8746; <i>L</i><sup>1</sup>
                    &#8746; &#8230;</li>
    <li>Note that <i>L*</i> always contains the empty string.</li>
   </ul>
  </ul>
 </ul>

 <h2>5. Regular Expressions</h2>
 <ul>
  <li>A regular expression is a notation for specifying a set of strings.</li>
  <li>Many of today's programming languages use regular expressions to match
      patterns in strings.</li>
  <ul>
   <li>E.g., awk, flex, lex, java, javascript, perl, python</li>
  </ul>

  <li>Definition of a regular expression and the language it denotes</li>
  <ul>
   <li>Basis</li>
   <ul>
    <li>&#949; is a regular expression that denotes { &#949; }.</li>
    <li>A single character <i>a</i> is a regular expression that denotes { <i>a</i> }.</li>
   </ul>

   <li>Induction: suppose <i>r</i> and <i>s</i> are regular expressions that
       denote the languages L(<i>r</i>) and L(<i>s</i>).</li>
   <ul>
    <li>(<i>r</i>)|(<i>s</i>) is a regular expression that denotes
        L(<i>r</i>) &#8746; L(<i>s</i>).</li>

    <li>(<i>r</i>)(<i>s</i>) is a regular expression that denotes
        L(<i>r</i>)L(<i>s</i>).</li>
    <li>(<i>r</i>)* is a regular expression that denotes
        L(<i>r</i>)*.</li>

    <li>(<i>r</i>) is a regular expression that denotes
        L(<i>r</i>).</li>
   </ul>
   <li>We can drop redundant parenthesis by assuming</li>
   <ul>
    <li>the Kleene star operator
        * has the highest precedence and is left associative</li>

    <li>concatenation
        has the next highest precedence and is left associative</li>
    <li>the union operator
        | has the lowest precedence and is left associative</li>
   </ul>
   <li>E.g., under these rules r|s*t is interpreted as (r)|((s)*(t)).</li>
   <li>Extensions of regular expressions</li>
   <ul>
    <li>Positive closure: <i>r</i>+ = <i>r</i><i>r</i>*</li>

    <li>Zero or one instance: <i>r</i>? = &#949; | <i>r</i>
    </li><li>Character classes:
    <ul>
     <li>[abc] = a | b | c</li>
     <li>[0-9] = 0 | 1 | 2 | &#8230; | 9</li>

    </ul>
   </li></ul>
  </ul>

  <li>Today regular expressions come many different forms.</li>
  <ul>
   <li>The earliest and simplest are the Kleene regular expressions: See ALSU, Sect. 3.3.3.</li>
   <li>Awk and egrep extended grep's regular expressions with union and parentheses.</li>
   <li>POSIX has a standard for Unix regular expressions.</li>
   <li>Perl has an amazingly rich set of regular expression operators.</li>
   <li>Python uses pcre regular expressions.</li>
  </ul>
  <li>Lex regular expressions</li>
  <ul>
   <li>The lexical analyzer generators flex and lex use extended regular expressions
       to specify lexeme patterns making up tokens: See ALSU, Fig. 3.8, p. 127.</li>
  </ul>
 </ul>

 <h2>6. Tokens/Patterns/Lexemes/Attributes</h2>
 <ul>
  <li>a <i>token</i> is a pair consisting of a token name and
      an optional attribute value.</li>
  <ul>
   <li>e.g., &lt;id, ptr to symbol table&gt;, &lt;=&gt;</li>
  </ul>
  <li>a <i>pattern</i> is a description of the form that the
      lexemes making up a token in a source program may have.</li>
  <ul>
   <li>We will use regular expressions to denote patterns.</li>
   <li>e.g., identifiers in C: <code>[_A-Za-z][_A-Za-z0-9]*</code></li>
  </ul>
  <li>a lexeme is a sequence of characters that matches the pattern for a
      token, e.g.,</li>
  <ul>
   <li>identifiers: <code>count, x1, i, position</code></li>
   <li>keywords: <code>if</code></li>
   <li>operators: <code>=, ==, !=, +=</code></li>
  </ul>
  <li>an <i>attribute</i> of a token is usually a pointer to the symbol
      table entry that gives additional information about the token,
      such as its type, value, line number, etc.</li>
 </ul>

<h2>7. Practice Problems</h2>
 <ol>
  <li>What language is denoted by the following regular expressions?</li>
  <ol style="list-style-type: lower-alpha;">
   <li><code>(a*b*)*</code></li>
   <li><code>a(a|b)*a</code></li>
   <li><code>(aa|bb)*((ab|ba)(aa|bb)*(ab|ba)(aa|bb)*)*</code></li>
   <li><code>a(ba|a)*</code></li>
   <li><code>ab(a|b*c)*bb*a</code></li>
  </ol>
  <li>Construct Lex-style regular expressions for the following patterns.</li>
  <ol style="list-style-type: lower-alpha;">
   <li>All lowercase English words with the five vowels in order.</li>
   <li>All lowercase English words with exactly one vowel.</li>
   <li>All lowercase English words beginning and ending with the substring "ad".</li>
   <li>All lowercase English words in which the letters are in strictly increasing
       alphabetic order.</li>
   <li>Strings of the form <code>abxba</code> where <code>x</code>
       is a string of <code>a</code>&#8217;s, <code>b</code>&#8217;s, and
       <code>c</code>&#8217;s that does not contain <code>ba</code> as a substring.</li>
  </ol>
 </ol>

 <h2>8. Reading Assignment</h2>
 <ul>
  <li>ALSU: Ch. 1, Sects. 3.1-3.3</li>
  <li>See <a href="http://dinosaur.compilertools.net/">
      The Lex &amp; Yacc Page</a>
      for lex, flex, yacc and bison tutorials and manuals.</li>
  <li>See <a href="http://javadude.com/articles/antlr3xtut/">
      ANTLR 3.x</a>
      for an antlr video tutorial.</li>
 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>
</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 4: February 4, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 4: Regular Expressions and Lexical Analysis<br/>
  February 4, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Regular expressions</li>
  <li>Lex regular expressions</li>
  <li>Specifying a lexical analyzer with Lex</li>
  <li>Example Lex programs</li>
  <li>Creating a lexical processor with Lex</li>
  <li>Lex history</li>
 </ol>


 <h2>1. Regular Expressions</h2>
 <ul>
  <li>Regular expressions in various forms are used in many programming languages
      and software tools to specify patterns and match strings.</li>
  <li>Regular expressions are well suited for matching lexemes in
      programming languages.</li>
  <li>In formal language theory regular expressions use a finite alphabet of symbols and the
      operators union, concatenation, and Kleene closure. They define the regular languages.</li>
  <li>Unix programs like egrep, awk, and Lex extend this simple notation
      with additional operators and shorthands.</li>
  <li>The POSIX (Portable Operating System Inteface for Unix) standard defines two
      flavors of regular expressions for Unix systems: Basic Regular Expressions
      and Extended Regular Expressions.</li>
  <li>Perl has amazingly rich regular expressions which further extend the
      egrep, awk, and Lex regular expressions. Perl compatible regular expressions
      have been adopted by Java, JavaScript, PHP, Python, and Ruby.</li>
  <li>The back-referencing operator in Perl regular expressions allows nonregular languages
      to be recognized and makes the pattern-matching problem NP-complete.
 </li></ul>

 <h2>2. Lex Regular Expressions</h2>
 <ul>
  <li>The declarative language Lex has been
      widely used for creating many useful lexical analysis tools
      including lexers.</li> 
  <li>The following symbols in Lex regular expressions have special meanings:</li>
  <ul>
   <dt><code>\ " . ^ $ [ ] * + ? { } | ( ) /</code></dt>
   <dt>To turn off their special meaning, precede the symbol by <code>\</code>.</dt>
   <ul>
    <li>Thus, <code>\*</code> matches <code>*</code>.</li>
    <li><code>\\</code> matches <code>\</code>.</li>
   </ul>
  </ul>
  <li>Examples of Lex regular expressions and the strings they match.</li>
  </ul><ol>
  <li><code>"a.*b"</code> matches the string <code>a.*b</code>.</li>
  <li><code>.</code> matches any character except a newline.</li>
  <li><code>^</code> matches the empty string at the beginning of a line.</li>
  <li><code>$</code> matches the empty string at the end of a line.</li>
  <li><code>[abc]</code> matches an <code>a</code>, or a <code>b</code>, or a <code>c</code>.</li>
  <li><code>[a-z]</code> matches any lowercase letter between <code>a</code>
      and <code>z</code>.</li>
  <li><code>[A-Za-z0-9]</code> matches any alphanumeric character.</li>
  <li><code>[^abc]</code> matches any character except an
      <code>a</code>, or a <code>b</code>, or a <code>c</code>.</li>
  <li><code>[^0-9]</code> matches any nonnumeric character.</li>
  <li><code>a*</code> matches a string of zero or more <code>a</code>'s.</li>
  <li><code>a+</code> matches a string of one or more <code>a</code>'s.</li>
  <li><code>a?</code> matches a string of zero or one <code>a</code>'s.</li>
  <li><code>a{2,5}</code> matches any string consisting of two to five <code>a</code>'s.</li>
  <li><code>(a)</code> matches an <code>a</code>.</li>
  <li><code>a/b</code> matches an <code>a</code> when followed by a <code>b</code>.</li>
  <li><code>\n</code> matches a newline.</li>
  <li><code>\t</code> matches a tab.</li>
  </ol>
  <li>Lex chooses the longest match if there is more than one match.
      E.g., <code>ab*</code> matches the prefix <code>abb</code>
      in <code>abbc</code>.</li>
 

 <h2>3. Specifying a Lexical Analyzer with Lex</h2>
 <ul>
  <li>Lex is a special-purpose programming language for creating programs
      to process streams of input characters.</li>
  <li>Lex has been widely used for contructing lexical analyzers.</li>
  <li>A Lex program has the following form:</li>
  </ul><pre>	declarations
	%%
	translation rules
	%%
	auxiliary functions
  </pre>
  <li>The declarations section can contain declarations of variables,
      manifest constants, and regular definitions. The declarations
      section can be empty.</li>
  <li>The translation rules are each of the form</li>
  <pre>	pattern	{action}
  </pre>

  <ul>
   <li>Each pattern is a regular expression which may use regular definitions
       defined in the declarations section.</li>
   <li>Each action is a fragment of C-code.</li>
  </ul>

  <li>The auxiliary functions section starting with the second %% is optional.
      Everything in this section is copied directly to the file <code>lex.yy.c</code>
      and can be used in the actions of the translation rules.</li>
 

 <h2>4. Example Lex programs</h2>
 <h4>Example 1: Lex program to print all words in an input stream</h4>
  <ul>
  <li>The following Lex program will print all alphabetic words in an input stream:</li>
  </ul><pre><code>
	%%
	[A-Za-z]+	{ printf("%s\n", yytext); }
	.|\n		{ }
  </code></pre>

   <li>The pattern part of the first translation rule says that if the
       current prefix of the unprocessed input stream consists of a sequence of one or more letters,
       then the longest such prefix is matched and assigned to the Lex string variable
       <code>yytext</code>. 
       The action part of the first translation rule prints the prefix that was matched.
       If this rule fires, then the matching prefix is removed
       from the beginning of the unprocessed input stream.</li>
   <li>The dot in pattern part of the second translation rule matches any character except
       a newline at the beginning of the unprocessed input stream.  The <code>\n</code>
       matches a newline at the beginning of the unprocessed input stream.
       If this rule fires, then the character of the beginning of the unprocessed
       input stream is removed.
       Since the action is empty, no output is generated.</li>
   <li>Lex repeated applies these two rules until the input stream is exhausted.</li>
  

 <h4>Example 2: Lex program to print number of words, numbers, and lines in a file</h4>
  <ul>
  </ul><pre><code>

         int num_words = 0, num_numbers = 0, num_lines = 0;
word     [A-Za-z]+
number   [0-9]+
%%
{word}   {++num_words;}
{number} {++num_numbers;}
\n       {++num_lines; }
.        { }
%%
int main()
{
  yylex();
  printf("# of words = %d, # of numbers = %d, # of lines = %d\n",
         num_words, num_numbers, num_lines );
}

  </code></pre>

  


 <h4>Example 3: Lex program for some typical programming language tokens</h4>
 <ul>
  <li>See ALSU, Fig. 3.23, p. 143.</li>
  </ul><pre><code>

%{ /* definitions of manifest constants */
   LT, LE,
   IF, ELSE, ID, NUMBER, RELOP */

/* regular definitions */
delim	 [ \t\n]
ws	 {delim}+
letter	 [A-Za-z]
digit    [0-9]
id	 {letter}({letter}|{digit})*
number	 {digit}+(\.{digit}+)?(E[+-]?{digit}+)?
%%
{ws}	 { }
if	 {return(IF);}
else	 {return(ELSE);}
{id}	 {yylval = (int) installID(); return(ID);}
{number} {yylval = (int) installNum(); return(NUMBER);}
"&lt;"	 {yylval = LT; return(RELOP); }
"&lt;="	 {yylval = LE; return(RELOP); }
%%
int installID()
{
  /* function to install the lexeme, whose first character
     is pointed to by yytext, and whose length is yyleng,
     into the symbol table; returns pointer to symbol table
     entry */
}

int installNum() {
  /* analogous to installID */
}

  </code></pre>

 
 
 

 <h2>5. Creating a Lexical Processor with Lex</h2>
 <ul>
  <li>Put lex program into a file, say <code>file.l</code>.</li>
  <li>Compile the lex program with the command:</li> 
</ul><pre><code>
lex file.l
</code></pre>
      <dt>This command produces an output file <code>lex.yy.c.
  </code><li>Compile this output file with the C compiler and the lex library
      <code>-ll</code>:</li>
<pre><code>
gcc lex.yy.c -ll
</code></pre>
  </dt><dt>The resulting <code>a.out</code> is the lexical processor.</dt>
 
 

 <h2>6. Lex History</h2>
 <ul>
  <li>The initial version of Lex was written by Michael Lesk at
      Bell Labs to run on Unix.</li>
  <li>The second version of Lex with more efficient regular expression
      pattern matching was written by Eric Schmidt at Bell Labs.</li>
  <li>Vern Paxson wrote the POSIX-compliant variant of Lex, called Flex, at Berkeley.</li>
  <li>All versions of Lex use variants of the regular-expression pattern-matching
      technology described in Chapter 3 of ALSU.</li>
  <li>Today, many versions of Lex use C, C++, C#, Java, and other languages
      to specify actions.</li>
 </ul>

 <h2>7. Practice Problems</h2>
 <ol>
  <li>Write a Lex program that copies a file, replacing each nonempty sequence
      of whitespace consisting of blanks, tabs, and newlines by a single blank.</li>
  <li>Write a Lex program that converts a file of English text to "Pig Latin."
      Assume the file is a sequence of words separated by whitespace. If a word
      begins with a consonant, move the consonant to the end
      of the word and add "ay". (E.g.,
      "pig" gets mapped into "igpay".) If a word begins with a vowel, just add "ay"
      to the end. (E.g., "art" gets mapped to "artay".)
  </li></ol>  

 <h2>8. Reading Assignment</h2>
 <ul>
  <li>ALSU, Sects. 3.3, 3.5</li>
  <li>See <a href="http://dinosaur.compilertools.net/">
      The Lex &amp; Yacc Page</a>
      for Lex and Flex tutorials and manuals.</li>

 </ul><br/>



<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<!-- Homework Assignment #1                         -->
<!-- Created by Al Aho on 2/1/2013                  -->
<!-- Last modified on 2/1/2013                      -->
<!-- ------------------------------------------------->
<title>Homework #1</title></head>
<body>
 <h1 style="text-align: left;"> COMS W4115
  <br/>Programming Languages and Translators
  <br/>Homework Assignment #1
  <br/>Submit solutions electronically on
  <br/>&#160;&#160;&#160;&#160;&#160;Courseworks/COMSW4115/Assignments
  <br/>&#160;&#160;&#160;&#160;&#160;by 2:40pm, February 13, 2013
 </h1>
 <hr/>

<h2>Instructions</h2>
 <ul>
  <li>You may discuss the questions with others but your answers
      must be your own words and your own code.</li>
  <li>Problems 1-4 are each worth 25 points.</li>
  <li>Solutions to these problems will be posted
      on Courseworks on February 20, 2013.</li>
  <li>This assignment may submitted electronically on Courseworks by 2:40pm,
      February 20, 2013 for 50% credit.</li>
 </ul>

<h2>Problems</h2>
 <ol>

  <li>Construct Lex-style regular expressions for the following patterns.
      For each pattern, create a Lex program to find the first longest lowercase word
      in the dictionary satisfying that pattern. As part of your
      answer list your program and the longest word found.  State what dictionary
      you used (e.g., on Linux systems <code>/usr/dict/words</code>).
      You can use any variant of Lex such as Flex, JLex, Ocamllex, PLY, etc.</li>
  <ol style="list-style-type: lower-alpha;">
   <li>All lowercase English words that can be made up using only the letters
       in your first name. Each letter in your name can be used zero or more times
       and in any order in a word.</li>
   <li>All lowercase English words that can be made up using exactly the letters
       in your first name. Each letter in your name must be used exactly the
       number of times it appears in your name but the letters can appear
       in any order in a word. If your name
       has more than five letters, use the prefix of length five.</li>
  </ol>
  <br/>

  <li>Let <i>L</i> be the language consisting of all
      strings of <i>a</i>'s, <i>b</i>'s, and
      <i>c</i>'s such that each string is of the form
      <i>abxba</i> where <i>x</i> does not contain <i>ba</i> as a substring.
      These strings model comments in the programming language C.
  </li>

  <ol style="list-style-type: lower-alpha;">
   <li>Write down a deterministic finite automaton
       (DFA) for <i>L</i>.</li>
   <li>Minimize the number states in your DFA. Briefly explain how you did the minimization.</li>
   <li>Explain what property of the scanned input each state of your minimized DFA recognizes.</li>
   <li>Write down a regular expression for <i>L</i>.
  </li></ol>
  <br/>

  <li>Let <i>L</i> be the language consisting of just balanced parentheses,
      i.e., {<code>&#949;, ( ), ( ( ) ), ( ) ( ), ( ( ( ) ) ), ( ( ) ( ) ), ...</code> }
  </li>

  <ol style="list-style-type: lower-alpha;">
   <li>Write down a recursive definition for <i>L</i>.</li>
   <li>Use the pumping lemma for regular languages to show that <i>L</i>
       cannot be specified by a regular expression.</li>
  </ol>

  <br/>

  <li>Let <i>R</i> be a regular expression of length <i>m</i> and let <i>w</i> be an input string
      of length <i>n</i>. Briefly discuss in terms
      of <i>m</i> and <i>n</i> the time-space complexity of the McNaughton-Yamada-Thompson
      algorithm  to
      determine whether <i>w</i> is in <i>L</i>(<i>R</i>).</li> 

 </ol>
<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address> 
</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 5: February 6, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 5: Implementing a Lexical Analyzer<br/>
  February 6, 2013
 </h1>

 <h2>Outline</h2>
 <ol>
  <li>Finite automata</li>
  <li>Converting an NFA to a DFA</li>
  <li>Equivalence of regular expressions and finite automata</li>
  <li>Simulating an NFA</li>
  <li>The pumping lemma for regular languages</li>
  <li>Closure and decision properties of regular languages</li>
 </ol>


 <h2>1. Finite Automata</h2>
 <ul>
  <li>Variants of finite automata are commonly used to match regular expression patterns.</li>
  <li>A nondeterministic finite automaton (NFA) consists of</li>
   <ul>
    <li>A finite set of states <i>S</i>.</li>
    <li>An input alphabet consisting of a finite set of symbols &#931;.</li>
    <li>A transition function &#948; that maps <i>S</i> &#215; (&#931; &#8746; {&#949;})
        to subsets of <i>S</i>. This transition function can be represented
        by a transition graph in which the nodes are labeled by states
        and there is a directed edge labeled <i>a</i> from node <i>w</i> to node <i>v</i>
        if &#948;(<i>w</i>, <i>a</i>) contains <i>v</i>.</li>
    <li>An initial state <i>s</i><sub>0</sub> in <i>S</i>.</li>
    <li><i>F</i>, a subset of <i>S</i>, called the final (or accepting) states.</li>
   </ul>
  <li>An NFA accepts an input string <i>x</i> iff there is a path in the transition
      graph from the initial state to a final state that spells out <i>x</i>.</li>
  <li>The language defined by an NFA is the set of strings accepted by the NFA.</li>
  <li>A deterministic finite automaton (DFA) is an NFA in which</li>
  </ul><ol>
   <li>There are no &#949; moves, and</li>
   <li>For each state <i>s</i> and input symbol <i>a</i> there is exactly one transition
       out of <i>s</i> labeled <i>a</i>.</li>
  </ol>
 

 <h2>2. Converting an NFA to a DFA</h2>
 <ul>
  <li>Every NFA can be converted to an equivalent DFA using the subset construction
      (Algorithm 3.20, ALSU, pp. 153-154).</li>
  <li>Every DFA can be converted into an equivalent minimum-state DFA
      Using Algorithm 3.39, ALSU, pp. 181-183.
      All equivalent minimum-state DFAs are isomorphic up to state renaming.</li>
 </ul>

 <h2>3. Equivalence of Regular Expressions and Finite Automata</h2>
 <ul>
  <li>Regular expressions and finite automata define the same
      class of languages, namely the regular sets.</li>
  <li>Every regular expression can be converted into an equivalent
      NFA using the McNaughton-Yamada-Thompson algorithm
      (Algorithm 3.23, ALSU, pp. 159-161).</li>
  <li>Every finite automaton can be converted into a regular expression
      using Kleene's algorithm.</li>
 </ul>

 <h2>4. Simulating an NFA</h2>
 <ul>
  <li>Two-stack simulation of an NFA: Algorithm 3.22, ALSU, pp. 156-159.</li>
 </ul>

 <h2>5. The Pumping Lemma for Regular Languages</h2>
 <ul>
  <li>The pumping lemma allows us to prove certain languages, like
      { <code>a</code><sup><i>n</i></sup><code>b</code><sup><i>n</i></sup> |
      <i>n</i> &#8805; 0 }, are not regular.</li>
  <li><b>The pumping lemma.</b> If L is a regular language, then there exists a
      constant <i>n</i> associated with L such that for every string <i>w</i> in L where
      |<i>w</i>| &#8805; <i>n</i>, we can partition <i>w</i> into three strings
      <i>xyz</i> (i.e., <i>w</i> = <i>xyz</i>) such that
  <ul>
   <li><i>y</i> is not the empty string,</li>
   <li>the length of <i>xy</i> is less than or equal to <i>n</i>, and</li>
   <li>for all <i>k</i> &#8805; 0, the string <i>xy<sup>k</sup>z</i> is in L.</li>
  </ul>
 </li></ul>

 <h2>6. Closure and Decision Properties of Regular Languages</h2>
 <ul>
  <li>The regular languages are closed under the following operations:</li>
  <ul>
   <li>union</li>
   <li>intersection</li>
   <li>complement</li>
   <li>reversal</li>
   <li>Kleene star</li>
   <li>homomorphism</li>
   <li>inverse homomorphism</li>
  </ul>
  <li>Decision properties</li>
  <ul>
   <li>Given a regular expression <i>r</i> and a string <i>w</i>, it is decidable
       whether <i>r</i> matches <i>w</i>.</li>
   <li>Give a finite automaton A, it is decidable whether L(A) is empty.</li>
   <li>Given two finite automata A and B, it is decidable whether L(A) = L(B).</li>
  </ul>
 </ul>


 <h2>7. Practice Problems</h2>
 <ol>
  <li>Write down deterministic finite automata for the following regular expressions:</li>
  <ol style="list-style-type: lower-alpha;">
   <li><code>(a*b*)*</code></li>
   <li><code>(aa|bb)*((ab|ba)(aa|bb)*(ab|ba)(aa|bb)*)*</code></li>
   <li><code>a(ba|a)*</code></li>
   <li><code>ab(a|b*c)*bb*a</code></li>
  </ol>
  <li>Construct a deterministic finite automaton that will recognize all strings
      of 0's and 1's representing integers that are divisible by 3.
      Assume the empty string represents 0.</li>
  <li>Use the McNaughton-Yamada-Thompson algorithm to convert the regular
      expression <code>a(a|b)*a</code> into a nondeterministic finite automaton.</li>
  <li>Convert the NFA of (3) into a DFA.</li>
  <li>Minimize the number of states in the DFA of (4).</li>
 </ol>

 <h2>8. Reading Assignment</h2>
 <ul>
  <li>ALSU Chapter 3, all sections except 3.9.</li>
  <li>Russ Cox's article <a href="http://swtch.com/%7Ersc/regexp/regexp1.html">
      Regular Expression Matching Can Be Simple and Fast (but is slow in
      Java, Perl, PHP, Python, Ruby, ...)</a> has a good historical account on
      the evolution of regular expression matching programs.</li>
      
 </ul><br/>



<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 6: February 11, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 6: Context-Free Grammars<br/>
  February 11, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ul>
  <li>Context-free grammars</li>
  <li>Derivations and parse trees</li>
  <li>Ambiguity</li>
  <li>Examples of context-free grammars</li>
  <li>Yacc: a language for specifying syntax-directed translators</li>  
 </ul>


 <h2>1. Context-Free Grammars (CFG's)</h2>
 <ul>
  <li>CFG's are very useful for representing the syntactic structure
      of programming languages.</li>
  <li>A CFG is sometimes called Backus-Naur Form (BNF).</li>
  <li>A context-free grammar consists of</li>
  </ul><ol>
   <li>A finite set of terminal symbols,</li>
   <li>A finite nonempty set of nonterminal symbols,</li>
   <li>One distinguished nonterminal called the start symbol, and</li>
   <li>A finite set of rewrite rules, called productions, each of the form
       A &#8594; &#945;
       where A is a nonterminal and &#945; is a string (possibly empty)
           of terminals and nonterminals.</li>
  </ol>
  <br/>
  
 <li>Consider the context-free grammar G with the productions</li>
  <pre><code>
      E &#8594; E + T | T
      T &#8594; T * F | F
      F &#8594; ( E ) | id
  </code></pre>
 <ul>
  <li>The terminal symbols are the alphabet from which strings are formed.
      In this grammar the set of terminal symbols is
      { id, +, *, (, ) }.  The terminal symbols are the token names.</li>
  <li>The nonterminal symbols are syntactic variables that denote sets
      of strings of terminal symbols. In this grammar the set of nonterminal
      symbols is { <code>E</code>, <code>T</code>, <code>F</code>}.</li>
  <li>The start symbol is <code>E</code>.</li>
 </ul>
 
 
 <h2>2. Derivations and Parse Trees</h2>
 <ul>
  <li><i>L</i>(G), the language generated by a grammar G, consists of all strings of
    terminal symbols that can be derived from the start symbol of G.</li>
 
  <li>A leftmost derivation expands the leftmost nonterminal in
      each sentential form:</li>
  </ul><pre><code>
      E &#8658; E + T
        &#8658; T + T
        &#8658; F + T
        &#8658; id + T
        &#8658; id + T * F
        &#8658; id + F * F
        &#8658; id + id * F
        &#8658; id + id * id
  </code></pre>
 
  <li>A rightmost derivation expands the rightmost nonterminal in each sentential form:</li>
  <pre><code>
      E &#8658; E + T
        &#8658; E + T * F
        &#8658; E + T * id
        &#8658; E + F * id
        &#8658; E + id * id
        &#8658; T + id * id
        &#8658; F + id * id
        &#8658; id + id * id
  </code></pre>
  <li>Note that these two derivations have the same parse tree.</li>
 
 
 <h2>3. Ambiguity</h2>
 <ul>
 <li>Consider the context-free grammar G with the productions</li>
  </ul><pre><code>
      E &#8594; E + E | E * E | ( E ) | id
  </code></pre>
 
 <li>This grammar has the following leftmost derivation for
     <code>id + id * id</code>
  <pre><code>
      E &#8658; E + E
        &#8658; id + E
        &#8658; id + E * E
        &#8658; id + id * E
        &#8658; id + id * id
  </code></pre>
 </li>
 
 <li>This grammar also has the following leftmost derivation for
     <code>id + id * id</code>
  <pre><code>
      E &#8658; E * E
        &#8658; E + E * E
        &#8658; id +  E * E
        &#8658; id + id * E
        &#8658; id + id * id
  </code></pre>
  </li>
 
  <li>These derivations have different parse trees.</li>
  <li>A grammar is <i>ambiguous</i> if there is a sentence with two
      or more parse trees.</li>
  <li>The problem is that the grammar above does not specify</li>
  <ul>
   <li>the precedence of the + and * operators, or</li>
   <li>the associativity of the + and * operators</li>
  </ul>
  <li>However, the grammar in section (3) generates the same language
      and is unambiguous because
      it makes * of higher precedence than +, and makes both operators
      left associative.</li>
  <li>A context-free language is <i>inherently ambiguous</i> if it
      cannot be generated by any unambiguous context-free grammar.</li>
  <li>The context-free language
      { <code>a<sup><i>m</i></sup>b<sup><i>m</i></sup>a<sup><i>n</i></sup>b<sup><i>n</i></sup></code>
      | <i>m</i> &gt; 0 and <i>n</i> &gt; 0} &#8746;
      { <code>a<sup><i>m</i></sup>b<sup><i>n</i></sup>a<sup><i>n</i></sup>b<sup><i>m</i></sup></code>
      | <i>m</i> &gt; 0 and <i>n</i> &gt; 0}
      is inherently ambiguous.</li>
  <li>Most (all?) natural languages are inherently ambiguous but no
      programming languages are inherently ambiguous.</li>
  <li>Unfortunately, there is no algorithm to determine whether a CFG is ambiguous;
      that is, the problem of determining whether a CFG is ambiguous is undecidable.</li>
  <li>We can, however, give some practically useful sufficient conditions to guarantee that a CFG
      is unambiguous.</li>
 

 <h2>4. Examples of Context-Free Grammars</h2>
 <ul>
   <li>Nonempty palindromes of <code>a</code>'s and <code>b</code>'s.
       (A palindrome is a string that reads the same forwards as backwards;
        e.g., <code>abba</code>.)</li>
   <dt>CFG: <code>S &#8594; a S b | b S a | a a | b b | a | b</code></dt>
   <dt>Note that the language generated by this grammar is not regular.
       Can you prove this using the pumping lemma for regular languages?</dt><br/>

   <li>Strings with an equal number of <code>a</code>'s and <code>b</code>'s:</li>
   <dt>CFG: <code>S &#8594; a S a | b S b | S S | &#949;</code></dt>
   <dt>Note that this grammar is ambiguous.
       Can you find an equivalent unambiguous grammar?</dt><br/>

   <li>If- and if-else statements:</li>
   </ul><pre><code>
      stmt &#8594; if ( expr ) stmt else stmt
           | if (expr) stmt
           | other
   </code></pre>
   <dt>Note that this grammar is ambiguous.</dt><br/>

   <li>Some typical programming language constructs:</li>
   <pre><code>
      stmt &#8594; expr ;
           | if (expr) stmt
           | for ( optexpr; optexpr; optexpr;) stmt
           | other
      optexpr &#8594; &#949;
           | expr
   </code></pre>

  

 <h2>5. Yacc: a Language for Specifying Syntax-Directed Translators</h2>
 <ul>
  <li>Yacc is popular language, created by
      Steve Johnson of Bell Labs, for implementing syntax-directed
      translators.</li>
  <li>Bison is a gnu version of Yacc, upwards compatible with the original Yacc,
      written by Charles Donnelly and Richard Stallman.
      Many other versions of Yacc are also available.</li>
  <li>The original Yacc used C for semantic actions. Yacc has been rewritten for
      many other languages including Java, ML, OCaml, and Python.</li>
  <li>Yacc specifications</li>
  <ul>
   <li>A Yacc program has three parts:</li>

   </ul></ul><pre>      <i>declarations</i>
      <code>%%</code>
      <i>translation rules</i>
      <code>%%</code>
      <i>supporting C-routines</i>
   </pre>

   <dt>The declarations part may be empty and the last part (<code>%%</code>
       followed by the supporting C-routines) may be omitted.</dt><br/><br/>
  

  <li>Here is a Yacc program for a desk calculator
      that adds and multiplies numbers.
      (See ALSU, p. 292, Fig. 4.59 for a more advanced desk calculator.)</li>

  <pre><code>
      %{
      #include &lt;ctype.h&gt;

      #include &lt;stdio.h&gt;
      #define YYSTYPE double
      %}

      %token NUMBER
      %left '+'
      %left '*'

      %%

      lines : lines expr '\n'   { printf("%g\n", $2); }
            | lines '\n'
            | /* empty */
            ;

      expr  : expr '+' expr      { $$ = $1 + $3; }
            | expr '*' expr      { $$ = $1 * $3; }
            | '(' expr ')'       { $$ = $2; }
            | NUMBER
            ;

      %%
      /* the lexical analyzer; returns &lt;token-name, yylval&gt; */
      int yylex() {
        int c;
        while ((c = getchar()) == ' ');
        if ((c == '.') || (isdigit(c))) {
          ungetc(c, stdin);
          scanf("%lf", &amp;yylval);
          return NUMBER;
        }
        return c;
      }
  </code></pre>

  <li>The declarations</li>
  <dt><code>%left '+'</code></dt>
  <dt><code>%left '*'</code></dt>
  <dt>make the operator <code>+</code> left associative and of lower
      precedence than the left-associative operator <code>*</code>.</dt>


  <li>On Linux, we can make a desk calculator from this Yacc program
      as follows:</li><br/>
   <ol>
    <li>Put the yacc program in a file, say <code>desk.y</code>.</li>
    <li>Invoke <code>yacc desk.y</code> to create the yacc output file <code>y.tab.c</code>.</li>
    <li>Compile this output file with a C compiler by typing <code>gcc y.tab.c -ly</code>
        to get <code>a.out</code>.
        (The library -ly contains the Yacc parsing program.)</li>
    <li><code>a.out</code> is the desk calculator. Try it!</li>
   </ol><br/>
  
 
 <h2>6. Practice Problems</h2>
 <ol>
  <li>Let G be the grammar
      S &#8594; a S b S | b S a S | &#949;.
  <ol style="list-style-type: lower-alpha;">
   <li>What language is generated by this grammar?</li>
   <li>Draw all parse trees for the sentence <code>abab</code>.</li>
   <li>Is this grammar ambiguous?</li>
  </ol>
  </li>
  <li>Let G be the grammar
      S &#8594; a S b | &#949;.
      Prove that <i>L</i>(G) = 
      { <code>a</code><sup><i>n</i></sup><code>b</code><sup><i>n</i></sup> | <i>n</i> &#8805; 0 }.
  </li>
  <li>Consider a sentence of the form <code>id + id + ... + id</code> where there are
      <i>n</i> plus signs. Let G be the grammar in section (3) above.
      How many parse trees are there in G for this sentence when <i>n</i> equals</li>
  <ol style="list-style-type: lower-alpha;">
   <li>1</li>
   <li>2</li>
   <li>3</li>
   <li>4</li>
   <li><i>m</i>?</li>
  </ol>
  <li>Write down a CFG for regular expressions over the alphabet {<code>a</code>, <code>b</code>}.
      Show a parse tree for the regular expression <code>a | b*a</code>.</li>
 </ol>



 <h2>7. Reading</h2>
 <ul>
  <li>ALSU Sects. 4.1-4.2, 4.9</li>
  <li><a href="http://www.scribd.com/doc/8669780/Lex-yacc-Tutorial">
        A nice Lex &amp; Yacc tutorial</a></li>
 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 7: February 13, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 7: Parsing Context-Free Grammars<br/>
  February 13, 2013
 </h1>

 <h2>Outline</h2>
 <ol>
  <li>Yacc: a language for specifying syntax-directed translators</li>
  <li>The pumping lemma for context-free languages</li>
  <li>The parsing problem for context-free grammars</li>
  <li>Top-down parsing</li>
  <li>Transformations on grammars</li>
 </ol>


 <h2>1. Yacc: a Language for Specifying Syntax-Directed Translators</h2>
 <ul>
  <li>Yacc is popular language, first implemented by
      Steve Johnson of Bell Labs, for implementing syntax-directed
      translators.</li>
  <li>Bison is a gnu version of Yacc, upward compatible with the original Yacc,
      written by Charles Donnelly and Richard Stallman.
      Many other versions of Yacc are also available.</li>
  <li>The original Yacc used C for semantic actions. Yacc has been rewritten for
      many other languages including Java, ML, OCaml, and Python.</li>
  <li>Yacc specifications</li>
  <ul>
   <li>A Yacc program has three parts:</li>

   </ul></ul><pre>      <i>declarations</i>
      <code>%%</code>
      <i>translation rules</i>
      <code>%%</code>
      <i>supporting C-routines</i>
   </pre>

   <dt>The declarations part may be empty and the last part (<code>%%</code>
       followed by the supporting C-routines) may be omitted.</dt><br/><br/>
  

  <li>Here is a Yacc program for a desk calculator
      that adds and multiplies numbers.
      (From ALSU, p. 292, Fig. 4.59, a more advanced desk calculator.)</li>
  <pre><code>
      %{
      #include &lt;ctype.h&gt;

      #include &lt;stdio.h&gt;
      #define YYSTYPE double
      %}

      %token NUMBER
      %left '+'
      %left '*'

      %%

      lines : lines expr '\n'   { printf("%g\n", $2); }
            | lines '\n'
            | /* empty */
            ;

      expr  : expr '+' expr      { $$ = $1 + $3; }
            | expr '*' expr      { $$ = $1 * $3; }
            | '(' expr ')'       { $$ = $2; }
            | NUMBER
            ;

      %%
      /* the lexical analyzer; returns &lt;token-name, yylval&gt; */
      int yylex() {
        int c;
        while ((c = getchar()) == ' ');
        if ((c == '.') || (isdigit(c))) {
          ungetc(c, stdin);
          scanf("%lf", &amp;yylval);
          return NUMBER;
        }
        return c;
      }
  </code></pre>

  <li>On Linux, we can make a desk calculator from this Yacc program
      as follows:</li><br/>
   <ol>
    <li>Put the yacc program in a file, say <code>desk.y</code>.</li>
    <li>Invoke <code>yacc desk.y</code> to create the yacc output file <code>y.tab.c</code>.</li>
    <li>Compile this output file with a C compiler by typing <code>gcc y.tab.c -ly</code>
        to get <code>a.out</code>.
        (The library -ly contains the Yacc parsing program.)</li>
    <li><code>a.out</code> is the desk calculator. Try it!</li>
   </ol><br/>
  
 

 <h2>2. The Pumping Lemma for Context-Free Languages</h2>
 <ul>
  <li>The pumping lemma for context-free languages can be used to show certain
      languages are not context free.</li>
  <li>The pumping lemma: If L is a context-free language, then there exists a
      constant <i>n</i> such that if z is any string in L of length <i>n</i> or more, then
      z can be written as uvwxy subject to the following conditions:
  <ol>
   <li>The length of vwx is less than or equal to <i>n</i>.</li>
   <li>The length of vx is one or more.  (That is, not both of v and x can be empty.)</li>
   <li>For all <i>i</i> &#8805; 0, uv<sup><i>i</i></sup>wx<sup><i>i</i></sup>y is in L.</li>
  </ol>
  </li><li>A typical proof using the pumping lemma to show a language L is not context free
      proceeds by assuming L is context free, and then finding a long string in L
      which, when pumped, yields a string not in L, thereby deriving a contradiction.</li>
  <li>Examples of non-context-free languages:</li>
  <ul>
   <li>{<code>a<sup>n</sup>b<sup>n</sup>c<sup>n</sup></code> | <code>n</code> &#8805; 0 }</li>
   <li>{<code>ww</code> | <code>w</code> is in (a|b)* }</li>
   <li>{<code>a<sup>m</sup>b<sup>n</sup>a<sup>m</sup>b<sup>n</sup></code> |
      <code>n</code> &#8805; 0 }</li>
  </ul>
 </ul>
  

 <h2>3. The Parsing Problem for Context-Free Grammars</h2>
 <ul>
  <li>The parsing problem for context-free grammars
      is given a CFG G and an input string w
      to construct all parse trees for w according to G, if w is in L(G).</li>
  <li>The Cocke-Younger-Kasami algorithm is a dynamic programming algorithm
      that given a Chomsky Normal Form grammar G and an input string w will
      create in O(|w|<sup>3</sup>) time a table from which all parse trees
      for w according to G can be constructed.</li>
  <li>For compiler applications two styles of parsing algorithms are common:
      top-down parsing and bottom-up parsing.</li>
 </ul>

 <h2>4. Top-Down Parsing</h2>
 <ul>
  <li>Top-down parsing consists of trying to construct a parse tree
      for an input string starting from the root and creating
      the nodes of the parse tree in preorder.</li>
  <li>Equivalently, top-down parsing consists of trying to find a
      leftmost derivation for the input string.</li>
  <li>Consider grammar <i>G</i>:</li>
    </ul><pre><code>
      S &#8594; + S S | * S S | a
    </code></pre>
  <li>Leftmost derivation for + a * a a:</li>
   <pre><code>
      S &#8658; + S S
        &#8658; + a S
        &#8658; + a * S S
        &#8658; + a * a S
        &#8658; + a * a a
  </code></pre>

  <li>Recursive-descent parsing</li>
  <ul>
   <li>Recursive-descent parsing is a top-down method of syntax
       analysis in which a set of recursive procedures is used
       to process the input string.</li>
   <li>One procedure is associated with each nonterminal of
       the grammar.  See Fig. 4.13, p. 219.</li>
   <li>The sequence of successful procedure calls defines the parse tree.</li>
  </ul>

  <li>Nonrecursive predictive parsing</li>
  <ul>
   <li>A nonrecursive predictive parser uses an explicit stack.
   </li><li>See Fig. 4.19, p. 227, for a model of table-driven predictive
       parser.</li>

   <li>Parsing table for <i>G</i>:</li>

    </ul><pre><code>
                        Input Symbol
   Nonterminal    a        +          *        $

       S        S &#8594; a    S &#8594; +SS    S &#8594; *SS
   </code></pre>

   <li>Moves made by this predictive parser on input <code>+a*aa</code>.
       (The top of the stack is to the left.)</li>

   <pre><code>
       Stack        Input   Output
        S$         +a*aa$
        +SS$       +a*aa$   S &#8594; +SS
        SS$         a*aa$
        aS$         a*aa$   S &#8594; a
        S$           *aa$
        *SS$         *aa$   S &#8594; *SS
        SS$           aa$
        aS$           aa$   S &#8594; a
        S$             a$
        a$             a$   S &#8594; a
        $               $        
   </code></pre>

   <li>Note that these moves trace out a leftmost derivation for the input.</li>
  
 

 <h2>5. Transformations on Grammars</h2>
 <ul>
  <li>Two common language-preserving transformations are often applied to
      grammars to try to make them parsable by top-down methods.
      These are eliminating left recursion and left factoring.</li>
  <li>Eliminating left recursion:</li>
  <ul>
   <li>Replace</li>
   </ul></ul><pre><code>
      expr &#8594;  expr + term
           |  term
   </code></pre> by

    <pre><code>
      expr  &#8594;  term expr'

      expr' &#8594;  + term expr'
            |  &#949;
   </code></pre>
  

  <li>Left factoring:</li>
  <ul>
   <li>Replace</li>
   </ul><pre><code>
      stmt &#8594; if ( expr ) stmt else stmt
           | if (expr) stmt
           | other
   </code></pre> by
   <pre><code>
      stmt  &#8594; if ( expr ) stmt stmt'
            | other

      stmt' &#8594; else stmt
            | &#949;
   </code></pre>
  
 

 <h2>6. Practice Problems</h2>
 <ol>
  <li>Write down a CFG for regular expressions over the alphabet
      {<code>a</code>, <code>b</code>}.
      Show a parse tree for the regular expression
      <code>a | b*a</code>.</li>
  <li>Using the nonterminals <code>stmt</code> and <code>expr</code>,
      design context-free grammar productions to model</li>
  <ol style="list-style-type: lower-alpha;">
   <li>C while-statements</li>
   <li>C for-statements</li>
   <li>C do-while statements</li>
  </ol>
  <li>Consider grammar <i>G</i>:</li>
    <pre><code>
      S &#8594;  S S + | S S * | a
    </code></pre>
  <ol style="list-style-type: lower-alpha;">
   <li>What language does this grammar generate?</li>
   <li>Eliminate the left recursion from this grammar.</li>
  </ol>

  <li>Use the pumping lemma to show that
      {<code>a<sup>n</sup>b<sup>n</sup>c<sup>n</sup></code> | <code>n</code> &#8805; 0 }
      is not context free.</li>
 </ol>

 <h2>7. Reading</h2>
 <ul>
  <li>ALSU, Sections 4.3, 4.4, 4.9.</li>
  <li>See <a href="http://dinosaur.compilertools.net/">
      The Lex &amp; Yacc Page</a>
      for lex and yacc tutorials and manuals.</li>
  <li><a href="http://www.scribd.com/doc/8669780/Lex-yacc-Tutorial">
        Another nice Lex &amp; Yacc tutorial</a></li>
 </ul><br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>
</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 9: February 20, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 9: Predictive Top-Down Parsers<br/>
  February 20, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Review</li>
  <li>FIRST</li>
  <li>FOLLOW</li>
  <li>How to construct a predictive parsing table</li>
  <li>LL(1) grammars</li>
  <li>Transformations on grammars</li>
 </ol>

 <h2>1. Review</h2>
 <ul>
  <li>Top-down parsing consists of constructing or tracing a parse tree
      for an input string starting from the root and creating
      the nodes of the parse tree in preorder.</li>
  <li>Recursive-descent parsing is a top-down method of syntax
      analysis in which a set of recursive procedures is used
      to process the input string with a
      procedure associated with each nonterminal of
      the grammar. See Fig. 4.13, p. 219.</li>
  <li>A nonrecursive predictive parser uses an explicit stack and
      a parsing table to do deterministic top-down parsing.</li>
  <li>In this class we will develop an algorithm to construct
      a predictive parsing table for a large class of useful
      grammars called LL(1) grammars.</li>
  <li>For this algorithm we need two functions on grammars,
      FIRST and FOLLOW.</li>
 </ul>
 

 <h2>2. FIRST</h2>
 <ul>
  <li>FIRST(&#945;) is the set of terminal symbols that begin the strings
      derivable from a string of terminal and nonterminal symbols &#945; in a grammar.<br/>
      If &#945; can derive &#949;, then &#949; is also in FIRST(&#945;).</li>

  <li>Algorithm to compute FIRST(<i>X</i>):</li>
  </ul><ol>
   <li>If <i>X</i> is a terminal, then FIRST(<i>X</i>) = { <i>X</i> }.</li>
   <li>If <i>X</i> &#8594; &#949; is a production, then add &#949; to FIRST(<i>X</i>).</li>
   <li>If <i>X</i>
       &#8594; <i>Y</i><sub>1</sub> <i>Y</i><sub>2</sub> ... <i>Y<sub>k</sub></i>
       is a production for <i>k</i> &#8805; 1, and<br/>
       &#160;&#160;&#160;for some <i>i</i> &#8804; <i>k</i>,
       <i>Y</i><sub>1</sub><i>Y</i><sub>2</sub> ... <i>Y<sub>i</sub></i><sub>-1</sub> derives the empty string,
       and <code>a</code> is in FIRST(<i>Y<sub>i</sub></i>),
       then add <code>a</code> to FIRST(<i>X</i>).<br/>
       If <i>Y</i><sub>1</sub><i>Y</i><sub>2</sub> ... <i>Y<sub>k</sub></i>
       derives the empty string,
       then add &#949; to FIRST(<i>X</i>).</li>
  </ol>
  <li><b>Example.</b> Consider the grammar <i>G</i>:</li>
  <ul>
   <dt><pre><code>S &#8594; ( S ) S | &#949;</code></pre></dt><dt>
  </dt></ul>
   <dt>For <i>G</i>, FIRST(<code>S</code>) = {<code>(, &#949;</code>}.</dt>

 

 <h2>3. FOLLOW</h2>
 <ul>
  <li>FOLLOW(<i>A</i>) is the set of terminals that can appear immediately
      to the right of <i>A</i> in some sentential form in a grammar.<br/>
      Let us assume the string to be parsed is terminated by an end-of-string
      endmarker $.
      Then if <i>A</i> can be the rightmost symbol in some sentential form,
      the right endmarker $ is also in FOLLOW(<i>A</i>).</li>
  <li>Algorithm to compute FOLLOW(<i>A</i>) for all nonterminals <i>A</i> of a grammar:</li>
  </ul><ol>
   <li>Place $ in FOLLOW(<i>S</i>) where <i>S</i> is the start symbol of the grammar.</li>
   <li>If <i>A</i> &#8594; &#945;<i>B</i>&#946; is a production,
       then add every terminal symbol <code>a</code> in FIRST(&#946;)
       to FOLLOW(<i>B</i>).</li>
   <li>If there is a production <i>A</i> &#8594; &#945;<i>B</i>, 
       or a production <i>A</i> &#8594; &#945;<i>B</i>&#946;,
       where FIRST(&#946;) contains &#949;,<br/>
       then add every symbol in FOLLOW(<i>A</i>) to FOLLOW(<i>B</i>).</li>
  </ol>
  <li><b>Example.</b> For <i>G</i> above, FOLLOW(<code>S</code>) = {<code>)</code>, $}.</li>
  
 

 <h2>4. How to Construct a Predictive Parsing Table</h2>
 <ul>
  <li>Input: Grammar <i>G</i>.</li>
  <li>Output: Predictive parsing table <i>M</i>.</li>

  <li>Method:</li>
  </ul><pre><code>
  for (each production <i>A</i> &#8594; &#945; in <i>G</i>) {
     for (each terminal <i>a</i> in FIRST(&#945;))
        add <i>A</i> &#8594; &#945; to <i>M</i>[<i>A, a</i>];
     if (&#949; is in FIRST(&#945;))
        for (each symbol <i>b</i> in FOLLOW(<i>A</i>))
           add <i>A</i> &#8594; &#945; to <i>M</i>[<i>A, b</i>];
  }
  make each undefined entry of <i>M</i> be <b>error</b>;
  </code></pre>

  <li><b>Example 1.</b> Predictive parsing table for the grammar:</li>
  <ul>
   <dt><pre><code>S &#8594; +SS | *SS | a;</code></pre></dt>
  </ul>
   <dt>FIRST(<code>S</code>) = {<code>+, *, a</code>}</dt>
   <dt>FOLLOW(<code>S</code>) = {<code>+, *, a, $</code>}</dt>

    <pre><code>
                        Input Symbol
   Nonterminal      a        +          *        $

       S          S &#8594; a    S &#8594; +SS    S &#8594; *SS  <b>error</b>
   </code></pre>

  <li><b>Example 2.</b> Predictive parsing table for the grammar:</li>
  <ul>
   <dt><pre><code>S &#8594; ( S ) S | &#949;</code></pre></dt>
  </ul>
   <dt>FIRST(<code>S</code>) = {<code>(, &#949;</code>}</dt>
   <dt>FOLLOW(<code>S</code>) = {<code>), $</code>}</dt>

    <pre><code>
                        Input Symbol
   Nonterminal      (         )        $

       S        S &#8594; (S)S    S &#8594; &#949;    S &#8594; &#949;
   </code></pre>

  <li><b>Example 3.</b> Predictive parsing table for the grammar:</li>
  <ul>
   <dt><pre><code>S &#8594; S ( S ) | &#949;</code></pre></dt>
  </ul>
   <dt>FIRST(<code>S</code>) = {<code>(, &#949;</code>}</dt>
   <dt>FOLLOW(<code>S</code>) = {<code>(, ), $</code>}</dt>

    <pre><code>
                        Input Symbol
   Nonterminal      (         )        $

       S        S &#8594; S(S)    S &#8594; &#949;    S &#8594; &#949;
                S &#8594; &#949;
   </code></pre>


 

 <h2>5. LL(1) Grammars</h2>
 <ul>
  <li>A grammar is LL(1) iff whenever <code>A &#8594; &#945; | &#946;</code>
      are two distinct productions, the following three conditions hold:</li>
  </ul><ol>
   <li>For no terminal <code>a</code> do both &#945; and &#946; derive
       strings beginning with <code>a</code>.</li>
   <li>At most one of &#945; and &#946; can derive the empty string.</li>
   <li>If &#946; derives the empty string, then &#945; does not derive any string
       beginning with a terminal in FOLLOW(<code>A</code>).<br/>
       Likewise, if &#945; derives the empty string, then &#946; does not derive
       any string beginning with a terminal in FOLLOW(<code>A</code>).</li>
  </ol>
  <li>We can use the algorithm above to construct a predictive parsing
      table with uniquely defined entries for any LL(1) grammar.</li>
  <li>The first "L" in LL(1) means scanning the input from left to right,
      the second "L" for producing a leftmost derivation, and the "1" for
      using one symbol of lookahead to make each parsing action decision.</li> 
 


 <h2>6. Transformations on Grammars</h2>
 <ul>
  <li>Two common language-preserving transformations are often applied to
      grammars to try to make them parsable by top-down methods.
      These are eliminating left recursion and left factoring.</li>
  <li>Eliminating left recursion:</li>
  <ul>
   <li>Replace</li>
   </ul></ul><pre><code>
      expr &#8594;  expr + term
           |  term
   </code></pre> by

    <pre><code>
      expr  &#8594;  term expr'

      expr' &#8594;  + term expr'
            |  &#949;
   </code></pre>
  

  <li>Left factoring:</li>
  <ul>
   <li>Replace</li>
   </ul><pre><code>
      stmt &#8594; if ( expr ) stmt else stmt
           | if (expr) stmt
           | other
   </code></pre> by
   <pre><code>
      stmt  &#8594; if ( expr ) stmt stmt'
            | other

      stmt' &#8594; else stmt
            | &#949;
   </code></pre>
  
 

 <h2>7. Practice Problems</h2>
 <ol>
  <dt>Consider the following grammar G for Boolean expressions:</dt>
  </ol><ul>
   </ul><pre><code>
     B &#8594; B or T | T
     T &#8594; T and F | F
     F &#8594; not B | ( B ) | true | false
   </code></pre>
  
  <li>What precedence and associativity does this grammar give to the operators
      <code>and, or, not</code>?</li>
  <li>Compute FIRST and FOLLOW for each nonterminal in G.</li>
  <li>Transform G into an equivalent LL(1) grammar G'.</li>
  <li>Construct a predictive parsing table for G'.</li>
  <li>Show how your predictive parser processes the input string</li>
  <ul>
      <code>true and not false or true</code>
  </ul>
  <dt>Draw the parse tree traced out by your parser.</dt>
 


 <h2>8. Reading</h2>
 <ul>
  <li>ALSU, Section 4.4.</li>
 </ul><br/>





<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 10: February 25, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 10: Bottom-Up Parsing<br/>
  February 25, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Bottom-up parsing</li>
  <li>LR(1) parsing</li>
  <li>Constructing a simple LR(1) parser</li>
  <li>DFA for viable prefixes</li>
 </ol>

 <h2>1. Bottom-up Parsing</h2>
 <ul>
   <li>Bottom-up parsing can be viewed as trying to find a
       rightmost derivation in reverse for an input string.</li>
   <li>A <i>handle</i> is a rightmost substring in right-sentential form
       that matches the body of a production and whose
       reduction by that production represents one step in
       the reverse of the rightmost derivation.</li>
 <ul>
  <li>Consider the grammar <i>G</i>:</li>
    </ul></ul><pre><code>
      (1) S &#8594; S ( S )
      (2) S &#8594; &#949;
   </code></pre>

  <li>The handles in a rightmost derivation for the input string <code>( ) ( )</code>:</li>
  <pre><code>
      S &#8658; S ( S )      // handle is S ( S )
        &#8658; S ( )        // handle is the empty string between ( )
        &#8658; S ( S ) ( )  // handle is S ( S )
        &#8658; S ( ) ( )    // handle is the empty string between first ( )
        &#8658; ( ) ( )      // handle is the empty string prefix
  </code></pre>
  

  <li><i>Shift-reduce parsing</i> is a form of bottom-up parsing 
       in which we shift terminal symbols of the string to be parsed onto
       a stack until a handle appears on top of the stack. We then
       replace the handle by the nonterminal symbol on the left-hand
       side of the associated production (this is a "reduce" action).  We keep
       repeating this process until we have reduced the input string
       to the start symbol of the grammar. This process simulates the
       reverse of a rightmost derivation for the input string.
       Thus, we can think of shift-reduce parsing as "handle pruning."</li>
 


 <h2>2. LR(1) Parsing</h2>
 <ul>
  <li>Model of an LR(1) parser (Fig. 4.35).</li>
  <li>"L" means left-to-right scanning of the input, the "R" means
      constructing a rightmost derivation in reverse, and the "1"
      means one symbol of lookahead in making parsing decisions.</li>

  <li>LR parsing table for <i>G</i>:</li><br/>
   <ul>
   <code>
   </code><table border="2">
    <tbody><tr>
     <th rowspan="2"><code>&#160;State&#160;</code></th>
     <th colspan="3"><code>Action</code></th>
     <th><code>Goto</code></th>
    </tr>
    <tr>
     <td style="text-align: center;"><code>&#160;(&#160;</code></td>
     <td style="text-align: center;"><code>&#160;)&#160;</code></td>
     <td style="text-align: center;"><code>&#160;$&#160;</code></td>
     <td style="text-align: center;"><code>&#160;S&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>0</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>1</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>1</code></td>
     <td style="text-align: center;"><code>s2</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>acc</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>2</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>3</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>3</code></td>
     <td style="text-align: center;"><code>s2</code></td>
     <td style="text-align: center;"><code>s4</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>4</code></td>
     <td style="text-align: center;"><code>r1</code></td>
     <td style="text-align: center;"><code>r1</code></td>
     <td style="text-align: center;"><code>r1</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
   </tbody></table>
   </ul><br/>

   <dt>r2 means reduce the handle on top of the stack by production (2) 
       <code>S &#8594; &#949;</code>.</dt>
   <dt>s2 means shift the input symbol on the stack and then push state 2
       on top of the stack.</dt>
   <dt>acc means accept and stop parsing.</dt>
   <dt>Goto[0,S] = 1 means push state 1 on top of the stack after reducing a
       handle to the nonterminal S in state 0.</dt>
   <dt>A blank entry means report a syntax error.</dt>
   
   <br/>


  <li>Moves made by an LR(1) parser on input <code>( ) ( )</code> [Alg. 4.44].</li>

    </ul><pre><code>
    <b>Stack      Input           Action</b>
    0          ()()$    reduce by (2) S &#8594; &#949;; push state 1 on stack
    0S1        ()()$    shift ( on stack; push state 2 on stack
    0S1(2       )()$    reduce by (2) S &#8594; &#949; and push state 3
    0S1(2S3     )()$    shift ( and push state 2
    0S1(2S3)4    ()$    reduce by (1) S &#8594; S(S) and push state 1
    0S1          ()$    shift ( and push state 2
    0S1(2         )$    reduce by (2) S &#8594; &#949; and push state 3
    0S1(2S3       )$    shift ) and push state 4
    0S1(2S3)4      $    reduce by (1) S &#8594; S(S) and push state 1
    0S1            $    accept       
   </code></pre>
  <li>Note that an LR parser is a shift-reduce parser that traces out a
      rightmost derivation in reverse.</li>

 


 <h2>3. Constructing a Simple LR(1) Parsing Table for a Grammar</h2>
 <ul>
  <li>An <i>LR</i>(0) <i>item</i> of a grammar is a production of the grammar with a dot
      at some position of the right side.
      E.g., <code>S &#8594; &#183;S(S)</code>, <code>S &#8594; S&#183;(S)</code>,
      or <code>S &#8594; S(S)&#183;</code>.</li>
  <li>We will use two functions to construct the sets of items for a grammar:</li>
  <ul>
   <li><i>closure</i>(<i>I</i>), where <i>I</i> is a set of items, is the set of items
       constructed by the following two rules:</li>
   </ul></ul><ol>
    <li>Initially, put every item in <i>I</i> into <i>closure</i>(<i>I</i>).</li>
    <li>If <i>A</i> &#8594; &#945;&#183;<i>B</i>&#946; is in <i>closure</i>(<i>I</i>)
        and <i>B &#8594; &#947;</i> is a production, then add the item
        <i>B</i> &#8594; &#183;&#947; to <i>closure</i>(<i>I</i>) if it is not already there.
        Keep repeating this step until no more new items can be added to <i>I</i>.</li>
   </ol>
   <li><i>goto</i>(<i>I</i>, <i>X</i>), where <i>I</i> is a set of items and
       <i>X</i> is a grammar symbol, is the closure of the set of all items
       <i>A</i> &#8594; &#945;<i>X</i>&#183;&#946; where
       <i>A</i> &#8594; &#945;&#183;<i>X</i>&#946; is in <i>I</i>.</li>
  
  <li>An <i>augmented</i> grammar <i>G'</i> is one to which we have added
      a new starting production <i>S'</i> &#8594; <i>S</i> where <i>S</i> is
      the start symbol of the given grammar <i>G</i>.
      Reducing by the new starting production signals acceptance of the input
      string being parsed. We will always augment a grammar when we construct
      an SLR parsing table for it.</li>

  <li>The sets-of-items construction
  <ul>

   <li>Input: An augmented grammar <i>G'</i>.</li>
   <li>Output: <i>C</i>, the canonical collection of sets of LR(0) items for <i>G'</i>.</li>
   <li>Method:</li>
   </ul><pre><code>
   I<sub>0</sub> = closure({[S' &#8594; &#183;S]});
   C = {I<sub>0</sub>};
   repeat
     for each set of items I in C and grammar symbol X such that
       goto(I,X) is not empty and not in C do
         add goto(I,X) to C;
   until no more sets of items can be added to C;
   </code></pre>
  
  </li><li>Example: Given the augmented grammar <i>G'</i></li>
  <pre><code>
  S' &#8594; S
  S  &#8594; S(S)
  S  &#8594; &#949;
  </code></pre>
  <dt><i>C</i>, the canonical collection of sets of LR(0) items for <i>G'</i>, is</dt>

  <pre><code>
  I<sub>0</sub>: S' &#8594; &#183;S
      S &#8594; &#183;S(S)
      S &#8594; &#183;

  I<sub>1</sub>: S' &#8594; S&#183;
      S &#8594; S&#183;(S)

  I<sub>2</sub>: S &#8594; S(&#183;S)
      S &#8594; &#183;S(S)
      S &#8594; &#183;

  I<sub>3</sub>: S &#8594; S(S&#183;)
      S &#8594; S&#183;(S)

  I<sub>4</sub>: S' &#8594; S(S)&#183;
  </code></pre>


  <li>Algorithm to construct the SLR(1) parsing table from <code>C</code>,
      the canonical collection of sets of LR(0) items for an augmented grammar
      <i>G'</i></li>
  <ul>
   <li>Input: <code>C = {I<sub>0</sub>, I<sub>1</sub>, ... , I<sub>n</sub></code>}.</li>
   <li>Output: The SLR parsing table functions <code>action</code> and
       <code>goto</code>.</li>
   <li>Method:</li>
   <ul>
    <li>State <code>i</code> and its <code>action</code> and <code>goto</code>
        functions are constructed from <code>I<sub>i</sub></code> as follows:</li>
    <ul>
     <li>If item [<code>A&#8594; &#945;&#183;a&#946;</code>] is in
         <code>I<sub>i</sub></code> and <code>goto(I<sub>i</sub>, a) =
         I<sub>j</sub></code>, then add "<code>shift j</code>" to
         <code>action[i, a]</code>. Here <code>a</code> is a terminal.</li>
     <li>If item [<code>A &#8594; &#945;&#183;</code>] is in <code>I<sub>i</sub></code>,
         then add "<code>reduce A &#8594; &#945;</code>" to <code>action[i, a]</code>
         for all <code>a</code> in FOLLOW(<code>A</code>).
         Here <code>A</code> cannot be <code>S'</code>.</li>
     <li>If item [<code>S' &#8594; S&#183;</code>] is in <code>I<sub>i</sub></code>,
         then add "<code>accept</code>" to <code>action[i, $]</code>.</li>
    </ul>
    <li>If <code>goto(I<sub>i</sub>, A) = I<sub>j</sub></code>, then in
        the parsing table set <code>goto[i, A] = j</code>.</li>
    <li>The initial state of the parser is constructed from the set of items
        containing [<code>S' &#8594; &#183;S</code>].</li>
   </ul>
  <li>Notes:</li>
  <ul>
   <li>If each parsing table entry has at most one action, then the grammar
       is said to be <i>SLR(1)</i>. If any entry has more than one action,
       then the algorithm fails to produce a parser.</li>
   <li>All undefined entries are made <code>error</code>.</li>
  </ul>
  <li>Example: the LR parsing table above is an SLR(1) parsing table for the
      balanced-parentheses grammar.</li>
      
  </ul>
 

 <h2>4. DFA for Viable Prefixes</h2>
 <ul>
  <li>A <i>viable prefix</i>
      is a prefix of a right sentential form that does not continue past the
      right end of the rightmost handle of that sentential form.</li>
  <li>The shift and goto functions of the canonical collection of sets
      of LR(0) items for a grammar <i>G</i> define a DFA that recognizes the
      viable prefixes of <i>G</i>.
  </li><li>An item [<code>A &#8594; &#946;&#183;&#947;</code>] is <i>valid</i> for a viable prefix <code>&#945;&#946;</code> if there is a rightmost derivation from <code>S'</code> to <code>&#945;Aw</code> to <code>&#945;&#946;&#947;w</code>.</li>
 </ul>

 <h2>5. Practice Problems</h2>
 <ol>
  <dt>Consider the following grammar G:</dt>

  </ol><ul>
   </ul><pre><code>
     (1) S &#8594; S S +
     (2) S &#8594; S S *
     (3) S &#8594; a
   </code></pre>
  
  <li>Construct a rightmost derivation and parse tree for the input string
      <code>aaa*+$</code>.</li>
  <li>Show the handle in each sentential form
      in the derivation.</li>
  <li>Construct the canonical collection of sets of LR(0) items for the augmented grammar.</li>
  <li>Construct an SLR(1) parsing table for G.</li>
  <li>Show how your SLR(1) parser processes the input string <code>aaa*+$</code>.</li>
 



 <h2>6. Reading</h2>
 <ul>
  <li>ALSU, Sects. 4.5, 4.6.</li>
 </ul><br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<!-- Homework Assignment #2                         -->
<!-- Created by Al Aho on 2/24/2013                 -->
<!-- Last modified on 2/24/2013                     -->
<!-- ------------------------------------------------->
<title>Homework #2</title></head>
<body>
 <h1 style="text-align: left;"> COMS W4115
  <br/>Programming Languages and Translators
  <br/>Homework Assignment #2
  <br/>Submit solutions electronically on
  <br/>&#160;&#160;&#160;&#160;&#160;Courseworks/COMSW4115/Assignments
  <br/>&#160;&#160;&#160;&#160;&#160;by 2:40pm, March 6, 2013
 </h1>
 <hr/>

<h2>Instructions</h2>
 <ul>
  <li>Problems 1-4 are each worth 25 points.</li>
  <li>You may discuss the questions with others in the class but your answers
      must be in your own words and your own code. You must not copy someone
      else's solutions.
      If you consult others or use external sources, please cite the
      people or sources in your answers.</li>
  <li>Solutions to these problems will be posted
      on Courseworks on March 11, 2013.</li>
  <li>This assignment may submitted electronically on Courseworks by 2:40pm,
      March 11, 2013 for 50% credit.</li>
  <li>Pdf files are preferred.</li>
 </ul>


<h2>Problems</h2>
 <ol>
  <li>Interactive desk calculator for boolean nor-expressions.</li>
  <ol style="list-style-type: lower-alpha;">
  <li>Construct a grammar that generates
      boolean nor-expressions
      containing the logical
      constants <code>true</code> and <code>false</code>, the
      left-associative binary boolean operator
      <code>nor</code> [where <code>p nor q</code>
      means not (p or q)], and
      parentheses.</li>
  <li>Show the parse tree according to your grammar for the nor-expression</li>
      <dt>&#160;&#160;&#160;<code>true nor true nor (false nor false)</code></dt>
  <li>Implement an interpreter that takes as input newline-terminated
      lines of boolean nor-expressions and produces as
      output the truth value of each expression.
      You can use lex and yacc or their equivalents
      to implement your interpreter.  Show the source
      code for your interpreter and the sequences of
      commands you used to test it.</li>
  <li>Run your interpreter on the following two inputs and show the outputs:</li>
      <dt>&#160;&#160;&#160;(a) <code>(true nor false) nor (true nor false)</code></dt>
      <dt>&#160;&#160;&#160;(b) <code>true nor true nor (false nor false)</code></dt>
  </ol><br/>


  <li>Infix to stack machine code translator</li>
  <ol style="list-style-type: lower-alpha;">

   <li>Consider the Yacc specification in Fig. 4.59 of ALSU (p. 292).
       Modify this translator to produce stack machine code for
       each input line.  For example, for the input "1*(2+3)" your translator
       should produce the instructions</li>

<pre><code>push 1
push 2
push 3
add
multiply
done
</code></pre>

   <li>Implement your translator in Yacc (or its equivalent) and show the
       stack machine code generated for each of the following inputs:</li>
      <dt>&#160;&#160;&#160;&#160;(i) <code>1+2*3</code></dt>
      <dt>&#160;&#160;&#160;(ii) <code>1+(2-3)</code></dt>
      <dt>&#160;&#160;(iii) <code>1+2-+3</code></dt>
      <dt>&#160;&#160;(iv) <code>1+2--3</code></dt>
  </ol>
  <br/>

  <li>Let <i>L</i> be the language generated by the following grammar:</li>
<pre><code>
S &#8594; a S b S | b S a S | &#949;
</code></pre>

  <ol style="list-style-type: lower-alpha;">
   <li>What language does this grammar generate?</li>
   <li>Show that this grammar is ambiguous.</li>
   <li>Construct the predictive parsing table for this grammar.</li>
   <li>Construct an LL(1) grammar for <i>L</i>.</li>
   <li>Construct the predictive parsing table for your grammar.</li>
  </ol>
  <br/>

  <li>Let <i>L</i> be the language of pure lambda calculus
      expressions generated by the following grammar:</li>
<pre><code>
E &#8594; ^ v . E  | E E | ( E ) | v
</code></pre>
      <dt>The symbols <code>^, ., (, )</code> and <code>v</code> are tokens.
          <code>^</code> represents lambda and
          <code>v</code> represents a variable.<dt>
      <dt>An expression of the form <code>^v.E</code> is a function definition
          where <code>v</code> is the formal parameter of the function and
          <code>E</code> is its body.</dt>
      <dt>If <i>f</i> and <i>g</i> are lambda expressions, then the lambda
          expression <i>fg</i> represents the application of the function <i>f</i>
          to the argument <i>g</i>.</dt>
      
      

  <ol style="list-style-type: lower-alpha;">
   <li>Show that this grammar is ambiguous.</li>
   <li>Construct an unambiguous grammar for <i>L</i> assuming that
       function application is left associative, e.g., <i>fgh</i> = (<i>fg</i>)<i>h</i>,
       and that function application binds tighter than <code>.</code>,
       e.g., (^<i>x</i><code>.</code> ^<i>y</i><code>.</code> <i>xy</i>)
       ^<i>z</i><code>.</code><i>z</i> =
       (^<i>x</i>. (^<i>y</i>. <i>xy</i>)) ^<i>z</i>.<i>z</i>.</li>
   <li>Using your grammar, construct a parse tree for the expression
       (^<i>x</i><code>.</code> ^<i>y</i><code>.</code> <i>xy</i>) ^<i>z</i><code>.</code><i>z</i>.
   </li><li>Using the command <code>yacc -v</code> on a file containing your grammar,
       show the LALR(1) parsing action and goto table for your grammar.</li>


 </ol>
 <br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address> 

</dt></dt></ol></body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"/>
<title>Lecture 11: February 27, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 11: Parsing Action Conflicts<br/>
  February 27, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Parsing action conflicts</li>
  <li>Resolving shift/reduce conflicts</li>
  <li>Using Yacc to generate LALR(1) parsers</li>
  <li>Using Yacc with ambiguous grammars</li>
  <li>Error recovery in Yacc</li>
 </ol>


 <h2>1. Parsing Action Conflicts</h2>
 <ul>
  <li>If a grammar is not SLR(1), the SLR parsing table construction produces one or more
      multiply defined entries in the parsing table action function.</li>
  <li>These entries are either <i>shift/reduce conflicts</i> or
      <i>reduce/reduce conflicts.</i>
 </li></ul>

 <h2>2. Resolving Shift/Reduce Conflicts</h2>
 <ul>
  <li><b>Example.</b> Given the augmented grammar <i>G'</i></li>
  </ul><pre><code>
  (0) E' &#8594; E
  (1) E  &#8594; E+E
  (2) E  &#8594; E*E
  (3) E  &#8594; id
  </code></pre>
  <dt>FOLLOW(<code>E</code>) = { <code>+, *, $</code>}</dt><br/>
  <li>Note that this grammar does not specify the relative precedence
      of the <code>+</code> and <code>*</code> operators
      nor their associativities.</li>
  <li><i>C</i>, the canonical collection of sets of LR(0) items for <i>G'</i>, is</li>

  <pre><code>
  I<sub>0</sub>: E' &#8594; &#183;E
      E &#8594; &#183;E+E
      E &#8594; &#183;E*E
      E &#8594; &#183;id

  I<sub>1</sub>: E' &#8594; E&#183;
      E &#8594; E&#183;+E
      E &#8594; E&#183;*E

  I<sub>2</sub>: E &#8594; id&#183;

  I<sub>3</sub>: E &#8594; E+&#183;E
      E &#8594; &#183;E+E
      E &#8594; &#183;E*E
      E &#8594; &#183;id

  I<sub>4</sub>: E &#8594; E*&#183;E
      E &#8594; &#183;E+E
      E &#8594; &#183;E*E
      E &#8594; &#183;id

  I<sub>5</sub>: E &#8594; E+E&#183;
      E &#8594; E&#183;+E
      E &#8594; E&#183;*E

  I<sub>6</sub>: E &#8594; E*E&#183;
      E &#8594; E&#183;+E
      E &#8594; E&#183;*E

  </code></pre>


  <li>SLR(1) parsing table for <i>G'</i>:</li><br/>

   <code>
   </code><table border="4">
    <tbody><tr>
     <th rowspan="2"><code>&#160;State&#160;</code></th>
     <th colspan="4"><code>Action</code></th>
     <th><code>Goto</code></th>
    </tr>
    <tr>
     <td style="text-align: center;"><code>&#160;id&#160;</code></td>
     <td style="text-align: center;"><code>&#160;+&#160;</code></td>
     <td style="text-align: center;"><code>&#160;*&#160;</code></td>
     <td style="text-align: center;"><code>&#160;$&#160;</code></td>
     <td style="text-align: center;"><code>&#160;E&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>0</code></td>
     <td style="text-align: center;"><code>s2</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>1</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>1</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>s3</code></td>
     <td style="text-align: center;"><code>s4</code></td>
     <td style="text-align: center;"><code>acc</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>2</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>r3</code></td>
     <td style="text-align: center;"><code>r3</code></td>
     <td style="text-align: center;"><code>r3</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>3</code></td>
     <td style="text-align: center;"><code>s2</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>5</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>4</code></td>
     <td style="text-align: center;"><code>s2</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>6</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>5</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>s3</code><br/>
        <code>r1</code></td>
     <td style="text-align: center;"><code>s4</code><br/>
        <code>r1</code></td>
     <td style="text-align: center;"><code>r1</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code>6</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
     <td style="text-align: center;"><code>s3</code><br/>
        <code>r2</code></td>
     <td style="text-align: center;"><code>s4</code><br/>
        <code>r2</code></td>
     <td style="text-align: center;"><code>r2</code></td>
     <td style="text-align: center;"><code>&#160;</code></td>
    </tr>
   </tbody></table>
   
   <br/>

   <li>Notes</li>
   <ul>
    <li>There is a shift/reduce conflict in <code>action[5,+]</code> between
        "<code>shift 3</code>" and "<code>reduce by E &#8594; E+E</code>"
        because the associativity of the operator <code>+</code> is not defined
        by the grammar. This conflict can be resolved in favor of
        "<code>reduce by E &#8594; E+E</code>" if we want <code>+</code>
        to be left associative.</li>

    <li>There is a shift/reduce conflict in <code>action[5,*]</code>
        between "<code>shift 4</code>" and "<code>reduce by E &#8594; E+E</code>"
        because the relative precedence of the operators <code>+</code> and
        <code>*</code> is not defined by the grammar. This conflict can be
        resolved in favor of "<code>shift 4</code>" if we want <code>*</code>
        to have higher precedence than <code>+</code>.</li>

    <li>Analogously, there is a shift/reduce conflict in <code>action[6,+]</code>
        between "<code>shift 3</code>" and "<code>reduce by E &#8594; E*E</code>"
        because the relative precedence of the operators <code>+</code> and
        <code>*</code> is not defined by the grammar. This conflict can be
        resolved in favor of "<code>reduce by E &#8594; E*E</code>" if we want
        <code>*</code> to have higher precedence than <code>+</code>.</li>

    <li>There is a shift/reduce conflict in <code>action[6,*]</code> between
        "<code>shift 4</code>" and "<code>reduce by E &#8594; E*E</code>"
        because the associativity of the operator <code>*</code> is not defined
        by the grammar. This conflict can be resolved in favor of
        "<code>reduce by E &#8594; E*E</code>" if we want <code>*</code> to be
        left associative.</li>
   </ul>
 

 <h2>3. Using Yacc to Generate LALR(1) Parsers</h2>
 <ul>
  <li>Consider the yacc program <code>expr1.y</code>:</li>
  </ul><pre><code>
%token id
%%
E : E '+' E
  | E '*' E
  | id
  ;
  </code></pre>

  <li>Invoking <code>yacc -v expr1.y</code>, we can see the kernels of the
      sets of items for this grammar in the yacc output file <code>y.output</code>.</li>
  <li>The parsing action conflicts are shown for states 5 and 6.</li>
  <pre><code>
state 0
        $accept : _E $end

        id  shift 2
        .  error

        E  goto 1

state 1
        $accept :  E_$end
        E :  E_+ E
        E :  E_* E

        $end  accept
        +  shift 3
        *  shift 4
        .  error

state 2
        E :  id_    (3)

        .  reduce 3

state 3
        E :  E +_E

        id  shift 2
        .  error

        E  goto 5

state 4
        E :  E *_E

        id  shift 2
        .  error

        E  goto 6

5: shift/reduce conflict (shift 3, red'n 1) on +
5: shift/reduce conflict (shift 4, red'n 1) on *
state 5
        E :  E_+ E
        E :  E + E_    (1)
        E :  E_* E

        +  shift 3
        *  shift 4
        .  reduce 1

6: shift/reduce conflict (shift 3, red'n 2) on +
6: shift/reduce conflict (shift 4, red'n 2) on *
state 6
        E :  E_+ E
        E :  E_* E
        E :  E * E_    (2)

        +  shift 3
        *  shift 4
        .  reduce 2


5/127 terminals, 1/600 nonterminals
4/300 grammar rules, 7/1000 states
4 shift/reduce, 0 reduce/reduce conflicts reported
4/601 working sets used
memory: states,etc. 17/2000, parser 2/4000
3/3001 distinct lookahead sets
0 extra closures
9 shift entries, 1 exceptions
3 goto entries
0 entries saved by goto default
Optimizer space used: input 25/2000, output 9/4000
9 table entries, 3 zero
maximum spread: 257, maximum offset: 257
  </code></pre>

 

 <h2>4. Using Yacc with Ambiguous Grammars</h2>
 <ul>
  <li>We can specify the associativities and relative precedence
      of the <code>+</code> and <code>*</code> operators in the
      declarations sections of a yacc program.</li>
  <li>Consider the yacc program <code>expr2.y</code>:</li>
  <li>The statement <code>%left '+'</code> makes <code>+</code> left associative.</li>
  <li>The statement <code>%left '*'</code> makes <code>*</code> left associative.</li>
  <li>Since the statement for <code>+</code> comes before the statement for
      <code>*</code>, <code>+</code> has lower precedence than <code>*</code>.</li>

  </ul><pre><code>
%token id
%left '+'
%left '*'
%%
E : E '+' E
  | E '*' E
  | id
  ;
  </code></pre>

  <li>Invoking <code>yacc -v expr2.y</code>, we can now see that
      no shift-reduce conflicts have been generated in
      the yacc output file <code>y.output</code>.</li>
  <pre><code>
state 0
        $accept : _E $end

        id  shift 2
        .  error

        E  goto 1

state 1
        $accept :  E_$end
        E :  E_+ E
        E :  E_* E

        $end  accept
        +  shift 3
        *  shift 4
        .  error

state 2
        E :  id_    (3)

        .  reduce 3

state 3
        E :  E +_E

        id  shift 2
        .  error

        E  goto 5

state 4
        E :  E *_E

        id  shift 2
        .  error

        E  goto 6

state 5
        E :  E_+ E
        E :  E + E_    (1)
        E :  E_* E

        *  shift 4
        .  reduce 1

state 6
        E :  E_+ E
        E :  E_* E
        E :  E * E_    (2)

        .  reduce 2


5/127 terminals, 1/600 nonterminals
4/300 grammar rules, 7/1000 states
0 shift/reduce, 0 reduce/reduce conflicts reported
4/601 working sets used
memory: states,etc. 17/2000, parser 2/4000
3/3001 distinct lookahead sets
0 extra closures
6 shift entries, 1 exceptions
3 goto entries
0 entries saved by goto default
Optimizer space used: input 19/2000, output 10/4000
10 table entries, 3 zero
maximum spread: 257, maximum offset: 257
  </code></pre>

 <li>Unless otherwise instructed, Yacc resolves all parsing
     action conflicts using the following two rules:</li>
 <ol>
  <li>A reduce/reduce conflict is resolved by choosing the
      conflicting production listed first in the Yacc
      specification.</li>
  <li>A shift/reduce conflict is resolved in favor of shift.
      Note that this rule correctly resolves the shift/reduce conflict
      arising from the dangling-else ambiguity.</li>
 </ol>

 

 <h2>5. Error Recovery in Yacc</h2>
 <ul>
  <li>In Yacc, error recovery can be performed with error productions.</li>
  <li>To process errors at the level of a nonterminal <code>A</code>,
      add an error production <code>A &#8594; error &#945;</code>
      where <code>error</code> is a Yacc reserved word, and &#945; is a string
      of grammar symbols, possibly empty.  Yacc will generate a parser
      including this production.</li>
  <li>If Yacc encounters an error during parsing, it pops symbols from its stack
      until it finds the topmost state on its stack whose underlying set of items
      includes an item of the form
      <dt>&#160;&#160;&#160;<code>A &#8594; . error &#945;</code></dt>
      The parser then shifts the special token <code>error</code> onto the stack
      as though it saw the token <code>error</code> on the input.</li>
  <li>If &#945; is not empty, Yacc skips ahead on the input looking for a
      substring that can be "reduced" to &#945; on the stack. Now the parser
      will have <code>error &#945;</code> on top of its stack, which it will
      reduce to <code>A</code>. The parser then resumes normal parsing.</li>
  <li>Example: See Fig. 4.61, p. 296.  This Yacc specification contains
      a translation rule</li>
  </ul><pre><code>
      lines : error '\n'  { yyerror("reenter previous line:"); yyerrok; }
  </code></pre>
  <dt>On encountering an error, the parser starts popping symbols from
      its stack until it encounters a state with a shift action on <code>error</code>.
      The parser shifts the token <code>error</code> on to the stack and
      then skips ahead in the input until it finds a newline which it shifts
      onto the stack.  The parser reduces <code>error '\n'</code> to
      <code>lines</code> and emits the error message "reenter previous line:".
      The special Yacc routine <code>yyerrok</code> resets the parser to its
      normal mode of operation.</dt>
 

 <h2>6. Practice Problems</h2>
 <ol>
 <dt>Consider the following grammar G:</dt>
  </ol><ul>
   </ul><pre><code>
     (1) S &#8594; a S b S
     (2) S &#8594; b S a S
     (3) S &#8594; &#949;
   </code></pre>
  
  <li>What language does G generate?</li>
  <li>Construct an SLR(1) parsing table for G.</li>
  <li>Explain why the parsing action conflicts arise in the parsing table.</li>
  <li>Construct an equivalent LALR(1) grammar for L(G).</li>
  <li>Show that your grammar is LALR(1) by using yacc to
      construct an LALR(1) parsing table for it.</li>
  <li>Is your grammar LL(1)?</li>
 


 <h2>7. Reading</h2>
 <ul>
  <li>ALSU, Sects. 4.8 and 4.9</li>

 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 12: March 4, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 12: Syntax-Directed Translation<br/>
  March 4, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>The "dangling-else" ambiguity</li>
  <li>Syntax-directed definitions and translation schemes</li>
  <li>Synthesized and inherited attributes</li>
  <li>S-attributed SDDs</li>
  <li>L-attributed SDDs</li>
  <li>Reading</li>
 </ol>

 <h2>1. The "Dangling-Else" Ambiguity</h2>
 <ul>
  <li>Consider the following simplified ambiguous grammar for if- and if-else statements:</li>
</ul><pre><code>
S' &#8594; S
S  &#8594; i S e S | i S | a
</code></pre>
  <li>Here the symbol <code>i</code> stands for <code>if expr then</code>,
      the symbol <code>e</code> stands for <code>else</code>, and
      the symbol <code>a</code>
      stands for all other productions.
      We have also added an augmenting production <code>S' &#8594; S</code>.</li>

  <li>The canonical collections of sets of LR(0) items for this grammar is as follows:</li>
<pre><code>
I<sub>0</sub>: S' &#8594; .S			I<sub>3</sub>: S &#8594; a.
    S  &#8594; .iSeS
    S  &#8594; .iS			I<sub>4</sub>: S &#8594; iS.eS
    S  &#8594; .a			    S &#8594; iS.
<sp>				
I<sub>1</sub>: S' &#8594; S.			I<sub>5</sub>: S &#8594; iSe.S
				    S &#8594; .iSeS				    
I<sub>2</sub>: S  &#8594; i.SeS			    S &#8594; .iS
    S  &#8594; i.S			    S &#8594; .a
    S  &#8594; .iSeS
    S  &#8594; .iS			I<sub>6</sub>: S &#8594; iSeS.
    S  &#8594; .a
</sp></code></pre>
  <li>The set of items <code>I<sub>4</sub></code> gives rise to a shift/reduce
      conflict. The item <code>S &#8594; iS.eS</code> calls for a shift on
      <code>e</code> and since FOLLOW(<code>S</code>} = {<code>e</code>, $},
      the item <code>S &#8594; iS.</code> calls for a reduction by production
      <code>S &#8594; iS.</code> on <code>e</code>. To associate each <code>e</code>
      with the closest unelsed if, we should resolve the conflict in favor of shift
      to state 5.</li>
  <li>See Section 4.8.2 of ALSU for a more detailed discussion.</li>


 

 <h2>2. Syntax-Directed Definitions and Translation Schemes</h2>
  <ul>
  <li>The syntax analyzer translates its input token stream into an intermediate
      language representation of the source program, usually an abstract
      syntax tree (AST).</li>

  <li>A syntax-directed definition can be used to specify this translation.</li>

  <li>A syntax-directed definition (SDD) is a context-free grammar with attributes
      attached to grammar symbols and semantic rules
      attached to the productions.</li>

  <li>The semantic rules define values for attributes associated with the
      symbols of the productions. These values can be computed by
      creating a parse tree for the input and then making a sequence of
      passes over the parse tree, evaluating some or all of the rules on each
      pass.  SDDs are useful for specifying translations.</li>
  <li>A syntax-directed translation scheme (SDTS) is a context-free grammar with program
      fragments, called semantic actions, embedded within production bodies.
      SDTSs are useful for implementing syntax-directed definitions.</li>
 </ul>

 <h2>3. Synthesized and Inherited Attributes</h2>
 <ul>
  <li>Attributes are values computed at the nodes of a parse tree.</li>
  <li><i>Synthesized attributes</i>
      are values that are computed at a node <i>N</i> in a parse tree from attribute
      values of the children of <i>N</i> and perhaps <i>N</i> itself.
      Synthesized attributes can be
      easily computed by a shift-reduce parser that keeps the
      values of the attributes on the parsing stack.
      See Sect. 5.4.2 of ALSU.</li>
  <li>An SDD is <i>S-attributed</i> if every attribute is synthesized.
      S-attributed SDDs are useful for bottom-up parsing.</li>
  <li><i>Inherited attributes</i>
      are values that are computed at a node <i>N</i> in a parse tree from attribute
      values of the parent of <i>N</i>, the siblings of <i>N</i>, and <i>N</i> itself.</li>
  <li>An SDD is <i>L-attributed</i> is every attribute is either synthesized or
      inherited from the parent or from the left.
      L-attributed SDDs are useful for top-down parsing.
      See Sect. 5.2.4 of ALSU for details.</li>
 </ul>

 <h2>4. Examples of S-Attributed SDDs</h2>
 <ul>
  <li><b>Example 1.</b> Here is an S-attributed SDD translating signed bit strings
      into decimal numbers. The attributes, <code>BNum.val</code>,
      <code>Sign.val</code>, <code>List.val</code>, and
      <code>Bit.val</code>, are all synthesized attributes that
      represent integers.</li>

  </ul><pre><code>
   BNum &#8594; Sign List      { BNum.val = Sign.val &#215; List.val }
   Sign &#8594; +              { Sign.val = +1 }
   Sign &#8594; -              { Sign.val = -1 }
   List &#8594; List<sub>1</sub> Bit      { List.val = 2 &#215; List<sub>1</sub>.val + Bit.val }
   List &#8594; Bit            { List.val = Bit.val }
   Bit  &#8594; 0              { Bit.val = 0 }
   Bit  &#8594; 1              { Bit.val = 1 }
  </code></pre>

  <li><b>Example 2.</b> Here are Yacc translation rules implementing the SDD above
      for translating signed bit strings
      into decimal numbers. The identifiers $$, $1, $2 and so on in Yacc actions
      are synthesized attributes.</li>

  <pre><code>
   BNum : Sign List      { $$ = $1 * $2; }
        ;
   Sign : '+'            { $$ = +1; }
        | '-'            { $$ = -1; }
        ;
   List : List Bit       { $$ = 2*$1 + $2; }
        | Bit
        ;        
   Bit  : '0'            { $$ = 0; }
        | '1'            { $$ = 1; }
        ;
  </code></pre>


  <li><b>Example 3.</b> Here is an S-attributed SDD based on an SLR(1) grammar that
      translates arithmetic expressions
      into ASTs.
      <code>E</code> has the synthesized attributed <code>E.node</code> and 
      <code>T</code> the synthesized attribute <code>T.node</code>.
      <code>E.node</code> and <code>T.node</code> point to a node in the AST.
      The function <code>Node(op, left, right)</code>
      returns a pointer to a node with three fields:
      the first labeled <code>op</code>, the second a pointer
      to a left subtree, and the third a pointer to a right subtree.
      The function <code>Leaf(op, value)</code> returns a pointer to a node
      with two fields: the first labeled <code>op</code>, the second the
      value of the token. See Example 5.11 in ALSU.</li>

  <pre><code>
   E &#8594; E<sub>1</sub> + T     { E.node = Node('+', E<sub>1</sub>.node, T.node); }

   E &#8594; T          { E.node = T.node; }

   T &#8594; ( E )      { T.node = E.node; }

   T &#8594; id         { T.node = Leaf(id, id.entry); }

  </code></pre>
 
 
 <h2>5. Example of an L-Attributed SDD</h2>
 <ul>
  <li><b>Example 4.</b> Here is an L-attributed SDD based on an LL(1) grammar for translating arithmetic
      expressions into ASTs.  See Example 5.12 in ALSU.</li>
  </ul><pre><code>
   E &#8594; T A        { E.node = A.s;
                    A.i = T.node; }

   A &#8594; + T A<sub>1</sub>     { A1.i = Node('+', A.i, T.node);
                    A.s = A<sub>1</sub>.s; }

   A &#8594; &#949;          { A.s = A.i; }

   T &#8594; ( E )      { T.node = E.node; }

   T &#8594; id         { T.node = Leaf(id, id.entry); }
  </code></pre>
 


 <h2>6. Practice Problems</h2>
 <ol>
  <li>Using Yacc, implement a syntax-directed translator that
      translates sequences of postfix Polish expressions into infix notation.
      For example, your translator should map <code>345+*</code>
      into <code>3*(4+5)</code>.</li>
  <li>Optimize your translator so it doesn't generate any redundant parentheses.
      For example, your translator should still map <code>345+*</code>
      into <code>3*(4+5)</code> but it should map <code>345*+</code>
      into <code>3+4*5</code>.</li>
 </ol>

 <h2>7. Reading</h2>
 <ul>
  <li>ALSU, Sects. 5.1-5.4</li>
 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 13: March 6, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 13: Intermediate Representations<br/>
  March 6, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Syntax-directed translation</li>
  <li>Variants of syntax trees</li>
  <li>Three-address code</li>
  <li>Semantic analysis</li>
 </ol>

 <h2>1. Syntax-Directed Translation</h2>
 <ul>
  <li>Postfix translation schemes</li>
  <li>Translation schemes with actions inside productions</li>
  <li>Producing ASTs with top-down parsing</li>
  <ul>
   <li>Here is the L-attributed SDD based on an LL(1) grammar for translating arithmetic
      expressions into ASTs from Lecture 12.</li>
  </ul></ul><pre><code>
   E &#8594; T A        { E.node = A.s;
                    A.i = T.node; }

   A &#8594; + T A<sub>1</sub>     { A1.i = Node('+', A.i, T.node);
                    A.s = A<sub>1</sub>.s; }

   A &#8594; &#949;          { A.s = A.i; }

   T &#8594; ( E )      { T.node = E.node; }

   T &#8594; id         { T.node = Leaf(id, id.entry); }
  </code></pre>
  
 

 <h2>2. Variants of Syntax Trees</h2>
 <ul>
  <li>Abstract syntax trees</li>
  <li>Directed acyclic graphs</li>
  <ul>
   <li>Algorithm 6.3: Value-number method for constructing a DAG (p. 361)</li>
  </ul>
 </ul>

 <h2>3. Three-Address Code</h2>
 <ul>
  <li>Three-address instructions</li>
  <li>Representations for three-address code</li>
  <ul>
   <li>Records</li>
   <li>Quadruples</li>
   <li>Triples</li>
  </ul>
  <li>Static single-assignment form</li>
 </ul>
  
 <h2>4. Semantic Analysis</h2>
 <ul>
  <li>Uses made of semantic information for a variable <code>x</code>:</li>
  <ul>
   <li>What kind of value is stored in <code>x</code>?</li>
   <li>How big is <code>x</code>?</li>
   <li>Who is responsible for allocating space for <code>x</code>?</li>
   <li>Who is responsible for initializing <code>x</code>?</li>
   <li>How long must the value of <code>x</code> be kept?</li>
   <li>If <code>x</code> is a procedure, what kinds of arguments does it take and what
       kind of return value does it have?</li>
  </ul>
  <li>Storage layout for local names</li>
 </ul>


 <h2>5. Practice Problems</h2>
 <ol>
  <li>Construct a DAG for the expression</li>
      <dt><code>((x + y) - ((x + y) * (x - y))) + ((x + y) * (x - y))</code></dt>
  <li>Translate the following assignments into (a) syntax trees, (b) quadruples, (c) triples,
      (d) three-address code:</li>
  <ol style="list-style-type: lower-alpha;">
   <li><code>x = a + -(b+c)</code></li>
   <li><code>x[i] = y[i] + z[i]</code></li>
   <li><code>x = f(y+1) + 2</code></li>
  </ol>
 </ol>

 <h2>6. Reading</h2>
 <ul>
  <li>ALSU, Sections 6.1-6.3</li>
 </ul><br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 14: March 11, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 14: Midterm Review<br/>
  March 11, 2013
 </h1>

 <h2>1. What you should know for the midterm</h2>
 <ol>
  <li>The different kinds of programming languages</li>
  </ol><ul>
   <li>Lecture 1</li>
  </ul>
  <li>The fundamental elements of programming languages</li>
  <ul>
   <li>Lecture 2, ALSU Ch.1</li>
  </ul>
  <li>Language processing tools</li>
  <ul>
   <li>Lecture 2, ALSU Ch. 1</li>
  </ul>
  <li>The structure of a compiler</li>
  <ul>
   <li>Lecture 3, ALSU Chs. 1 and 2</li>
  </ul>
  <li>Regular languages, regular expressions, finite automata</li>
  <ul>
   <li>Lectures 4 and 5, ALSU Ch. 3 except for Sect. 3.9</li>
  </ul>
  <li>Lexical analysis</li>
  <ul>
   <li>Lectures 4 and 5, ALSU Chs. 2 and 3 except for Sect. 3.9</li>
  </ul>
  <li>Context-free languages and grammars</li>
  <ul>
   <li>Lectures 6 and 7, ALSU Ch. 4 except for Sect. 4.7</li>
  </ul>
  <li>Top-down parsing</li>
  <ul>
   <li>Lecture 9, ALSU Chs. 2 and 4 except for Sect. 4.7</li>
  </ul>
  <li>Bottom-up parsing</li>
  <ul>
   <li>Lectures 10 and 11, ALSU Ch. 4 except for Sect. 4.7</li>
  </ul>
  <li>Syntax-directed translation</li>
  <ul>
   <li>Lectures 12 and 13, ALSU Chs. 2 and Ch. 5 except for Sect. 5.5</li>
  </ul>
 

 <h2>2. Automata and Language Theory Review</h2>
 <ul>
  <li>Regular languages</li>
  <ul>
   <li>Finite automata</li>
   <li>Regular expressions</li>
   <li>Closure properties of regular languages</li>
   <li>Decision properties of regular languages</li>
   <li>Pumping lemma for regular languages and its uses
  </li></ul>
  <li>Context-free languages</li>
  <ul>
   <li>Context-free grammars</li>
   <li>Parse trees, derivations, and ambiguity</li>
   <li>Pushdown automata and deterministic pushdown automata</li>
   <li>Closure properties of CFLs</li>
   <li>Decision properties of CFLs</li>
   <li>Pumping lemma for CFLs and its uses
  </li></ul>
  <li>Syntax-directed translation</li>
  <ul>
   <li>Syntax-directed definitions and translation schemes</li>
   <li>Attribute grammars, inherited and synthesized attributes</li>
   <li>S-attributed and L-attributed SDDs</li>
  </ul>
 </ul>

 <h2>3. Not all LL(1) grammars are SLR(1) and vice versa</h2>
 <ul>
  <li>An LL(1) grammar that is not SLR(1)</li>
</ul><pre><code>
S &#8594; AaAb | BbBa
A &#8594; &#949;
B &#8594; &#949;
</code></pre>

  <li>An SLR(1) grammar that is not LL(1)</li>
<pre><code>
S &#8594; SA | A
A &#8594; a
</code></pre>

 




<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
 <title>
  Lecture 15: March 25, 2013
 </title>
</head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 15: Intermediate Code Generation<br/>
  March 25, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Intermediate Representations</li>
  <li>Semantic analysis</li>
  <li>Purpose of types in programming languages</li>
  <li>Type systems</li>
  <li>Typing in programming languages</li>
  <li>Type inference rules</li>
  <li>Type conversions</li>
 </ol>

 <h2>1. Intermediate Representations</h2>
 <ul>
  <li>Abstract syntax trees</li>
  <li>Directed acyclic graphs (DAGs)</li>
  <li>Three-address code</li>
  <ul>
   <li>Addresses</li>
   <ul>
    <li>Names</li>
    <li>Constants</li>
    <li>Compiler-generated temporaries</li>
   </ul>
   <li>Instructions</li>
   <ul>
    <li>Three-address code has instructions of various forms for
        assignments, conditional and unconditional jumps, procedure calls and returns, indexed copy,
        address and pointer assignments.</li>
   </ul>
   <li>Common representations for three-address code include
      records, quadruples, and triples.</li>
  </ul>
  <li>Static single-assignment (SSA) form is an intermediate reprentation that facilitates
      certain code optimizations. Assignments in SSA are to variables with distinct names.
      It also uses a special function, called a &#966;-function, to combine two definitions of the same variable
      arising from two different control-flow paths.</li>
 </ul>
  
 <h2>2. Semantic Analysis</h2>
 <ul>
  <li>The semantic analyzer uses the syntax tree and information in the symbol
      table to check the source program for semantic consistency with the
      language definition.</li>
  <li>It gathers type information for intermediate code generation.</li>
  <li>Type checking is an important part of semantic analysis. During type
      checking the compiler checks that each operator has compatible operands.</li>
  <li>Uses made of semantic information for a variable <code>x</code>:</li>
  <ul>
   <li>What kind of value is stored in <code>x</code>?</li>
   <li>How big is <code>x</code>?</li>
   <li>Who is responsible for allocating space for <code>x</code>?</li>
   <li>Who is responsible for initializing <code>x</code>?</li>
   <li>How long must the value of <code>x</code> be kept?</li>
   <li>If <code>x</code> is a procedure, what kinds of arguments does it take and what
       kind of return value does it have?</li>
  </ul>
  <li>Storage layout for local names</li>
 </ul>

 <h2>3. Purpose of Types in Programming Languages</h2>
 <ul>
  <li>Virtually all high-level programming languages associate types with values.</li>
  <li>Types often provide an implicit context for operations.
      For example, in C the expression <code>x + y</code> will use integer addition
      if <code>x</code> and <code>y</code> are <code>int</code>'s, and floating-point
      addition if <code>x</code> and <code>y</code> are <code>float</code>'s.</li>
  <li>Types can catch programming errors at compile time by making sure
      operators are applied to semantically valid operands. For example, a Java compiler
      will report an error if <code>x</code> and <code>y</code> are <code>String</code>'s
      in the expression <code>x * y</code>.
 </li></ul>

 <h2>4. Type Systems</h2>
 <ul>
  <li>The type of a construct in a program can be denoted by a type expression.</li>
  <li>A type expression is either a basic type (e.g., <code>integer</code>) or
      a type constructor applied to a type expression (e.g., a function from an integer
      to an integer).</li>
  <li>A type system is a set of rules for assigning type expressions to the
      syntactic constructs of a program and for specifying</li>
  <ul>
   <li>type equivalence (when the types of two values are the same),</li>
   <li>type compatibility (when a value of a given type can be used in
       a given context), and</li>
   <li>type inference (rules that determine the type of a language construct based
       on how it is used).</li>
  </ul>
  <li>Forms of type equivalence</li>
  <ul>
   <li>Name equivalence: two types are equivalent iff they have the same name.</li>
   <li>Structural equivalence: two types are equivalent iff they have the same structure.</li>
   <li>To test for structural equivalence, a compiler must encode the structure
       of a type in its representation. A tree (or type graph) is typically
       used.</li>
  </ul>
  <li>A type checker makes sure that a program obeys the type-compability rules
      of the language.</li>
  <li>We can think about types in several different ways:</li>
  <ul>
   <li>Denotational: a type is a set of values called a domain.</li>
   <li>Constructive: a type is either a primitive type (such as an integer or a character)
       or a composite type created by applying a type constructor (such as a structure
       or an array) to simpler types.</li>
   <li>Abstraction-based: a type is an interface consisting of a set of operations
      with well-defined and mutually consistent semantics.</li>
  </ul>
 </ul>

 <h2>6. Typing in Programming Languages</h2>
 <ul>
  <li>The type system of a language determines whether type checking can be
      be performed at compile time (statically) or at run time (dynamically).</li>
  <li>A statically typed language is one in which all constructs of a language can be
      typed at compile type. C, ML, and Haskell are statically typed.</li>
  <li>A dynamically typed language is one in which some of the constructs of a language
      can only be
      typed at run time. Perl, Python, and Lisp are dynamically typed.</li>
  <li>A strongly typed language is one in which the compiler can guarantee that the
      programs it accepts will run without type errors.
      ML and Haskell are strongly typed.</li>
  <li>A type-safe language is one in which the only operations that can be performed
      on data in the language are those sanctioned by the type of the data. [Vijay
      Saraswat]</li>
 </ul>

 <h2>7. Type Inference Rules</h2>
 <ul>
  <li>Type inference rules specify for each operator the mapping between the types
      of the operands and the type of the result.</li>
  <li>E.g., result types for <code>x + y</code>:</li>
   </ul><pre><code>
   </code></pre><table border="1">
    <tbody><tr>
     <th><code>&#160;+&#160;</code></th>
     <th><code>int</code></th>
     <th><code>float</code></th>
    </tr>
    <tr>
     <td style="text-align: center;"><code>&#160;<b>int</b>&#160;</code></td>
     <td style="text-align: center;"><code>&#160;int&#160;</code></td>
     <td style="text-align: center;"><code>&#160;float&#160;</code></td>
    </tr>
    <tr>
     <td style="text-align: center;"><code><b>float</b></code></td>
     <td style="text-align: center;"><code>float</code></td>
     <td style="text-align: center;"><code>float</code></td>
    </tr>
    </tbody>
   </table>
   <br/>

  <li>Operator and function overloading</li>
  <ul>
   <li>In Java the operator <code>+</code> can mean addition or string concatenation
       depending on the types of its operands.</li>
   <li>We can choose between two versions of an overloaded function by
       looking at the types of their arguments.</li>
  </ul>
  <li>Function calls</li>
  <ul>
   <li>Compiler must check that the type of each actual parameter is compatible with
       the type of the corresponding formal parameter. It must check that the
       type of the returned value is compatible with the type of the function.</li>
   <li>The <i>type signature</i> of a function specifies the types of the formal
       parameters and the type of the return value.</li>
   <li>Example: <code>strlen</code> in C</li>
   <ul>
    <li>Function prototype in C:</li>
    </ul></ul><pre><code>
    unsigned int strlen(const char *s);
    </code></pre>
    <li>Type expression:</li>
    <pre><code>
    strlen: const char * &#8594; unsigned int
    </code></pre>
   
  

  <li>Polymorphic functions</li>
  <ul>
   <li>A polymorphic function allows a function to manipulate data structures
       regardless of the types of the elements in the data structure</li>
   <li>Example: Fig. 6.28 (p. 391) -- an ML program for the length of a list</li>
  </ul>
 

 <h2>8. Type Conversions</h2>
 <ul>
  <li>Implicit type conversions</li>
  <ul>
   <li>In an expression like <code>f + i</code> where <code>f</code> is a float and
       <code>i</code> is an integer a compiler must first convert the integer to a
       float before the floating point addition operation is performed. That is, the
       expression must be transformed into an intermediate representation like</li>
       </ul></ul><pre><code>
         t1 = INTTOFLOAT i
         t2 = x FADD t1
       </code></pre>
  
  <li>Explicit type conversions</li>
  <ul>
   <li>In C, explicit type conversions can be forced ("coerced") in an expression using a
       unary operator called a cast.  E.g., <code>sqrt((double) n)</code> converts the
       value of the integer <code>n</code> to a <code>double</code> before passing it
       on to the square root routine <code>sqrt</code>.</li>
  </ul>
 

 <h2>9. Practice Problems</h2>
 <ol>
 <dt>The following grammar generates programs consisting of
     a sequence of declarations D followed by a single expression E.
     Each identifier must be declared before its use.</dt>
  </ol><ul>
   </ul><pre><code>
     P &#8594; D ; E
     D &#8594; D ; D | T id
     T &#8594; int | float | T [ num ]
     E &#8594; num | id | E [ E ] | E + E
   </code></pre>
  
  <li>Construct type expressions as in Section 6.3.1 (pp. 371-372)
      for the following programs:</li>
  <ol style="list-style-type: lower-alpha;">
   <li>int a; int b; a + b
   </li><li>float[10][20] a; a[1] + a[2]</li>
  </ol>
  <li>Write pseudcode for a function <code>sequiv(exp1, exp2)</code> that will test the
      structural equivalence of two type expressions <code>exp1</code> and <code>exp2</code>.</li>
  <li>Show how your function computes
      <code>sequiv(array(2, array(2, int)), array(2, array(3, int))).</code></li>
 

  
 <h2>10. Reading</h2>
 <ul>
  <li>ALSU, Sects. 6.1-6.4.</li>
 </ul><br/>



 <br/><br/>





<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>
</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 16, March 27, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 16: Translation of Statements<br/>
  March 27, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Logical rules of inference for type checking</li>
  <li>Run-time storage organization</li>
  <li>Translation of assignments</li>
  <li>arrays</li>
  <li>Boolean expressions</li>
  <li>If-statements</li>
  <li>While-statements</li>
 </ol>

 <h2>1. Logical Rules of Inference for Type Checking</h2>
 <ul>
  <li>Type inference templates</li>
  <ul>
   <li>We can specify the type inference rule "if expression <code>e1</code> has the
       type <code>int</code> and expression <code>e2</code> has the type <code>int</code>,
       then the expression <code>e1 + e2</code> has the type <code>int</code>" with a type inference
       template of the form</li>
</ul></ul><pre>
&#8866; e1: int &#160;&#160;&#160; &#8866; e2: int
_______________________

&#160;&#160;&#160; &#8866; e1 + e2: int
</pre>

   <li>The turnstile symbol &#8866; is read "it is provable that"
       so the template can be read as "if it is provable that <code>e1</code> has type <code>int</code>
       and it is provable
       that <code>e2</code> has type <code>int</code>,
       then it is provable that <code>e1 + e2</code> has type <code>int</code>."</li>
   <li>Templates of this form provide a compact way of expressing the type
       rules of a language.</li>
   <li>We say a type system is sound if whenever &#8866; <i>e</i>: <i>T</i> then <i>e</i> evaluates
       to a value of type <i>T</i>.</li>
   <li>We can apply the type-inference templates by making a bottom-up traversal of the AST.
       We determine the types of the leaves using information from the symbol table.
       We can then move up the tree determining the type of the interior nodes from the types
       of their children by applying the inference rule for the operator at a given interior node.</li>
  

 <li>Type environments</li>
 <ul>
  <li>A type environment is often needed to determine the type of a variable
      at a given node in the AST.
      A type environment is just a mapping from variables to types that is stored in the symbol table.
      The type environment for each node can be determined by making
      a top-down pass over the AST respecting the type scoping rules of the language.</li>
  <li>The type inference rules are augmented with the type environment information.
      For example, if <code>E</code> is a type environment, we modify the template to make type
      inferences within the context of <code>E</code>:</li>
</ul><pre>
E &#8866; e1: int &#160;&#160;&#160; E &#8866; e2: int
____________________________

&#160;&#160;&#160; E &#8866; e1 + e2: int
</pre>
   <li>Static type checking can be done by making a top-down pass to compute the
       type environment for each node followed by a bottom-up pass to check the types
       at each node.</li> 
  
 


 <h2>2. Run-time Storage Organization</h2>
 <ul>
  <li>Run-time memory layout</li>
  <ul>
   <li>Typical memory layout</li>
   </ul></ul><pre><code>
   (low address) Code
                 Static data
                 Runtime heap
                   &#8595;

                 Free memory

                   &#8593;                                 
  (high address) Runtime stack
   </code></pre>
  
 

 <h2>3. Translation of Assignments</h2>
 <ul>
  <li>Here is Fig. 6.20 (p. 381), an SDTS generating three-address code for
      assignments:</li>
</ul><pre><code>
S &#8594; id = E ; { gen(top.get(id.lexeme) '=' E.addr); }
E &#8594; E<sub>1</sub> + E<sub>2</sub>  { E.addr = new Temp();
               gen(E.addr '=' E<sub>1</sub>.addr '+' E<sub>2</sub>.addr); }
  | - E<sub>1</sub>     { E.addr = new Temp();
               gen(E.addr '=' 'uminus' E<sub>1</sub>.addr); }
  | ( E<sub>1</sub> )   { E.addr = E<sub>1</sub>.addr; }
  | id       { E.addr = top.get(id.lexeme); }
</code></pre>

  <li>Example. Here is the three-address code generated by this SDTS
      for the assignment statement:  <code>a = b + -c;</code></li>
  <pre><code>
  t1 = uminus c
  t2 = b + t1
  a = t2
  </code></pre>
 

 <h2>4. Arrays</h2>
 <ul>
  <li>Referencing a one-dimensional array</li>
  <ul>
   <li>In C and Java, array elements are numbered <code>0, 1,..., n-1</code>
       for an array <code>A</code> with n elements.</li>
   <li>Element <code>A[i]</code> begins in location <code>(base + i &#215; w)</code>
       where <code>base</code> is the relative address of the storage allocated for
       <code>A</code> and <code>w</code> is the width of each element.</li>
  </ul>
  <li>Common layouts for multidimensional arrays</li>
  <ul>
   <li>Row-major order</li>
   <li>Column-major order</li>
  </ul>
  <li>See Fig. 6.22 (p. 383) for an SDD generating three-address code for
      assignments with array references.</li>
  <li>Example: three-address code for the expression
      <code>c + a[i][j]</code> assuming the width of an integer is 4</li>
  </ul><pre><code>
  t1 = i * 12
  t2 = j * 4
  t3 = t1 + t2
  t4 = a[t3]
  t5 = c + t4
  </code></pre>
 

 <h2>5. Boolean Expressions</h2>
 <ul>
  <li>Boolean expressions are composed of boolean operators (&amp;&amp;, ||, !)
      applied to boolean variables, relational expressions, and other
      boolean expressions.</li>
  <li>Short-circuit evaluation: Some languages, such as C and Java, do not require an entire boolean
      expression to be evaluated.</li>
  <ul>
   <li>Given <code>x &amp;&amp; y</code>, if <code>x</code>
       is false, then we can conclude the entire expression is false without
       evaluating <code>y</code>.</li>
   <li>Given <code>x || y</code>, if <code>x</code>
       is true, then we can conclude the entire expression is true without
       evaluating <code>y</code>.</li>
  </ul>
  <li>Numerical encoding</li>
  <ul>
   <li>In C, the numerical value 0 represents false; a nonzero value represents true.</li>
  </ul>
  <li>Positional encoding</li>
  <ul>
   <li>The value of a boolean expression can be represented by a position in three-address
       code, and the boolean operators can be translated into jumps.</li>
   <li>The expression</li>
       </ul></ul><pre><code>
       if (x &lt; 100 || x &gt; 200 &amp;&amp; x != y)
         x = 0;
       </code></pre>
       <dt>can be translated into the following three-address instructions:</dt>
       <pre><code>
           if x &lt; 100 goto L2
           ifFalse x &gt; 200 goto L1
           ifFalse x != y goto L1
       L2: x = 0
       L1:
       </code></pre>
   
 

 <h2>6. Translation of If-statements</h2>
 <ul>
  <li>Boolean expressions often appear in the context of flow-of-control statements
      such as:</li>
  <ul>
   <li>If statements</li>
   <li>If-else statement </li>
  </ul>
  <li>See Figs. 6.36 (p. 402) and 6.37 for SDDs translating these statements
      with booleans into three-address code.</li>
  <li>For the expression</li>
       </ul><pre><code>
       if (x &lt; 100) || x &gt; 200 &amp;&amp; x != y)
         x = 0;
       </code></pre>
       <dt>these SDDs produce the following three-address instructions:</dt>
       <pre><code>
           if x &lt; 100 goto L2
           goto L3
       L3: if x &gt; 200 goto L4
           goto L1
       L4: if x != y goto L2
           goto L1
       L2: x = 0
       L1:
       </code></pre>
       <dt>This code can be transformed into the code in Section 5 by eliminating the redundant goto
           and changing the directions of the tests in the second and third if-statements.</dt>
 

 <h2>7. Translation of While-statements</h2>
 <ul>
  <li>Consider the production <code>S &#8594; while ( B ) S1</code>
      for while-statements.
      The shape of the code for implementing this production
      can take the form:</li>

  </ul><pre><code>
   begin: // beginning of code for S
      code to evaluate B
      if B is true goto B.true
      if B is false goto B.false
   B.true:
      code to evaluate S1
      goto begin
   B.false:  // this is where control flow will go after executing S
  </code></pre>

  <li>Here is an SDD for this translation (from Fig. 6.36, p. 402):</li>

  <pre><code>
   S &#8594; while ( B ) S1 {
            begin = newlabel()
            B.true = newlabel()
            B.false = S.next
            S1.next = begin
            S.code = label(begin) || B.code ||
                     label(B.true) || S1.code ||
                     gen('goto' begin)
   }
  </code></pre>

 

 <h2>8. Practice Problems</h2>
 <ol>
  <li>Use the SDD of Fig. 6.22 (ALSU, p. 383) to translate the assignment
      <code>x = a[i][j] + b[i][j]</code>.</li>
  <li>Add rules to the SDD in Fig. 6.36 (ALSU, p. 402) to translate
      do-while statements of the form:</li>
      <dt>&#160;&#160;&#160;<i>S</i> &#8594; <b>do</b> <i>S</i> <b>while</b> <i>B</i></dt>
      <dt>Show the code your SDD would generate for the program</dt><dt>
  <pre><code>
     do
       do
         assign1
       while a &lt; b
     while c &lt; d
  </code></pre>
 </dt></ol>


 <h2>9. Reading</h2>
 <ul>
  <li>ALSU, Sections 6.4 - 6.8, 7.1</li>
 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 17, April 1, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 17: Procedures<br/>
  April 1, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Names</li>
  <li>Procedures</li>
  <li>Parameter-passing mechanisms</li>
  <li>Evaluation strategies</li>
  <li>Storage-allocation strategies</li>
  <li>Activation trees and records</li>
 </ol>

 <h2>1. Names</h2>
 <ul>
  <li>A name is a character string used to represent something.</li>
  <li>Names in most languages are identifiers.</li>
  <li>In some programming languages, certain identifiers fall into distinct namespaces that
      do not interfere with one another.
      For example, in the C structure declaration</li>
  <ul>
</ul></ul><pre><code>
struct id {  /* here id is a structure tag */
  int id;    /* here id is a structure member */
} id;        /* here id is a structure variable */
</code></pre>
  
<dt>the structure tag <code>id</code>, the structure member <code>id</code>,
and the structure variable <code>id</code>
are in different namespaces and hence distinct identifiers that
can be distinguished by context.</dt>

  <li>A binding is an association between two things such as between a
      name and its type or between a symbol and the operation it represents.
      The time at which this association is determined is called
      the binding time. Bindings can take place
      from language design time to runtime.

  </li><li>The textual region of a program in which a binding is active
      is called its scope. A scope is often with respect to a namespace.</li>
  <ul>
   <li>In a language with static scoping,
       the bindings between names and objects are determined at compile time
       by examining the text of the program.
       Static scoping is sometimes called lexical scoping.
       In static scoping, a name refers to its lexically closest declaration.</li>
   <li>In a language with dynamic scoping,
       the bindings between names and objects depend on the order in which
       procedures are called at runtime.</li>
  </ul>

  <li>Lifetimes</li>
  <ul>
   <li>The lifetime of a name-object binding is the time between the creation and
       destruction of that binding.</li>
   <li>The lifetime of an object is the time between the creation and destruction of the
       object. Depending on the language,
       the lifetime of an object may be different than that of lifetime of the name-object binding.</li>
   <ul>
    <li>In C, an object is a location in storage. The storage class determines the lifetime of the
        storage associated with an object.</li>
    <li>In C, there are two storage classes: automatic and static. Automatic objects are local to
        a block and are discarded on exit from the block. Static objects retain their values across
        entry from and reentry to functions and blocks.</li>
   </ul>
   <li>Object lifetimes are usually determined by the storage-allocation strategy
       used to manage the storage for that object.</li>
   <ul>
    <li>Static objects are allocated memory in the code space and have an address that is retained
        throughout the execution of a program.</li>
    <li>Stack objects are allocated in a last-in, first-out order on the runtime stack
        usually in conjunction with
        procedure calls and returns.</li>
    <li>Heap objects are dynamically allocated and deallocated at arbitrary times on the runtime heap.
        Some languages such as Java and C# use a garbage collection mechanism to
        identify and reclaim heap objects that
        become unreachable during program execution.</li>
   </ul>
  </ul>
       
 


 <h2>2. Procedures</h2>
 <ul>
  <li>A procedure <i>P</i> in a programming language is a collection
      of statements that defines a parameterized computation.
      An invocation of <i>P</i> is called an activation of <i>P</i>.</li>
  <li>We use the term <i>actual parameters</i> to denote the parameters
      used in the call of a procedure.</li>
  <li>We use the term <i>formal parameters</i> to denote the parameters
      used in the definition of a procedure.</li>
  <li>We will often call a procedure that returns a value a <i>function</i>.
      (C uses the term function for procedure as well.)</li>
  <li>The type of the function <code>return_type f(arg1_type a, arg2_type b)</code>
      can be denoted by the type expression</li>
      <dt>&#160;&#160;&#160;<code>arg1_type x arg2_type &#8594; return_type</code></dt>
  <li>Some design issues for implementing procedures</li>
  <ul>
   <li>choice of parameter-passing mechanism</li>
   <li>storage allocation for local variables: static or dynamic</li>
   <li>can procedure declarations nest</li>
   <li>can procedures be passed as parameters, returned as values</li>
   <li>can procedure names be overloaded</li>
   <li>generic procedures, ones whose computations can be done on different types</li>
   <li>does language have closures (encapsulations of procedures with their runtime context)</li>
  </ul>
 </ul>

 <h2>3. Parameter-Passing Mechanisms</h2>
 <ul>
  <li>Programming languages differ in how the values of parameters are passed to called procedures.</li>
  <li>Call by value</li>
  <ul>
   <li>The actual parameter is evaluated if it is an expression or copied
       if it is a variable. The r-value is placed in the location
       belonging to the corresponding formal parameter of the called procedure.</li>
   <li>C and Java use call by value. C leaves the order in which the parameters
       are evaluated unspecified;
       Java evaluates the parameters left to right.</li>
   <li>"swap" example from C</li>
   <ul>
    <li>Consider the following C program fragment</li>
    </ul></ul></ul><pre><code>
      a = 1;
      b = 2;
      swap(a, b);
      printf("a = %d, b = %d\n", a, b);
    </code></pre>
    <dt>where the function <code>swap</code> is defined as
    <pre><code>
      void swap(int x, int y) {
        int temp;
        temp = x;
        x = y;
        y = temp;
      }
    </code></pre>
    <li>Now consider the same program fragment with
        <code>swap(&amp;a, &amp;b)</code> in place of
        <code>swap(a, b)</code> and with
        <code>swap</code> defined as</li>
    <pre><code>
      void swap(int *px, int *py) {
        int temp;
        temp = *px;
        *px = *py;
        *py = temp;
      }
    </code></pre>


   </dt>
  

  <li>Call by reference</li>
  <ul>
   <li>The address of the actual parameter is passed to the callee as the
       value of the corresponding formal parameter.</li>
   <li>If the parameter is an expression, the expression is evaluated and its value
       is stored in a new location
       before the call. The address
       of that location is passed.</li>
   <li>Useful for passing large parameters to procedures.</li>
   <li>Used for reference parameters in C++. In C++, <code>swap</code> can be written
       with reference parameters as</li>
    </ul><pre><code>
      void swap(int &amp;x, int &amp;y) {
        int temp;
        temp = x;
        x = y;
        y = temp;
      }
    </code></pre>
       <dt>In the body, <code>x</code> and <code>y</code> are <code>int</code>'s,
           not pointers to <code>int</code>'s. The caller passses as parameters
           the variables whose values are to be swapped, not their
           addresses.</dt>
  

  <li>Call by name</li>
  <ul>
   <li>A call-by-name parameter is re-evaluated in the caller's
       referencing environment each time it is used. The effect is
       as though the called procedure is textually expanded at the
       point of the call with each actual parameter substituted
       for the corresponding formal parameter at every occurrence
       in the body of the procedure.
       Local names in the called procedure may need to be
       renamed to keep them distinct.</li>
   <li>Used in Algol 60.</li>
   <li>Also used at compile time by macros in the C preprocessor.</li>
   <li>Example: Consider the macro definition in C</li>
   <ul><dt><code>#define max(a, b)   ((a) &gt; (b) ? (a) : (b))</code></dt></ul>
   <dt>The C statement</dt>
   <ul><dt><code>x = max(p+q, r*s);</code></dt></ul>
   <dt>will be replaced by the statement</dt>
   <ul><dt><code>x = ((p+q) &gt; (r*s) ? (p+q) : (r*s);</code></dt></ul>
  </ul>
 

 <h2>4. Evaluation Strategies for the Arguments of a Procedure</h2>
  <ul>
   <li>An evaluation strategy defines when and in what order the parameters
       to a procedure are evaluated.</li>
   <li>In applicative-order evaluation, all parameters are evaluated
       before applying the procedure. C functions
       and Java methods use applicative-order evaluation.</li>
   <li>In normal-order evaluation, parameters are evaluated after applying
       the procedure, and then only if the result is needed to complete the
       evaluation of the procedure. Normal-order evaluation is used with
       macros and call-by-name parameters.
       Haskell uses a memoized version of call by name called call by need.</li>
  </ul>
 

 <h2>5. Storage-Allocation Strategies</h2>
 <ul>
  <li>Static allocation</li>
  <ul>
   <li>Storage for all data objects is laid out at compile time.</li>
   <li>Names are bound to storage as program is compiled.</li>
   <li>Static allocation was used in early versions of Fortran.</li>
   <li>Recursion is restricted.</li>
   <li>Size of all data objects must be known at compile time.</li>
   <li>No dynamic data structures can be supported.</li>
  </ul>

  <li>Stack allocation</li>
  <ul>
   <li>Run-time storage is organized as a stack.</li>
   <li>Activation records (ARs) are pushed and popped as activations
       of procedures begin and end.</li>
   <li>Typical kinds of data appearing in an activation record:</li>

  </ul></ul><pre><code>
                 Actual parameters
                 Returned values
                 Control link
                 Access link
                 Saved machine status
                 Local data
                 Temporaries
  </code></pre>

   <li>Storage for the locals in each call is contained in the AR for that call.</li>
   <li>Used by C and Java.</li>
  

  <li>Heap allocation
   <ul>
   <li>Storage is allocated and deallocated as needed at run time from a data area called a heap.</li>
   <li>Necessary when data outlives the call to the procedure that created it.</li>
   <li>Also needed when the values of local names must be retained after an activation ends.</li>
  </ul>
 </li>

 <h2>6. Activation Trees and Records</h2>
 <ul>
  <li>Consider the following C program for Euclid's algorithm.</li>

</ul><pre><code>
#include <stdio.h>
int x, y;
int gcd(int u, int v) {
   if (v == 0)
      return u;
   else
      return gcd(v, u%v);
}

main() {
   scanf("%d%d", &amp;x, &amp;y);
   printf("%d\n", gcd(x, y));
   return 0;
}
</stdio.h></code></pre>

  <li>A tree, called an activation tree, can be used to represent the procedure calls made during
      an execution of gcd because the lifetimes of the procedure activations are nested.</li>
  <li>Note that:</li>
  <ul>
   <li>The sequence of procedure calls corresponds to a preorder traversal
       of the tree.</li>
   <li>The sequence of returns corresponds to a postorder travseral.</li>
   <li>The path from the root to a node <i>N</i> shows the activations
       that are live at the time <i>N</i> is executing.</li>
  </ul>
  <li>Procedure calls and returns are managed by a control stack.</li>
  <li>On each procedure call, an activation record for that procedure
      is pushed on the stack. The activation record for each procedure call contains the information
      needed to manage the execution of that procedure call.
      When the call returns, that activation
      record is popped from the stack.</li>
 


 <h2>7. Reading</h2>
 <ul>
  <li>ALSU, Sections 6.9, 7.1, 7.2</li>
 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<!-- Homework Assignment #3                         -->
<!-- Created by Al Aho on 4/1/2013                  -->
<!-- Last modified on 4/1/2013                      -->
<!-- ------------------------------------------------->
<title>Homework #3</title></head>
<body>
 <h1 style="text-align: left;"> COMS W4115
  <br/>Programming Languages and Translators
  <br/>Homework Assignment #3
  <br/>Submit solutions electronically on
  <br/>&#160;&#160;&#160;&#160;&#160;Courseworks/COMSW4115/Assignments
  <br/>&#160;&#160;&#160;&#160;&#160;by 2:40pm, April 10, 2013
 </h1>
 <hr/>

<h2>Instructions</h2>
 <ul>
  <li>Problems 1-4 are each worth 25 points.</li>
  <li>You may discuss the questions with the TAs and others in the class but your answers
      must be in your own words. You must not copy someone
      else's solutions.
      If you consult others or use external sources, please cite the
      people or sources used in your answers.</li>
  <li>Solutions to these problems will be posted
      on Courseworks on April 17, 2013.</li>
  <li>This assignment may submitted electronically on Courseworks by 2:40pm,
      April 17, 2013 for 50% credit.</li>
  <li>Pdf files are preferred.</li>
 </ul>


<h2>Problems</h2>
 <ol>

  <li>Consider the syntax-directed definitions in Figs. 6.19, 6.36 and 6.37 in ALSU
      for expressions, if-statements and booleans. Assume the boolean operators <code>&amp;&amp;</code>,
      <code>||</code>, and <code>!=</code> have the customary associativities and precedences.</li>

  <ol style="list-style-type: lower-alpha;">

   <li>Construct a parse tree for the C-like if-statement</li>
       <dt><code>if( x &lt; 10 &amp;&amp; x &gt; 20 || x != y ) x = 30;</code></dt>

   <li>Show the values of all the attributes computed at each node in the parse tree by these SDDs.</li>

   <li>Show the three-address code produced for this if-statement.</li>

   <li>Can you see any ways in which the three-address code can be optimized?</li>
  </ol>
  <br/>

  <li>Write pseudcode for a function <code>sequiv(exp1, exp2)</code> that will test the
      structural equivalence of two type expressions <code>exp1</code> and <code>exp2</code>.
      Show how your function computes
      <code>sequiv(array(2, array(2, int)), array(2, array(3, int))).</code></li>
  <br/>


  <li>Let <code>fib(n)</code> be the function</li>

<pre><code>
     int fib(n) {
       if (n == 0)
         return 0;
       else if (n == 1)
         return 1;
       else
         return fib(n-1) + fib(n-2);
     }
</code></pre>

   <ol style="list-style-type: lower-alpha;">  
    <li>Show the activation tree for <code>fib(3)</code>.</li>
    <li>Show the activation records that are on the run-time stack
        when <code>fib(1)</code> is invoked for the first time. Just show the return value,
        actual parameters, and the caller&#8217;s frame pointer in each activation record.</li>
   </ol>
   <br/>

  <li>Give an example from some common programming languages to illustrate the difference between</li>

  <ol style="list-style-type: lower-alpha;">
   <li>Normal-order evaluation and applicative-order evaluation.</li>

   <li>Call by reference and call by value.</li>

   <li>Lexical scope and dynamic scope.</li>

   <li>Stack allocation and heap allocation.</li>

   <li>A static type and a dynamic type.</li>
  </ol>
  <dt>You can use different programming languages for each part.</dt>


 </ol>
 <br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address> 

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 18, April 3, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 18: Run-time Environments<br/>
  April 3, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Activation trees</li>
  <li>Calling sequences</li>
  <li>Access to local arrays</li>
  <li>Heap memory management</li>
 </ol>

 <h2>1. Activation Trees</h2>
 <ul>
   <li>Consider the following C program:</li>
    </ul><pre><code>
    int x = 2;

    void f(int n) {
      static int x = 1;
      g(n);
      x--;
    }

    void g(int m) {
      int y = m-1;
      if (y &gt; 0) {
        f(y);
        x--;
        g(y);
      }
    }

    main() {
      g(x);
      return 0;
    }
    </code></pre>

   <li>The activation tree for this program is</li>
    <pre><code>
       main()
         |
        g(2)
        / \
     f(1) g(1)
      |
     g(1)
    </code></pre>

  


 <h2>2. Calling Sequences</h2>
 <ul>
  <li>Procedure calls are implemented by calling sequences, code that allocates
      an activation record on the control stack and enters information into its
      fields.</li>
  <li>A return sequence is code invoked after the call to restore the state
      of the machine so the calling procedure can continue its execution after
      the call.</li>
  <li>The code in a calling sequence is usually divided between the calling
      procedure (the "caller") and the procedure it calls (the "callee").</li>
  <li>When a procedure p calls a procedure q, we might do something like the following:</li>
  <ul>
   <li>Evaluate the parameters of q and store them in a new AR for q.</li>
   <li>Store the frame pointer (fp) for p as the control link in the AR for q.</li>
   <li>Update fp to point to the AR of q.</li>
   <li>Store the return address in the AR for q.</li>
   <li>Jump to the code for q.</li>
   <li>Have q allocate and initialize its local data and temporaries on the stack.</li>
  </ul>
  <li>When q exits:</li>
  <ul>
   <li>Copy the fp of the AR for q into sp (the top of stack pointer).</li>
   <li>Load the control link of the AR for q into the fp.</li>
   <li>Jump to the return address.</li>
   <li>Change the stack pointer to pop the parameters of q off the run-time stack.</li>
  </ul>

  <li>Contrast the run-time stack during the first and second calls to <code>g(1)</code>
      in the program in section (1).</li>
 </ul>

 <h2>3. Allocating Space for Arrays</h2>
 <ul>
  <li>Fixed-size arrays
   <pre><code>
   void f(int x) {
     int a;
     int b[4];
     int c;
     ...
   }
   </code></pre>
  </li><li>Space for the array <code>b</code> can be allocated on the runtime stack
      between the space for <code>a</code> and <code>c</code>.</li>

  <li>Variable-size arrays
   <pre><code>
   void f(int x) {
     int a;
     int b[n];
     int c;
     ...
   }
   </code></pre>
  </li><li>A pointer to the space for <code>b</code> can be allocated on the runtime stack
      between the space for <code>a</code> and <code>c</code> so variables remain
      a constant offset from the frame pointer.
      The actual space for <code>b</code> can be allocated after <code>c</code>.</li>
 </ul>

 <h2>4. Heap Memory Management</h2>
 <ul>
  <li>The runtime heap is used for data objects whose lifetimes may exist long after
      the procedure that created them.</li>
  <li>The heap memory manager is the subsystem that allocates and deallocates space
      within the heap.</li>
  <li>Garbage collection is the process of finding memory within the heap that is no
      longer used by the program and can therefore be reallocated to house other
      data objects. In some programming languages like C allocatation and deallocation
      needs to be done manually using library functions like <code>malloc</code>
      and <code>free</code>. In other languages like Java it is the garbage collector
      that deallocates memory.</li>
  <li>There are many different garbage collection algorithms that vary in performance
      metrics such as execution time, space usage, pause time, and program locality.</li>
  <li>Mark-and-sweep garbage collection</li>
  <ul>
   <li>A basic mark-and-sweep (M&amp;S) garbage collection algorithm is straightforward to
       implement and works well for languages like C because it does not move
       data objects during garbage collection.</li>
   <li>A M&amp;S collector firsts visits and marks all reachable data objects in the heap,
       setting the reached-bit of each object to 1.</li>
   <li>It then sweeps the entire heap, freeing up unreachable objects.</li>
   <li>See Algorithm 7.12 (ALSU, p. 471) for details.</li>
  </ul>
 </ul>



 <h2>5. Practice Problem</h2>
 <ul>
  <li>Consider the following program written in a hypothetical
      statically scoped
      language that allows nested functions. In this program,
      main calls f which calls g2 which calls h
      which calls g1.</li>
  </ul><pre><code>
     function main() {
       int i;

       function f() {
         int a, b, c;

         function g1() {
           int a, d;

           a = b + c;               // point 1

         }; // end of g1

         function g2(int x) {
           int b, e;

           function h() {
             int c, e;

             g1();
             e = b + a;             // point 2

           }; // end of h

           h();
           a = d + e;               // point 3

         }; // end of g2

         g2(1);

       }; //end of f

       // execution of main begins here

       f();

     }; // end of main
  </code></pre>

  <ol style="list-style-type: lower-alpha;">
   <li>Suppose we have activation records with the following fields:</li>
   </ol><ul>
       <dt>Parameters</dt>
       <dt>Control Link</dt>
       <dt>Access Link</dt>
       <dt>Return Address</dt>
       <dt>Local data</dt>
   </ul>
   <dt>If function p is nested immediately within function q, then the
       access link in any AR for p points to the most recent AR for q.</dt>
   <dt>Show the activation records on the run-time stack
       when execution first arrives at point 1 in the program above.</dt>
   <li>To which declarations are the references to variables a, b, c at position 1?</li>
   <li>To which declarations are the references to variables a, b, e at position 2?</li>
   <li>To which declarations are the references to variables a, d, e at position 3?</li>
  
 

 <h2>6. Reading</h2>
 <ul>
  <li>ALSU, Sections 7.1, 7.2, 7.6.1</li>
  <li>K. C. Louden, Compiler Construction, PWS Publishing, 1997.</li>
 </ul><br/>


<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 19, April 8, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 19: Code Generation<br/>
  April 8, 2013
 </h1>

 <h2>Lecture Outline</h2>
 <ol>
  <li>Issues in code generation</li>
  <li>Memory hierarchy</li>
  <li>Categories of target machines</li>
  <li>Primary tasks of a code generator</li>
  <li>Basic blocks and flow graphs</li>
 </ol>

 <h2>1. Issues in Code Generation</h2>
 <ul>
  <li>Role of the code generator</li>
  <ul>
   <li>Input: IR of the source program produced by the font end</li>
   <li>Output: good target machine code</li>
  </ul>
  <li>Challenges of code generation</li>
  <ul>
   <li>The target program must preserve the semantic meaning of the
       source program.</li>
   <li>The target program should make efficient use of the target
       machine's resources.</li>
   <li>The code generator itself should be efficient.</li>
   <li>The problem of optimal code generation is undecidable.</li>
   <li>Many subproblems in code generation, such as optimal register
       allocation, are computationally intractable.</li>
   <li>The design of a good code generator often reverts to the problem of
       designing good heuristics.</li>
  </ul>
 </ul>

 <h2>2. Memory Hierarchy</h2>
 <ul>
  <li>Processor registers are the fastest devices in a computer's
      memory hierarchy.  Here are some typical access times and sizes
      of the components of a computer's memory hierarchy.</li>
 </ul><pre><code>
		 Access Time		   Size
registers	0.2 - 0.5 ns		256 -  1024 B
L1 cache	0.4 -   1 ns		 32 -   256 KB
L2 cache	  4 -  10 ns		512 KB -  2 MB
main memory	 50 - 500 ns		256 MB - 16 GB
disk		  5 -  15 ms		 80 GB
tape		  1 -  50 s		infinite
 </code></pre>
  <li>As we can see, there are several orders of magnitude difference
      in the access time between registers and main memory.  As a consequence,
      compiler code generators attempt to use registers to hold the most frequently
      used objects in the target program. But since there are a small number
      of registers, the compiler will have to make choices as to what quantities
      it keeps in registers and what quantities it must spill into main memory.</li>
 

 <h2>3. Categories of Target Machines</h2>
 <ul>
  <li>Reduced instruction set machines (RISC)</li>
  <ul>
   <li>many registers</li>
   <li>three-address instructions</li>
   <li>simple addressing modes</li>
   <li>simple instruction set architecture</li>
  </ul>
  <li>Complex instruction set machines (CISC)</li>
  <ul>
   <li>few registers</li>
   <li>two-address instructions</li>
   <li>variety of addressing modes</li>
   <li>several register classes</li>
   <li>variable-length instructions</li>
   <li>instructions with side effects</li>
  </ul>
  <li>Stack-based machines</li>
  <ul>
   <li>push operands onto stack</li>
   <li>perform operations on operands at top of stack</li>
   <li>stack kept in registers</li>
   <li>model for Java Virtual Machine</li>
  </ul>
  <li>Multicore machines</li>
 </ul>


 <h2>4. Primary Tasks of a Code Generator</h2>
  <ul>
   <li>Instruction selection</li>
   <ul>
    <li>Determining factors:</li>
    <ul>
     <li>level of IR</li>
     <li>nature of ISA</li>
     <li>desired quality of generated code</li>
    </ul>
   </ul>
   <li>Register allocation and assignment</li>
   <ul>
    <li>Register allocation determines the set of variables that
        will reside in registers at each point in the program.</li>
    <li>Register assignment determines the specific register in which
        a variable will reside.</li>
   </ul>
   <li>Evaluation order</li>
   <ul>
    <li>Some computation orders require fewer registers to hold intermediate
        results than others.</li>
    <li>Picking an optimal order in the general case is NP-complete.</li>
   </ul>
  </ul>

 <h2>5. Basic Blocks and Flow Graphs</h2>
 <ul>
  <li>A basic block is a maximal sequence of consecutive three-address
     instructions such that</li>
  </ul><ol>
   <li>The flow of control can only enter the basic block throught the
       first instruction in the block.</li>
   <li>Control will leave the block without halting or branching
       except possibly at the last instruction in the block.</li>
  </ol>
  <li>A flow graph for the basic blocks of an intermediate program
      can be constructed as follows:</li>
  <ul>
   <li>The basic blocks are the nodes of the flow graph.</li>
   <li>There is an edge from block B to block C iff it is possible
       for the first instruction in C to immediately follow the last
       instruction in B.</li>
  </ul>
  <li>A set of nodes L in a flow graph is a loop if</li>
  <ol>
   <li>There is a node in L called the loop entry with the property
       that no other node in L has a predecessor outside L.</li>
   <li>Every node in L has a nonempty path completely within L to
       the entry of L.</li>
  </ol>
 

 <h2>6. Practice Problem</h2>
 <ol>
  <li>ALSU, Exercise 8.4.1, p. 531.</li>
 </ol>

 <h2>7. Reading</h2>
 <ul>
  <li>ALSU, Sections 8.1-8.5</li>
 </ul><br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<title>Lecture 20, April 10, 2013</title>
</head>
<body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 20: Code Generation Algorithms<br/>
  April 10, 2013
 </h1>


 <h2>1. Target Machine</h2>
 <ul>
   <li><i>n</i> general-purpose registers</li>
   <li>Instructions: load, store, compute, jump, conditional jump</li>
   <li>Various addressing modes:</li>
   <ul>
    <li>indexed address</li>
    <li>integer indexed by a register</li>
    <li>indirect addressing</li>
    <li>immediate constant</li>
   </ul>
   <li>Example 1: for <code>x = y + z</code> we can generate</li>
   </ul><pre><code>
   LD  R1, y       // R1 = y
   LD  R2, z       // R2 = z
   ADD R1, R1, R2  // R1 = R1 + R2
   ST  x, R1       // x  = R1
   </code></pre>

   <li>Example 2: for <code>b = a[i]</code> where a is an
       array of integers we can generate</li>
   <pre><code>
   LD  R1, i       // R1 = i
   MUL R1, R1, #4  // R1 = R1 * 4
   LD  R2, a(R1)   // R2 = contents(a + contents(R1))
   ST  b, R2       // b  = R2
   </code></pre>

   <li>Example 3: for <code>a[j] = c</code> where a is an
       array of integers we can generate</li>
   <pre><code>
   LD  R1, c       // R1 = c
   LD  R2, j       // R2 = j
   MUL R2, R2, #4  // R2 = R2 * 4
   ST  a(R2), R1   // contents(a + contents(R2)) = R1
   </code></pre>

   <li>Example 4: for <code>x = *p</code> we can generate</li>
   <pre><code>
   LD  R1, p       // R1 = p
   LD  R2, 0(R1)   // R2 = contents(0 + contents(R1))
   ST  x, R2       // x = R2
   </code></pre>

   <li>Example 5: for <code>*p = y</code> we can generate</li>
   <pre><code>
   LD  R1, p       // R1 = p
   LD  R2, y       // R2 = y
   ST  0(R1), R2   // contents(0 + contents(R1)) = R2
   </code></pre>

   <li>Example 6: for <code>if x &lt; y goto L</code> we can generate</li>
   <pre><code>
   LD   R1, x      // R1 = x
   LD   R2, y      // R2 = y
   SUB  R1, R1, R2 // R1 = R1 - R2
   BLTZ R1, M      // if R1 &lt; 0 jump to M
   </code></pre>
   <dt>Here <code>M</code> is the label of the first machine instruction
       generated from the three-address instruction that has label <code>L</code></dt>

  <li>Instruction cost: 1 + cost associated with the addressing modes of the operands</li>
 

 <h2>2. Names to Addresses</h2>
 <ul>
  <li>Addresses in the target code are in the runtime address space.</li>
  <li>Names in the IR need to be converted into addresses in the target code.</li>
  <li>Example: managing the addresses in the runtime stack</li>
  <ul>
   <li>The code for the first procedure initializes the runtime stack by setting
       the stack pointer SP to the start of the stack.</li>
   <li>A procedure call increments SP, saves the return address, and
       transfers control to the called procedure.</li>
   <li>In the return sequence</li>
   <ul>
    <li>the called procedure transfers control to the return address
        using the instruction <code>BR *0(SP)</code></li>
    <li>the caller decrements SP to its previous value</li>
   </ul>
  </ul>
 </ul>

 <h2>3. Computing Next-Use Information</h2>
 <ul>
  <li>Knowing when the value of a variable will be used next is essential
      for generating good code.</li>
  <li>If there is a three-address instruction sequence of the form</li>
  </ul><pre><code>
       i:  x = y + z

       .
       . no assignments to x between instructions i and j
       .

       j:  a = x + b
  </code></pre>

  <dt>then we say statement <code>j</code> <i>uses</i> the
      value of <code>x</code> computed at <code>i</code>.</dt>

  <li>We also say that variable <code>x</code> is <i>live</i> at statement <code>i</code>.</li>
  <li>A simple way to find next uses is to scan backward from
      the end of a basic block keeping track for each name
      <code>x</code> whether <code>x</code> has a next use in
      the block and if not whether <code>x</code> is live on
      exit from that block. See Alg. 8.7, p. 528.</li>
 


 <h2>4. A Simple Code Generator</h2>
 <ul>
  <li>Here we describe an algorithm for generating code for a basic block
      that keeps track of what values are in registers so it can avoid
      generating unnecessary loads and stores.</li>
  <li>It uses a register descriptor to keep track of what variable values
      are in each available register.</li>
  <li>It uses an address descriptor to keep track of the location or locations
      where the current value of each variable can be found.</li>
  <li>For the instruction x = y + z it generates code as follows:</li>
  <ul>
   <li>It calls a function getReg(x = y + z) to select registers Rx, Ry, and Rz for
       variables x, y, and z.</li>
   <li>If y is not in Ry, it issues the load instruction <code>LD Ry, My</code>
       where My is one of the memory locations for y in the address descriptor.</li>
   <li>Similarly, if z is not in Rz, it issues a load instruction <code>LD Rz, Mz</code>.</li>
   <li>It then issues the instruction <code>ADD Rx, Ry, Rz</code>.</li>
  </ul>
  <li>For the instruction x = y it generates code as follows:</li>
  <ul>
   <li>It calls a function getReg(x = y) to select a register Ry for
       both x and y.  We assume retReg will always choose the same register
       for both x and y.</li>
   <li>If y is not in Ry, issue the load instruction <code>LD Ry, My</code>
       where My is one of the memory locations for y in the address descriptor.</li>
   <li>If y is already in Ry, we issue no instruction.</li>
  </ul>
  <li>At the end of the basic block, it issues a store instruction
      <code>ST x, R</code> for every variable x that is live on exit from
      the block and whose current value resides only in a register R.</li>
  <li>The register and address descriptors are updated appropriately as each
      machine instruction is issued.</li>
  <li>If there are no empty registers and a register is needed, the function
      getReg generates a store instruction <code>ST v, R</code> to store the value
      of the variable v in some occupied register R.  Such a store is called
      a <i>spill</i>.  There are a number of heuristics to choose the register
      to spill.</li>
 </ul>


 <h2>5. Optimal Code Generation for Expression Trees</h2>
 <ul>
  <li>In this section we assume we are using a <i>k</i>-register machine with
      instructions of the form</li>
  <ul>
   <li><code>LD reg, mem</code></li>
   <li><code>ST mem, reg</code></li>
   <li><code>OP reg, reg, reg</code></li>
  </ul>
  <dt>to evaluate expressions.
  <li>Ershov numbers</li>
  <ul>
   <li>An expression tree is a syntax tree for an expression.</li>
   <li>Numbers, called Ershov numbers, can be assigned to label the nodes of an expression
       tree. The Ershov number at a node gives the minimum number of registers needed
       to evaluate on a register machine the expression generated by that node
       with no spills.</li>
   <li>Algorithm to label the nodes of an expression tree with Ershov numbers</li>
   </ul><ol>
    <li>Label all leaves 1.</li>
    <li>The label of an interior node with one child is the label of its child.</li>
    <li>The label of an interior node with two children is the larger of the
        labels of its children if these labels are different; otherwise, it is
        one plus the label of the left child.</li>
   </ol>
  <li>Sethi-Ullman algorithm
      generates register machine code that minimizes the number of spills to evaluate
      an expression tree. Ershov numbers guide the evaluation order.</li>
  <ul>
   <li>Input: an expression tree labeled with Ershov and a <i>k</i>-register machine.</li>
   <li>Output: an optimal sequence of register machine instructions to evaluate
       the root of the tree into a register.</li>
   <li>The details of the algorithm are in Section 8.10 of ALSU, pp. 567-573.</li>
  </ul>
  </dt></ul>
 
 
 <h2>6. Practice Problems</h2>
 <ol>
  <li>ALSU, Exercise 8.2.5 (p. 517).</li>
  <li>ALSU, Exercise 8.10.2 (p. 573).</li>
 </ol>

 <h2>7. Reading</h2>
 <ul>
  <li>ALSU, Sections 8.2-8.4, 8.6, 8.10</li>
 </ul>
 <br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>
</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
<!-- Homework Assignment #4                         -->
<!-- Created by Al Aho on 4/15/2013                 -->
<!-- Last modified on 4/15/2013                     -->
<!-- ------------------------------------------------->
<title>Homework #4</title></head>
<body>
 <h1 style="text-align: left;"> COMS W4115
  <br/>Programming Languages and Translators
  <br/>Homework Assignment #4
  <br/>Submit solutions electronically on
  <br/>&#160;&#160;&#160; Courseworks/COMSW4115/Assignments
  <br/>&#160;&#160;&#160; by 2:40pm, April 24, 2013
 </h1>
 <hr/>

<h2>Instructions</h2>
 <ul>
  <li>Problems 1-4 are each worth 25 points.</li>
  <li>You may discuss the questions with the TAs and others in the class but your answers
      must be in your own words. You must not copy someone
      else's solutions.
      If you consult others or use external sources, please cite the
      people or sources used in your answers.</li>
  <li>Solutions to these problems will be posted
      on Courseworks on May 1, 2013.</li>
  <li>This assignment may submitted electronically on Courseworks by 2:40pm,
      May 1, 2013 for 50% credit.</li>
  <li>Pdf files are preferred.</li>
 </ul>

<h2>Problems</h2>
 <ol>


  <li>Consider the arithmetic expression  <code>u * (v - w) + x / y</code>
      and a register machine with instructions of the form</li>

<pre><code>
     LD reg, src
     ST dst, reg
     OP reg1, reg2, reg3      // the registers need not be distinct
</code></pre>

   <ol style="list-style-type: lower-alpha;">
    <li>Draw an abstract syntax tree for the expression and label the nodes with Ershov numbers.</li>
    <li>Generate machine code for the expression on a two-register machine
        minimizing the number of spills.</li>
   </ol>
  <br/>


  <li>Consider the following sequence of three-address code:</li>

<pre><code>
   x = 0
   i = 0
L: t1 = i * 4
   t2 = a[t1]
   t3 = i * 4
   t4 = b[t3]
   t5 = t2 * t4
   x = x + t5
   i = i + 1
   if i &lt; n goto L
</code></pre>

   <ol style="list-style-type: lower-alpha;">
    <li>Draw a flow graph for this three-address code.</li>
    <li>Optimize this code by eliminating common subexpressions,
        performing reduction in strength on induction variables,
        and eliminating all the induction variables that you can.
        State what transformations you are using at each optimization step.</li>
   </ol>
  <br/>	



  <li>Consider the lambda-calculus expression
      (&#955;u. (&#955;x. u) u) ((&#955;y. y) (&#955;w. (&#955;v.v) w) ).</li>
  <ol style="list-style-type: lower-alpha;">
   <li>Draw a parse tree for this expression.</li>
   <li>Identify all redexes in this expression.</li>
   <li>Evaluate this expression using normal order evaluation.</li>
   <li>Evaluate this expression using applicative order evaluation.</li>
  </ol>

  <br/>
  <li>Evaluate the lambda-calculus expression
      <code>(&#955;x.(&#955;y.(x(&#955;x.xy))))y</code>. What order of
      evaluation did you use? Explain all the steps you               used in the evaluation.</li>

 </ol>

 <br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address> 

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
 <title>Lecture 22: April 17, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 22: April 17, 2013<br/>
  Introduction to Lambda Calculus
 </h1>

<h2>Outline</h2>
 <ul>
  <li>The evolution of programming languages</li>
  <li>Programming paradigms</li>
  <li>History of lambda calculus and functional programming languages</li>
  <li>CFG for lambda calculus</li>
  <li>Function abstraction</li>
  <li>Function application</li>
  <li>Free and bound variables</li>
  <li>Beta reductions</li>
  <li>Evaluating a lambda expression</li>
  <li>Currying</li>
  <li>Renaming bound variables by alpha reduction</li>
  <li>Eta conversion</li>
  <li>Substitutions</li>
  <li>Disambiguating lambda expressions</li>
  <li>Normal form</li>
  <li>Evaluation strategies</li>
 </ul>

 <h2>1. The Evolution of Programming Languages</h2>
 <ul>
  <li>There are thousands of programming languages in the world today.
      Why so many?</li>
  <ul>
   <li>Software is used in virtually every domain of human endeavor.
       Each application domain has its distinct and often conflicting programming needs.
       No one language is ideal for all application domains.</li>
   <ul>
    <li>Scientific programming demands support for matrices and efficient floating point computations.
        Fortran, first developed in the 1950s, is still widely used for scientific computation.</li>
    <li>Systems programming demands low-level control of machine resources, often
        with real-time constraints. C and C++ are the dominant systems programming languages.</li>
    <li>Business applications are characterized by data persistence and report generation.
        SQL is the dominant query language in business data processing.</li>
   </ul>
  </ul>
  <li>Why are there so many new programming languages?</li>
   <ul>
    <li>Software reliability and programmer productivity are critical concerns.</li>
    <li>Programmer training is a significant cost, so old languages tend to persist.</li>
    <li>But new languages can arise to meet the demands of new application domains.
        For example, Java was designed in the 1990s to meet the demands of Internet programming.
        But to gain acceptance, Java was designed to look a lot like C++.</li>
   </ul>
  <li>What makes a good programming language?</li>
   <ul>
    <li>This is a hotly debated question.  There is no universally accepted metric for
        the goodness of a programming language. Technical excellence is rarely a criterion
        for adoption.</li>
    <li>Functional programming languages have many ardent advocates and many features of
        functional languages have been incorporated into modern languages.
        The next sequence of lectures will discuss the theoretical underpinnings
        of functional languages and highlight a few important functional languages.</li>
   </ul>
 </ul>

 <h2>2. Programming Paradigms</h2>
 <ul>
  <li>A programming paradigm is a style of computer programming.</li>
  <li>There are many paradigms, each with its distinctive characteristics and
      prototypical programming languages. The four most common are:</li>
  <ul>
   <li>imperative also known as procedural programming</li>
   <ul>
    <li>imperative programming tends to be the most popular paradigm</li>
    <li>an imperative program tells the computer <i>how</i> to do a computation</li>
    <li>the typical imperative languages are C and Fortran</li>
   </ul>
   <li>object-oriented programming</li>
   <ul>
    <li>an object-oriented program consists of a collection of interacting objects</li>
    <li>C++, Java, and Smalltalk are prototypical object-oriented languages</li>
   </ul>
   <li>functional programming</li>
   <ul>
    <li>a functional program is typified by the recursive definition of functions</li>
    <li>Haskell, Lisp, Scheme, ML, and OCaml are popular functional languages</li>
   </ul>
   <li>declarative programming</li>
   <ul>
    <li>a declarative program specifies <i>what</i> should be computed</li>
    <li>SQL and Prolog are declarative languages</li>
   </ul>
  </ul>
  <li>Some programming languages are multi-paradigm languages in the sense that they can
      support more than one programming paradigm. For example, C#, OCaml, Ruby, and Scala
      support both imperative, object-oriented, and functional programming.</li> 
 </ul>


 <h2>3. History of Lambda Calculus and Functional Programming Languages</h2>
 <ul>
  <li>Lambda calculus was introduced in the 1930s by Alonzo Church at Princeton University
      as a mathematical system for defining computable functions.</li>
  <li>Lambda calculus is equivalent in definitional power to that of Turing machines.</li>
  <li>Lambda calculus serves as the theoretical model for functional programming languages
      and has applications to artificial intelligence, proof systems, and logic.</li>
  <li>Lisp was developed by John McCarthy at MIT in 1958 around lambda calculus.</li>
  <li>ML, a general-purpose functional language, was developed
      by Robin Milner at the University of Edinburgh in the early 1970s. Caml and OCaml are dialects of ML
      developed at INRIA in 1985 and 1996, respectively.</li>
  <li>Haskell, considered by many as one of the purest functional
      programming languages, was developed by Simon Peyton Jones,
      Paul Houdak, Phil Wadler and others in the late 1980s
      and early 90s.</li>
  <li>Because of its simplicity, lambda calculus is a very useful tool
      for the study and analysis of programming languages.</li>
 </ul>

 <h2>4. CFG for The Lambda Calculus</h2>
 <ul>
  <li>The central concept in lambda calculus is an expression which
      we can think of as a program that when
      evaluated returns a result consisting of another lambda calculus expression.</li>
  <li>Here is the grammar for lambda expressions:</li>
  <ul>
</ul></ul><pre><code>
expr &#8594; &#955; variable . expr | expr expr | variable | ( expr ) | built_in
</code></pre>
  
  <li>A <code>variable</code> is an identifier.</li>
  <li>A <code>built_in</code> is a built-in function such as addition or
      multiplication, or a constant such as an integer
      or boolean. However, as we shall see,
      all programming language construct
      can be implemented as functions with the pure lambda calculus so these       built-ins are unnecessary.
      However, we will use built-ins for notational simplicity.</li>
 

 <h2>5. Function Abstraction</h2>
 <ul>
  <li>A function abstraction, often called a lambda abstraction, is a lambda expression
      that defines a function.</li>
  <li>A function abstraction consists of four parts: a lambda followed by a variable, a          period, and then an expression as in
      <code>&#955;x.expr</code>.</li>

  <li>In the function abstraction <code>&#955;x.expr</code> the variable <code>x</code>
      is the formal parameter of the function and <code>expr</code> is the body of the           function.</li>
  <li>For example, the function abstraction &#955;<i>x</i>. + <i>x</i> 1 is a function of
       <i>x</i> that adds <i>x</i> to 1. Parentheses can be added to lambda expressions        for clarity.
       Thus, we could have
       written this function abstraction as &#955;<i>x</i>.(+ <i>x</i> 1) or even as
       (&#955;<i>x</i>. (+ <i>x</i> 1)).</li> 
  <li>In C this function definition might be written as</li>
</ul><pre><code>
int addOne (int x)
{
  return (x + 1);
}
</code></pre>
  <li>Note that unlike C the lambda abstraction does not give a name to the function.
      The lambda expression itself is the function.</li>
  <li>We say that <code>&#955;x.expr</code> <i>binds</i> the variable
      <code>x</code> in <code>expr</code> and that <code>expr</code> is the            <i>scope</i> of the variable.</li>
 

 <h2>6. Function Application</h2>
 <ul>
  <li>A function application, often called a lambda application, consists of
      an expression followed by an expression: <code>expr expr</code>.
      The first expression is a function abstraction and the second expression
      is an argument to which the function is applied.</li>
  <li>For example, the lambda expression &#955;<i>x</i>. (+ <i>x</i> 1) 2 is an
      application of the function &#955;<i>x</i>. (+ <i>x</i> 1) to the argument 2.</li>
  <li>This function application &#955;<i>x</i>. (+ <i>x</i> 1) 2 can be evaluated
      by substituting the argument 2 for the formal parameter <i>x</i> in the body
      (+ <i>x</i> 1). Doing this we get (+ 2 1). This substitution is called a
      beta reduction.</li>
  <li>Beta reductions are like macro substitutions in C. To do beta reductions correctly
      we may need to rename bound variables in lambda expressions to avoid name       clashes.</li>
  <li>Functions can be used as arguments to functions and functions can return functions
      as results.
 </li></ul>

 <h2>7. Free and Bound Variables</h2>
 <ul>
  <li>In the function definition &#955;<i>x</i>.<i>x</i> the variable <i>x</i>
      in the body of the           definition
      (the second <i>x</i>) is <i>bound</i> because its first occurrence in the
      definition is &#955;<i>x</i>.</li>
  <li>A variable that is not bound in <code>expr</code> is said to be <i>free</i> in             <code>expr</code>.
      In the function (&#955;<i>x</i>.<i>xy</i>), the variable <i>x</i> in the
      body of the function is bound and the variable <i>y</i> is free.</li>
  <li>Every variable in a lambda expression is either bound or free.
      Bound and free variables have quite a different status in functions.</li>

  <li>In the expression (&#955;<i>x</i>.<i>x</i>)(&#955;<i>y</i>.<i>yx</i>):</li>

  <ul>
    <li>The variable <i>x</i> in the body of the leftmost
        expression is bound to the first lambda.</li>
    <li>The variable <i>y</i> in the body of the second expression is bound to the second lambda.</li>
    <li>The variable <i>x</i> in the body of the second expression is free.</li>
    <li>Note that <i>x</i> in second expression is independent of the <i>x</i> in the first expression.</li>
  </ul>

  <li>In the expression (&#955;<i>x</i>.<i>xy</i>)(&#955;<i>y</i>.<i>y</i>):</li>
  <ul>
    <li>The variable <i>y</i> in the body of the leftmost
        expression is free.</li>
    <li>The variable <i>y</i> in the body of the second expression is bound to the second lambda.</li>
  </ul>

  <li>Given an expression <i>e</i>, the following rules define FV(<i>e</i>),
      the set of free variables in <i>e</i>:</li>

  <ul>
   <li>If <i>e</i> is a variable <i>x</i>, then FV(<i>e</i>) = {<i>x</i>}.</li>
   <li>If <i>e</i> is of the form &#955;<i>x</i>.<i>y</i>, then FV(<i>e</i>) = FV(<i>y</i>) - {<i>x</i>}.</li>
   <li>If <i>e</i> is of the form xy, then FV(<i>e</i>) = FV(<i>x</i>) &#8746; FV(<i>y</i>).</li>
  </ul>
  <li>An expression with no free variables is said to be <i>closed</i>.</li>

 </ul>


 <h2>8. Beta Reductions</h2>
 <ul>

  <li>A function application <code>&#955;x.e f</code> is evaluated by substituting the argument <code>f</code> for the
      formal parameter <code>x</code> in the body <code>e</code>
      of the function definition.</li>
  <li>We will use the notation [<code>f/x]e</code> to indicate that <code>f</code> is
      to be substituted
      for all free occurrences
      of <code>x</code> in the expression <code>e</code>.</li>
  <li>Examples:</li>
  </ul><ol>
   <li>(&#955;<i>x</i>.<i>x</i>)<i>y</i> &#8594; [<i>y</i>/<i>x</i>]<i>x</i> =        <i>y</i>.</li>
   <li>(&#955;<i>x</i>.<i>xzx</i>)<i>y</i> &#8594; [<i>y</i>/<i>x</i>]<i>xzx</i> =        <i>yzy</i>.</li>
   <li>(&#955;<i>x</i>.<i>z</i>)<i>y</i> &#8594; [<i>y</i>/<i>x</i>]<i>z</i> =        <i>z</i>.</li>
  </ol>
  <li>This substitution in a function application is called a <i>beta reduction</i>
      and we use a right arrow
      to indicate it.</li>

  <li>If expr1 &#8594; expr2, we say expr1 <i>reduces</i> to expr2 in one step.</li>
  <li>In general, <code>(&#955;x.e)f &#8594; [f/x]e</code>
      means that applying the function
      <code>(&#955;x.e)</code> to the argument expression <code>f</code> reduces to the 
      expression <code>[f/x]e</code>
      where the argument expression <code>f</code> is substituted for the function's             formal parameter <code>x</code>
      in the function body <code>e</code>.</li>
  <li>A lambda calculus expression (aka a "program") is "run" by computing a final result       by repeatly applying beta reductions.
      We use &#8594;* to denote the reflexive and transitive closure of &#8594;;
      that is, zero or more applications of beta reductions.</li>

  <li>Examples:</li>
  <ul>
   <li>(&#955;<i>x</i>.<i>x</i>)<i>y</i> &#8594; <i>y</i>
       (illustrating that &#955;<i>x</i>.<i>x</i> is the identity function).</li>
   <li>(&#955;<i>x</i>.<i>xx</i>)(&#955;<i>y</i>.<i>y</i>) &#8594;
       (&#955;<i>y</i>.<i>y</i>)(&#955;<i>y</i>.<i>y</i>) &#8594; (&#955;<i>y</i>.<i>y</i>);
       thus, we can write (&#955;<i>x</i>.<i>xx</i>)(&#955;<i>y</i>.<i>y</i>)
       &#8594;* (&#955;<i>y</i>.<i>y</i>).
       Note that here we have applied a function to a function as an argument
       and the result is a function.</li>
  </ul>
 


 <h2>9. Evaluating a Lambda Expression</h2>
 <ul>
  <li>A lambda calculus expression can be thought of as a program which can be
      executed by evaluating it. Evaluation is done by repeatedly finding a
      reducible expression (called a redex) and reducing it by a function evaluation
      until there are no more redexes.</li>
  <li>Example 1: The lambda expression (&#955;<i>x</i>.<i>x</i>)<i>y</i> in its entirety
      is a redex that reduces to <i>y</i>.</li>
  <li>Example 2: The lambda expression (+ (* 1 2) (- 4 3)) has two redexes:
      (* 1 2) and (- 4 3).
      If we choose to reduce the first redex,
      we get (+ 2 (- 4 3)).
      We can then reduce (+ 2 (- 4 3)) to get (+ 2 1).
      Finally we can reduce (+ 2 1) to get 3.</li>
  <li>Note that if we had chosen the second redex to revaluate first, we would
      have ended up with the same result:</li>
      <ul>
       <di>(+ (* 1 2) (- 4 3)) &#8594; (+ (* 1 2) 1) &#8594; (+ 2 1) &#8594; 3.</di>
      </ul>
 </ul>


 <h2>10. Currying</h2>
 <ul>
  <li>All functions in lambda calculus are prefix and take exactly one argument.</li>
  <li>If we want to apply a function to more than one argument, we can use a technique
      called <i>currying</i> that treats a function applied to more than
      one argument to a sequence of applications of one-argument functions.
      For example, to express the sum of 1 and 2 we can write (+ 1 2) as ((+ 1) 2)
      where the expression (+ 1) denotes the function that adds 1 to its argument.
      Thus ((+ 1) 2) means the function + is applied to the argument 1 and the result is
      a function (+ 1) that adds 1 to its argument:
      (+ 1 2) = ((+ 1) 2) &#8594; 3</li>
  <li>Note that function application associates to the left.</li>
 </ul>


 <h2>11. Renaming Bound Variables by Alpha Reduction</h2>
 <ul>
  <li>The name of a formal parameter in a function definition is arbitrary.
      We can use any variable to name a parameter, so that the function
      &#955;<i>x</i>.<i>x</i> is equivalent to &#955;<i>y</i>.<i>y</i> and       &#955;<i>z</i>.<i>z</i>.
      This kind of renaming is called <i>alpha reduction</i>.</li>
  <li>Note that we cannot rename free variables in expressions.</li>
  <li>Also note that we cannot change the name of a bound variable in an
      expression to conflict
      with the name of a free variable in that expression.</li>
 </ul>




 <h2>12. Eta Conversion</h2>
 <ul>
  <li>The two lambda expressions (&#955;<i>x</i>.+ 1 <i>x</i>) and (+ 1) are equivalent in the
      sense that these expressions behave in exactly the same way when they are applied to
      an argument -- they add 1 to it. &#951;-conversion is a rule that expresses this
      equivalence.</li>
  <li>In general, if <i>x</i> does not occur free in the function <i>F</i>, then (&#955;<i>x</i>.<i>F</i> <i>x</i>)
      is &#951;-convertible to <i>F</i>.</li>
 </ul>


 <h2>13. Substitutions</h2>
 <ul>
  <li>For a beta reduction, we introduced the notation <code>[f/x]e</code>
      to indicate that the expression <code>f</code> is to be substituted
      for all free occurrences
      of the formal parameter <code>x</code> in the expression <code>e</code>:</li>
  <ul>
   <dt><code>(&#955;x.e) f &#8594; [f/x]e</code></dt>
  </ul>
  <li>To avoid name clashes in a substitution <code>[f/x]e</code>,
      first rename the bound variables in <code>e</code> and <code>f</code>
      so they become distinct. Then perform the textual substituion of <code>f</code>
      for <code>x</code> in <code>e</code>.</li>
  <ul>
   <li>For example,
       consider the substitution <code>[y(&#955;x.x)/x] &#955;y.(&#955;x.x)yx</code>.</li>
   <li>After renaming all the bound variables
       to make them all distinct we get <code>[y(&#955;u.u)/x]        &#955;v.(&#955;w.w)vx</code>.</li>
   <li>Then doing the substitution we get <code>&#955;v.(&#955;w.w)v(y(&#955;u.u))</code>.</li>
  </ul>


  <li>The rules for substitution are as follows. We assume <code>x</code> and <code>y</code>
      are distinct variables, and <code>e</code>, <code>f</code>
      and <code>g</code> are expressions.</li>
  <ul>
   <li>For variables</li>
    <code>
    <dt>&#160;&#160;&#160;[e/x]x = e</dt>
    <dt>&#160;&#160;&#160;[e/x]y = y</dt>
    </code>
   <li>For function applications</li>
    <code><dt>&#160;&#160;&#160;[e/x](f g) = ([e/x]f) ([e/x]g)</dt></code>
   <li>For function abstractions</li>
    <dt><code>&#160;&#160;&#160;[e/x](&#955;x.f) = &#955;x.f</code></dt>

    <dt><code>&#160;&#160;&#160;[e/x](&#955;y.f) = &#955;y.[e/x]f</code>,
        provided <code>y</code> is not free in <code>e</code>
        (the "freshness" condition).</dt>
  </ul>
  <li>Examples:</li>
  </ul><ol>
   <li><code>[y/y](&#955;x.x) = &#955;x.x</code></li>
   <li><code>[y/x]((&#955;x.y) x) = ([y/x](&#955;x.y)) ([y/x]x) = (&#955;x.y)y</code></li>
   <li>Note that the freshness condition does not allow us to make the
       substitution <code>[y/x](&#955;y.x) = &#955;y.([y/x]x) = &#955;y.y</code>
       because <code>y</code> is free in the expression <code>y</code>.</li>
  </ol>
 

 <h2>14. Disambiguating Lambda Expressions</h2>
 <ul>
  <li>The grammar we gave in section 4 for lambda expressions is ambgious.
      A few simple rules will remove the ambiguities.</li>
  <li>Function application is left associative:
      <i>f g h</i> = ((<i>f g</i>) <i>h</i>)</li>
  <li>Function application binds more tightly than lambda:
      &#955;<i>x</i>.<i>f g x </i> = (&#955;<i>x</i>.(<i>f g</i>) <i>x</i>) </li>
  <li>The body in a function abstraction extends as far to the right as possible:
      &#955;<i>x</i>. + <i>x</i> 1 =
      &#955;<i>x</i>. (+ <i>x</i> 1).</li>
 </ul>

 <h2>15. Normal Form</h2>
 <ul>
  <li>An expression containing no possible beta reductions is said to be in normal form.
      A normal form expression is one containing no redexes.</li>  
  <li>Examples of normal form expressions:</li>
  <ul>
   <li><code>x</code> where <code>x</code> is a variable.</li>
   <li><code>x e</code> where <code>x</code> is a variable and <code>e</code>
       is a normal form expression.</li>
   <li><code>&#955;x.e</code> where <code>x</code> is a variable and
       <code>e</code> is a normal form expression.</li>
  </ul>
  <li>The expression <code>(&#955;x.x x)(&#955;x.x x)</code> does not have a normal form
      because it always evaluates to itself. We can think of this expression as
      a representation for an infinite loop.</li>
 </ul>

 <h2>16. Evaluation Strategies</h2>
 <ul>
  <li>An expression may contain more than one redex so there can be
      several reduction sequences. For example, the expression
      <code>(+ (* 1 2) (- 4 3))</code> can be reduced to normal
      form with two reduction sequences:</li>
   <ul><code>
     <dt>(+ (* 1 2) (- 4 3))</dt>
     <dt>&#8594; (+ 2 (- 4 3))</dt>
     <dt>&#8594; (+ 2 1)</dt>
     <dt>&#8594; 3</dt></code>
   </ul>
   <dt>or the sequence</dt>
   <ul><code>
     <dt>(+ (* 1 2) (- 4 3))</dt>
     <dt>&#8594; (+ (* 1 2) 1)</dt>
     <dt>&#8594; (+ 2 1)</dt>
     <dt>&#8594; 3</dt></code>
   </ul>

  <li>As we pointed out in (15), the expression <code>(&#955;x.x x)(&#955;x.x x)</code>
      does not have a terminating sequence of reductions.</li>
  <li>Some reduction sequences for a given expression may reach a normal form while
      others may not. For example, consider the expression
      <code>(&#955;x.1)((&#955;x.x x)(&#955;x.x x))</code>.
      If we reduce the application of <code>(&#955;x.1)</code> to
      <code>(&#955;x.x x)(&#955;x.x x)</code>, we get the result 1,
      but if we keep reducing the application of <code>(&#955;x.x x)</code>
      to <code>(&#955;x.x x)</code>, the evaluation will not terminate.</li>
  <li>We shall see that repeatedly reducing the leftmost outermost redex will always             yield a normal form, if a normal form exists.</li>
 </ul>

 <h2>17. Practice Problems</h2>
 <ol>
  <li>Evaluate <code>(&#955;x. &#955;y. + x y)1 2</code>.</li>
  <li>Evaluate <code>(&#955;f. f 2)(&#955;x. + x 1)</code>.</li>
  <li>Evaluate <code>(&#955;x. (&#955;x. + (* 1)) x 2) 3</code>.</li>
  <li>Evaluate <code>(&#955;x. &#955;y. + x((&#955;x. * x 1) y))2 3</code>.</li>
  <li>Is <code>(&#955;x. + x x)</code> &#951;-convertible to
      <code>(+ x)</code>?</li>
  <li>Show how all bound variables in a lambda expression
      can be given distinct names.</li>
  <li>Construct an unambiguous grammar for lambda calculus.</li>
 </ol>

 
 <h2>18. References</h2>
 <ul>
  <li>Simon Peyton Jones, <i>The Implementation of Functional Programming Languages</i>,
      Prentice-Hall, 1987.</li>
  <li><a href="13-04-22_Edwards-LC.pdf">
      Stephen A. Edwards: The Lambda Calculus</a> &#160;</li>
  <li>http://www.inf.fu-berlin.de/lehre/WS01/ALPI/lambda.pdf</li>
  <li>http://www.soe.ucsc.edu/classes/cmps112/Spring03/readings/lambdacalculus/project3.html</li>
 </ul>
<br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html><html><head>
<meta http-equiv="content-type" content="text/html; charset=windows-1252"/>
 <title>Lecture 23: April 22, 2013</title></head><body>
 <h1>COMS W4115<br/>
  Programming Languages and Translators<br/>
  Lecture 23: April 22, 2013<br/>
  The Lambda Calculus
 </h1>

<h2>Outline</h2>
 <ul>
  <li>Reduction orders</li>
  <li>The Church-Rosser theorems</li>
  <li>The Y combinator</li>
  <li>Implementing factorial using the Y combinator</li>
  <li>Church numerals</li>
  <li>Arithmetic</li>
  <li>Logic</li>
  <li>Other programming language constructs</li>
  <li>The influence of the lambda calculus on functional languages</li>
 </ul>


 <h2>1. Reduction Orders</h2>
 <ul>
  <li>Programming languages use many different techniques to pass parameters to procedures
      such as call by value, call by reference, call by value-return, call by name, and so on.
      We can model many of these parameter-passing mechanisms using the lambda calculus.</li> 
  <li>The order in which reductions are applied within a lambda expression
      can affect the final result.
      We will use the terms reduction order and evaluation order synonymously.</li>
  <li>A reducible expression (redex) in a lambda expression is a subexpression
      that can be reduced using beta reduction.</li>
  <li>An expression that contains no redexes is said to be in normal form.</li>
  <li>Some reduction orders for an expression may yield a normal form
      expression while
      other orders may not. For example, consider the expression
  <ul>
      <code>(&#955;x.1)((&#955;x.x x)(&#955;x.x x))</code>
  </ul>
  </li><li>This expression has two redexes:</li>
  </ul><ol>
   <li>The entire expression is a redex in which we can apply the function
       <code>(&#955;x.1)</code> to
       the argument
       <code>((&#955;x.x x)(&#955;x.x x))</code> to yield the value 1.</li>
   <li>The rightmost subexpression <code>((&#955;x.x x)(&#955;x.x x))</code>
       is also a redex in which
       we can apply the function <code>(&#955;x.x x)</code> to the argument
       <code>(&#955;x.x x)</code>. But if we do this reduction we get same subexpression:
       <code>(&#955;x.x x)(&#955;x.x x)</code> &#8594;
       <code>(&#955;x.x x)(&#955;x.x x)</code>.
       Thus, continuing this order of evaluation will not terminate in a normal form.</li>
  </ol>
  <li>A remarkable property of lambda calculus is that every lambda expression
      has a unique normal form if one exists.</li>
  <li>The expression <code>(&#955;x.1)((&#955;x.x x)(&#955;x.x x))</code>
      has the normal form 1.</li>
  <li>The expression <code>(&#955;x.x x)(&#955;x.x x)</code> does not have a normal form
      because it always evaluates to itself. We can think of this expression as
      a representation for an infinite loop.</li>
  <li>There are two common reduction orders for lambda expressions:
      normal order evaluation and applicative order evaluation.</li>

  <li>Normal order evaluation</li>
  <ul>
   <li>In normal order evaluation we always reduce the leftmost outermost redex at each step.</li>
   <li>The first reduction order above is a normal order evaluation.</li>
   <li>If an expression has a normal form, then normal order evaluation will always find it.</li>
  </ul>

  <li>Applicative order evaluation</li>
  <ul>
   <li>In applicative order evaluation we always reduce the leftmost innermost redex
       at each step.</li>
   <li>The second reduction order above is an applicative order evaluation.</li>
   <li>Thus, even though an expression may have a normal form, applicative order evaluation
       may fail to find it.</li>
  </ul>


 

 <h2>2. The Church-Rosser Theorems</h2>
 <ul>
  <li>A remarkable property of lambda calculus is that every expression has a unique
      normal form if one exists.</li>
  <li><b>Church-Rosser Theorem I:</b> If <code>e &#8594;* f</code> and <code>e &#8594;* g</code>
      by any two reduction orders,
      then there always exists
      a lambda expression <code>h</code> such that <code>f &#8594;* h</code>
      and <code>g &#8594;* h</code>.</li>
  <ul>
   <li>A corollary of this theorem is that no lambda expression can be reduced to
       two distinct normal forms. To see this, suppose <code>f</code> and <code>g</code>
       are in normal form. The Church-Rosser theorem says there must be an expression
       <code>h</code> such that <code>f</code> and <code>g</code> are each reducible
       to <code>h</code>.  Since <code>f</code> and <code>g</code> are in normal form,
       they cannot have any redexes so <code>f = g = h</code>.</li>
   <li>This corollary says that all reduction sequences that terminate will always
       yield the same result and that result must be a normal form.</li>
   <li>The term <i>confluent</i> is often applied to a rewriting system that has the
       Church-Rosser property.</li>
  </ul>
  <li><b>Church-Rosser Theorem II:</b> If <code>e &#8594;* f</code> and
      <code>f</code> is in normal form, then there exists a normal order
      reduction sequence from <code>e</code> to <code>f</code>.</li>
 </ul>


 <h2>3. The Y Combinator</h2>
 <ul>
  <li>The <code>Y</code> combinator (sometimes called the paradoxical combinator)
      is a function that takes a function
      <code>G</code> as an argument and returns <code>G(YG)</code>.
      With repeated applications we can get
      <code>G(G(YG)), G(G(G(YG))),... </code>.</li>
  <li>We can implement recursive functions using the <code>Y</code> combinator.</li>
  <li><code>Y</code> is defined as follows:</li>
  <ul>
   <dt><code>(&#955;f.(&#955;x.f(x x))(&#955;x.f(x x)))<code>
  </code></code></dt></ul>
  <li>Let us evaluate <code>YG</code> where <code>G</code>
      is any expression:</li>
  <ul>
    <dt><code>(&#955;f.(&#955;x.f(x x))(&#955;x'.f(x' x'))) G<br/>
        &#8594; (&#955;x.G(x x))(&#955;x'.G(x' x'))<br/>
        &#8594; G((&#955;x'.G(x' x'))(&#955;x'.G(x' x')))<br/>
        &#8596; G((&#955;f.(&#955;x.f(x x))(&#955;x.f(x x)))G)<br/>
        = G(YG)</code><br/></dt>
  </ul>
  <li>Thus, <code>YG &#8594;<sup>*</sup> G(YG)</code>;
      that is, <code>YG</code> reduces to a call of <code>G</code> on
      <code>(YG)</code>.</li>
  <li>We will use <code>Y</code> to implement recursive functions.</li>
  <li><code>Y</code> is an example of a fixed-point combinator.</li>
 </ul>


 <h2>4. Implementing Factorial using the Y Combinator</h2>
 <ul>
  <li>If we could name lambda abstractions, we could define the
      factorial function with the following recursive definition:</li>
  <ul>
    <dt><code>FAC = (&#955;n.IF (= n 0) 1 (* n (FAC (- n 1 ))))</code></dt>
  </ul>
  <dt>where <code>IF</code> is a conditional function.</dt>
  <li>However, functions in lambda calculus cannot be named; they
      are anonymous.</li>
  <li>But we can express recursion as the fixed-point of a function <code>G</code>.
      To do this, let us simplify the essence of the problem.
      We begin with a skeletal recursive definition:</li>
  <ul>
    <dt><code>FAC = &#955;n.(... FAC ...)</code></dt>
  </ul>
  <li>By performing beta abstraction on <code>FAC</code>, we can transform its
      definition to:</li>
  <ul>
    <dt><code>FAC = (&#955;f.(&#955;n.(... f ...))) FAC</code><br/>
        <code>&#160;&#160;&#160;&#160;= G FAC</code></dt>
  </ul>
  <dt>where</dt>
  <ul>
   <dt><code>G = &#955;f.&#955;n.IF (= n 0) 1 (* n (f (- n 1 )))</code></dt>
  </ul>
  <dt>Beta abstraction is just the reverse of beta reduction.</dt>

  <li>The equation</li>
  <ul>
   <dt><code>FAC = G FAC</code><br/>
  </dt></ul>
  <dt>says that when the function <code>G</code> is applied to <code>FAC</code>,
      the result is <code>FAC</code>.
      That is, <code>FAC</code> is a fixed-point of <code>G</code>.</dt>
  <li>We can use the Y combinator to implement <code>FAC</code>:</li>
  <ul>
    <dt><code>FAC = Y G</code></dt>
  </ul>

  <li>As an example, let compute <code>FAC 1</code>:</li>

  <ul>
   <dt><code>FAC 1 = Y G 1<br/>
&#160;&#160;&#160;&#160;&#160;&#160;= G (Y G) 1<br/>
&#160;&#160;&#160;&#160;&#160;&#160;= &#955;f.&#955;n.IF (= n 0) 1 (* n (f (- n 1 ))))(Y G) 1<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; &#955;n.IF (= n 0) 1 (* n ((Y G) (- n 1 ))))1<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; IF (= n 0) 1 (* n ((Y G) (- 1 1 )))<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; * 1 (Y G 0)<br/>
&#160;&#160;&#160;&#160;&#160;&#160;= * 1 (G(Y G) 0)<br/>
&#160;&#160;&#160;&#160;&#160;&#160;= * 1((&#955;f.&#955;n.IF (= n 0) 1 (* n (f (- n 1 ))))(Y G 0)<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; * 1((&#955;n.IF (= n 0) 1 (* n ((Y G) (- n 1 ))))0<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; * 1(IF (= 0 0) 1 (* 0 ((Y G) (- 0 1 )))<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; * 1 1<br/>
&#160;&#160;&#160;&#160;&#160;&#160;&#8594; 1</code></dt>

  </ul>
 </ul>


 <h2>5. Church Numerals</h2>
 <ul>
  <li>Church numberals are a way of representing the integers in lambda calculus.
  </li><li>Church numerals are defined as functions taking two parameters:</li>
  <ul>
   <dt><code>0</code> is defined as <code>&#955;f.&#955;x. x</code></dt>
   <dt><code>1</code> is defined as <code>&#955;f.&#955;x. f x</code></dt>
   <dt><code>2</code> is defined as <code>&#955;f.&#955;x. f (f x)</code>.</dt>
   <dt><code>3</code> is defined as <code>&#955;f.&#955;x. f (f (f x))</code>.</dt>
   <dt><code>n</code> is defined as
       <code>&#955;f.&#955;x. f<sup><i>n</i></sup> x</code></dt>
  </ul>
  <li><code>n</code> has the property that for any lambda expressions <code>g</code> and
      <code>y</code>, <code>ngy &#8594;* g<sup><i>n</i></sup>y</code>.
      That is to say, <code>ngy</code> causes <code>g</code> to be
      applied to <code>y</code> <i>n</i> times.</li>
 </ul>

 <h2>6. Arithmetic</h2>
  <ul>
   <li>In lambda calculus, arithmetic functions can be represented by
       corresponding operations on Church numerals.</li>
   <li>We can define a successor function <code>succ</code> of three arguments that adds one
       to its first argument:</li>
   <ul>
    <dt><code>&#955;n.&#955;f.&#955;x. f (n f x)</code></dt>

    <li>Example: Let us evaluate <code>succ 0</code>:</li>

      <dt><code>(&#955;n.&#955;f.&#955;x. f (n f x)) 0<br/>
          &#160;&#160;&#160;&#8594; &#955;f.&#955;x. f (0 f x)<br/>
          &#160;&#160;&#160;= &#955;f.&#955;x. f ((&#955;f.&#955;x. x) f x)<br/>
          &#160;&#160;&#160;&#8594; &#955;f.&#955;x. f (&#955;x. x) x)<br/>
          &#160;&#160;&#160;&#8594; &#955;f.&#955;x. f x<br/>
          &#160;&#160;&#160;= 1</code>
      </dt>
   </ul>

  <li>We can define a function <code>add</code> using the identity
       <code>f<sup>(<i>m</i>+<i>n</i>)</sup> =
             f<sup><i>m</i></sup> &#186; f<sup><i>n</i></sup></code>
       as follows:</li>
   <ul>
      <dt><code>&#955;m.&#955;n.&#955;f.&#955;x. m f (n f x)</code></dt>
   </ul>
   <ul>
    <li>Example: Let us evaluate <code>add 1 2</code>:</li>

      <dt><code>&#955;m.&#955;n.&#955;f.&#955;x. m f (n f x) 1 2<br/>
          &#160;&#160;&#160;&#8594;&#160;&#160;&#955;n.&#955;f.&#955;x. 1 f (n f x) 2<br/>
          &#160;&#160;&#160;&#8594;*&#160;&#955;f.&#955;x. f (f (f x))<br/>
          &#160;&#160;&#160;=&#160;&#160;3</code>
      </dt>
   </ul>
 </ul>


 <h2>7. Logic</h2>
 <ul>
  <li>The boolean value true can be represented by a function of two arguments that
      always selects its first argument: <code>&#955;x.&#955;y.x</code></li>
  <li>The boolean value false can be represented by a function of two arguments that
      always selects its second argument: <code>&#955;x.&#955;y.y</code></li>

  <li>An if-then-else statement can be represented
      by a function of three arguments <code>&#955;c.&#955;i.&#955;e. c i e</code> that uses its
      condition <code>c</code> to select either the if-part <code>i</code>
      or the else-part <code>e</code>.</li>
   <ul>
    <li>Example: Let us evaluate if true then 1 else 2:</li>

      <dt><code>(&#955;c.&#955;i.&#955;e. c i e) true 1 2<br/>
          &#160;&#160;&#160;&#8594; (&#955;i.&#955;e. true i e) 1 2<br/>
          &#160;&#160;&#160;&#8594; (&#955;e. true 1 e) 2<br/>
          &#160;&#160;&#160;&#8594; true 1 2<br/>
          &#160;&#160;&#160;= (&#955;x.&#955;y.x) 1 2<br/>
          &#160;&#160;&#160;&#8594; (&#955;y.1) 2<br/>
          &#160;&#160;&#160;&#8594; 1<br/>
      </code></dt>
   </ul>

  <li>The boolean operators and, or, and not can be implemented as follows:</li>
  <ul>
      <dt><code>
          and = &#955;p.&#955;q. p q p<br/>
          or&#160; = &#955;p.&#955;q. p p q<br/>
          not = &#955;p.&#955;a.&#955;b. p b a<br/>
      </code></dt>

    <li>Example: Let us evaluate <code>not true</code>:</li>

      <dt><code>(&#955;p.&#955;a.&#955;b. p b a) true<br/>
          &#160;&#160;&#160;&#8594; &#955;a.&#955;b. true b a<br/>
          &#160;&#160;&#160;= &#955;a.&#955;b. (&#955;x.&#955;y.x) b a<br/>
          &#160;&#160;&#160;&#8594; &#955;a.&#955;b. (&#955;y.b) a<br/>
          &#160;&#160;&#160;&#8594; &#955;a.&#955;b. b<br/>
          &#160;&#160;&#160;= false </code> (under renaming)<br/>
      </dt>
   </ul>

 </ul>

 <h2>8. Other Programming Language Constructs</h2>
 <ul>
  <li>We can readily implement other programming language constructs in lambda calculus.
      As an example, here are lambda calculus expressions for various list operations
      such as
      cons (constructing a list),
      head (selecting the first item from a list), and
      tail (selecting the remainder of a list after the first item):</li>
  <ul>
      <dt><code>
          cons = &#955;h.&#955;t.&#955;f. f h t<br/>
          head = &#955;l.l (&#955;h.&#955;t. h)<br/>
          tail = &#955;l.l (&#955;h.&#955;t. t)<br/>
      </code></dt>
  </ul>
 </ul>

 <h2>9. The Influence of The Lambda Calculus on Functional Languages</h2>
 <ul>
  <li>Our next lecture will be by Maria Taku who will talk about the influence
      of the lambda calculus on functional languages and her experiences
      implementing the PLT compiler project using OCaml.</li>
 </ul>



 <h2>10. Practice Problems</h2>
 <ol>
  <li>Evaluate <code>(&#955;x.((&#955;w.&#955;z. + w z)1))
               ((&#955;x. xx)(&#955;x. xx)))
               ((&#955;y. * y 1) (- 3 2))</code>
      using normal order evaluation and applicative order evaluation.</li>
  <li>Give an example of a code optimization transformation that has the Church-Rosser property.</li>
  <li>Evaluate <code>FAC 2</code>.</li>
  <li>Evaluate <code>succ two</code>.</li>
  <li>Evaluate <code>add two three</code>.</li>
  <li>Let <code>mul</code> be the function</li>
   </ol><ul>
      <dt><code>&#955;m.&#955;n.&#955;f.&#955;x. m (n f x)</code></dt>
   </ul>
   <dt>Evaluate <code>mul two three</code>.</dt>
  <li>Write a lambda expression for the boolean predicate <code>isZero</code>
      and evaluate <code>isZero one</code>.</li>
 

 
 <h2>11. References</h2>
 <ul>
  <li>Simon Peyton Jones, <i>The Implementation of Functional Programming Languages</i>,
      Prentice-Hall, 1987.</li>
  <li><a href="13-04-22_Edwards-LC.pdf">
      Stephen A. Edwards: The Lambda Calculus</a> &#160;</li>
  <li>http://www.inf.fu-berlin.de/lehre/WS01/ALPI/lambda.pdf</li>
  <li>http://www.soe.ucsc.edu/classes/cmps112/Spring03/readings/lambdacalculus/project3.html</li>
 </ul>
<br/>

<hr/>
<address><a href="mailto:aho@cs.columbia.edu">aho@cs.columbia.edu</a></address>

</body></html>